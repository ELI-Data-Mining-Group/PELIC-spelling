{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PELIC spelling\n",
    "\n",
    "This notebook adds further processing to `answer.csv`  in the [`PELIC-dataset`](https://github.com/ELI-Data-Mining-Group/PELIC-dataset) repo ([`corpus_files` folder](https://github.com/ELI-Data-Mining-Group/PELIC-dataset/tree/master/corpus_files)) by creating a column of tok_POS and lemma_POS whose spelling has been automatically corrected.\n",
    "\n",
    "**Notebook contents:**\n",
    "- [Building `non_words_df`](#Building-non_words_df)\n",
    "- [Building `misspell_df`](#Building-misspell_df)\n",
    "- [Applying spelling correction](#Applying-spelling-correction)\n",
    "- [Incorporating corrections into `answer_df`](#Incorporating-corrections-into-answer_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building non_words_df\n",
    "In this section, we build a dataframe, `non_words_df`, which collects all of the non-words from the PELIC dataset (in `answer.csv`). The final dataframe has the following columns:\n",
    "- `non_word`: tuples with the non-words and their parts of speech\n",
    "- `sentence`: the complete sentence containing the non-word to provide context\n",
    "- `answer_id`: the id of the text they come from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anon_id</th>\n",
       "      <th>L1</th>\n",
       "      <th>gender</th>\n",
       "      <th>course_id</th>\n",
       "      <th>level_id</th>\n",
       "      <th>class_id</th>\n",
       "      <th>question_id</th>\n",
       "      <th>version</th>\n",
       "      <th>text_len</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tok_lem_POS</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eq0</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>Male</td>\n",
       "      <td>149</td>\n",
       "      <td>4</td>\n",
       "      <td>g</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>177</td>\n",
       "      <td>I met my friend Nife while I was studying in a...</td>\n",
       "      <td>[I, met, my, friend, Nife, while, I, was, stud...</td>\n",
       "      <td>((I, i, PRP), (met, meet, VBD), (my, my, PRP$)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>am8</td>\n",
       "      <td>Thai</td>\n",
       "      <td>Female</td>\n",
       "      <td>149</td>\n",
       "      <td>4</td>\n",
       "      <td>g</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>137</td>\n",
       "      <td>Ten years ago, I met a women on the train betw...</td>\n",
       "      <td>[Ten, years, ago, ,, I, met, a, women, on, the...</td>\n",
       "      <td>((Ten, ten, CD), (years, year, NNS), (ago, ago...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dk5</td>\n",
       "      <td>Turkish</td>\n",
       "      <td>Female</td>\n",
       "      <td>115</td>\n",
       "      <td>4</td>\n",
       "      <td>w</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>In my country we usually don't use tea bags. F...</td>\n",
       "      <td>[In, my, country, we, usually, do, n't, use, t...</td>\n",
       "      <td>((In, in, IN), (my, my, PRP$), (country, count...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dk5</td>\n",
       "      <td>Turkish</td>\n",
       "      <td>Female</td>\n",
       "      <td>115</td>\n",
       "      <td>4</td>\n",
       "      <td>w</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>I organized the instructions by time.</td>\n",
       "      <td>[I, organized, the, instructions, by, time, .]</td>\n",
       "      <td>((I, i, PRP), (organized, organize, VBD), (the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ad1</td>\n",
       "      <td>Korean</td>\n",
       "      <td>Female</td>\n",
       "      <td>115</td>\n",
       "      <td>4</td>\n",
       "      <td>w</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>59</td>\n",
       "      <td>First, prepare a port, loose tea, and cup.\\nSe...</td>\n",
       "      <td>[First, ,, prepare, a, port, ,, loose, tea, ,,...</td>\n",
       "      <td>((First, first, RB), (,, ,, ,), (prepare, prep...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          anon_id       L1  gender course_id level_id class_id question_id  \\\n",
       "answer_id                                                                    \n",
       "1             eq0   Arabic    Male       149        4        g           5   \n",
       "2             am8     Thai  Female       149        4        g           5   \n",
       "3             dk5  Turkish  Female       115        4        w          12   \n",
       "4             dk5  Turkish  Female       115        4        w          13   \n",
       "5             ad1   Korean  Female       115        4        w          12   \n",
       "\n",
       "          version  text_len  \\\n",
       "answer_id                     \n",
       "1               1       177   \n",
       "2               1       137   \n",
       "3               1        64   \n",
       "4               1         6   \n",
       "5               1        59   \n",
       "\n",
       "                                                        text  \\\n",
       "answer_id                                                      \n",
       "1          I met my friend Nife while I was studying in a...   \n",
       "2          Ten years ago, I met a women on the train betw...   \n",
       "3          In my country we usually don't use tea bags. F...   \n",
       "4                      I organized the instructions by time.   \n",
       "5          First, prepare a port, loose tea, and cup.\\nSe...   \n",
       "\n",
       "                                                      tokens  \\\n",
       "answer_id                                                      \n",
       "1          [I, met, my, friend, Nife, while, I, was, stud...   \n",
       "2          [Ten, years, ago, ,, I, met, a, women, on, the...   \n",
       "3          [In, my, country, we, usually, do, n't, use, t...   \n",
       "4             [I, organized, the, instructions, by, time, .]   \n",
       "5          [First, ,, prepare, a, port, ,, loose, tea, ,,...   \n",
       "\n",
       "                                                 tok_lem_POS  \n",
       "answer_id                                                     \n",
       "1          ((I, i, PRP), (met, meet, VBD), (my, my, PRP$)...  \n",
       "2          ((Ten, ten, CD), (years, year, NNS), (ago, ago...  \n",
       "3          ((In, in, IN), (my, my, PRP$), (country, count...  \n",
       "4          ((I, i, PRP), (organized, organize, VBD), (the...  \n",
       "5          ((First, first, RB), (,, ,, ,), (prepare, prep...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in PELIC_compiled.csv\n",
    "\n",
    "pelic_df = pd.read_csv(\"../PELIC-dataset/PELIC_compiled.csv\", index_col = 'answer_id', # answer_id is unique\n",
    "                      dtype = {'level_id':'object','question_id':'object','version':'object','course_id':'object'}, # str not ints\n",
    "                               converters={'tokens':literal_eval,'tok_lem_POS':literal_eval}) # read in as lists\n",
    "pelic_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The focus here is the `tok_lem_POS` column, but all columns will be kept as the entire df will be written out at the end of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating small dataframe to be used for finding non-words\n",
    "\n",
    "non_words = pelic_df[['text','tok_lem_POS']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** For spelling correction, it is necessary to decide what list of words will be used for determining if a word is real or not.\n",
    "\n",
    "Here, we use the [`SCOWL_condensed.txt`](https://github.com/ELI-Data-Mining-Group/PELIC-spelling/blob/master/SCOWL_condensed.txt) file which is a combination of wordlists available for download at http://wordlist.aspell.net/. We include items from all the dictionaries _except_ the abbreviations dictionary. For a detailed look at the compilation of this dictionary, please see the [SCOWL_wordlist](https://github.com/ELI-Data-Mining-Group/PELIC-spelling/blob/master/SCOWL_wordlist.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['unsensitizing', 'irreversibilities', 'handjar', 'polyfenestral', 'inseverably']\n"
     ]
    }
   ],
   "source": [
    "#Reading in SCOWL_condensed as a set as a lookup list for spelling (500k words)\n",
    "\n",
    "scowl = set(open(\"SCOWL_condensed.txt\", \"r\").read().split('\\n'))\n",
    "print(random.sample(scowl,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a list of words which should be considered words but which were previously being labelled as non-words. These items have been manually added to this list based on output later in this notebook. Most of these items are food items, names, or abbreviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n",
      "['adha', 'adj', 'ahamed', 'alaikum', 'anonurlpage', 'antiretroviral', 'arpa', 'beyonce', 'bibimbap', 'bio', 'biodiesel', 'bioethanol', 'bulgogi', 'bundang', 'cafe', 'carnaval', 'cds', 'cf', 'co', 'comscore', 'cyber', 'ddukboggi', 'def', 'dr', 'eg', 'eid', 'electrospray', 'entrees', 'erectus', 'etc', 'fiance', 'fiancee', 'fiter', 'fitir', 'fitr', 'fl', 'freediving', 'fukubukuro', 'geolinguist', 'hikikomori', 'hp', 'ibt', 'iq', 'iriver', 'jetta', 'jul', 'kabsa', 'kaled', 'kawader', 'km', 'leisureville', 'll', 'maamool', 'mayumi', 'mcdonalds', 'min', 'mongongo', 'nc', 'neuro', 'nian', 'notting', 'okroshka', 'onsen', 'pajeon', 'pbt', 'pc', 'pcs', 'pp', 'pudim', 'puket', 'samear', 'shui', 'sq', 'st', 'staycation', 'sth', 'taoyuan', 'toefl', 'trans', 'transgene', 'tv', 'unsub', 'va', 'vol', 'vs', 'webaholic', 'webaholics', 'webaholism', 'wenjing', 'woong', 'yaoming', 'ying', 'yingdong', 'yugong', 'yuval', 'zi']\n"
     ]
    }
   ],
   "source": [
    "actually_ok = open(\"actually_ok\", \"r\").read().split(',')\n",
    "actually_ok = [x[2:-1] for x in actually_ok]\n",
    "print(len(actually_ok))\n",
    "print(actually_ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py:5303: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "# Lower case all toks\n",
    "\n",
    "non_words.tok_lem_POS = non_words.tok_lem_POS.apply(lambda row: [(x[0].lower(),x[1],x[2]) for x in row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find non-words\n",
    "\n",
    "def spell_check(tok_lem_POS_list):\n",
    "    word_list = scowl # Choose word_list here. Default is scowl described above.\n",
    "    not_in_word_list = []\n",
    "    for tok_lem_POS in tok_lem_POS_list:\n",
    "        if tok_lem_POS[0] not in word_list and tok_lem_POS[0] not in actually_ok:\n",
    "            not_in_word_list.append(tok_lem_POS)\n",
    "    return not_in_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# Apply spell check function to find all misspelled-words. \n",
    "\n",
    "non_words['misspelled_words'] = non_words.tok_lem_POS.apply(spell_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tok_lem_POS</th>\n",
       "      <th>misspelled_words</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I met my friend Nife while I was studying in a...</td>\n",
       "      <td>[(i, i, PRP), (met, meet, VBD), (my, my, PRP$)...</td>\n",
       "      <td>[(., ., .), (., ., .), (., ., .), (;, ;, :), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ten years ago, I met a women on the train betw...</td>\n",
       "      <td>[(ten, ten, CD), (years, year, NNS), (ago, ago...</td>\n",
       "      <td>[(,, ,, ,), (,, ,, ,), (., ., .), (;, ;, :), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In my country we usually don't use tea bags. F...</td>\n",
       "      <td>[(in, in, IN), (my, my, PRP$), (country, count...</td>\n",
       "      <td>[(., ., .), (,, ,, ,), (., ., .), (., ., .), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I organized the instructions by time.</td>\n",
       "      <td>[(i, i, PRP), (organized, organize, VBD), (the...</td>\n",
       "      <td>[(., ., .)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>First, prepare a port, loose tea, and cup.\\nSe...</td>\n",
       "      <td>[(first, first, RB), (,, ,, ,), (prepare, prep...</td>\n",
       "      <td>[(,, ,, ,), (,, ,, ,), (,, ,, ,), (., ., .), (...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        text  \\\n",
       "answer_id                                                      \n",
       "1          I met my friend Nife while I was studying in a...   \n",
       "2          Ten years ago, I met a women on the train betw...   \n",
       "3          In my country we usually don't use tea bags. F...   \n",
       "4                      I organized the instructions by time.   \n",
       "5          First, prepare a port, loose tea, and cup.\\nSe...   \n",
       "\n",
       "                                                 tok_lem_POS  \\\n",
       "answer_id                                                      \n",
       "1          [(i, i, PRP), (met, meet, VBD), (my, my, PRP$)...   \n",
       "2          [(ten, ten, CD), (years, year, NNS), (ago, ago...   \n",
       "3          [(in, in, IN), (my, my, PRP$), (country, count...   \n",
       "4          [(i, i, PRP), (organized, organize, VBD), (the...   \n",
       "5          [(first, first, RB), (,, ,, ,), (prepare, prep...   \n",
       "\n",
       "                                            misspelled_words  \n",
       "answer_id                                                     \n",
       "1          [(., ., .), (., ., .), (., ., .), (;, ;, :), (...  \n",
       "2          [(,, ,, ,), (,, ,, ,), (., ., .), (;, ;, :), (...  \n",
       "3          [(., ., .), (,, ,, ,), (., ., .), (., ., .), (...  \n",
       "4                                                [(., ., .)]  \n",
       "5          [(,, ,, ,), (,, ,, ,), (,, ,, ,), (., ., .), (...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_words.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding context to the dataframe\n",
    "Seeing the mistakes in the context of a sentence will allow for better manual checking if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# Sent-tokenizing the text\n",
    "\n",
    "non_words['sents'] = non_words['text'].apply(lambda x: nltk.sent_tokenize(x))\n",
    "\n",
    "# And delete text column which is no longer needed\n",
    "\n",
    "del non_words['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py:5303: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "# Removing punctuation, numbers, and non-alphanumeric symbols from misspelled_words\n",
    "\n",
    "non_words.misspelled_words = non_words.misspelled_words.apply(lambda row: [x for x in row if x[0].isalpha()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing proper names - NNP, NNPS\n",
    "\n",
    "non_words.misspelled_words = non_words.misspelled_words.apply(lambda row: [x for x in row if x[2] != 'NNP' and x[1] != 'NNPS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing all words with length 1\n",
    "\n",
    "non_words.misspelled_words = non_words.misspelled_words.apply(lambda row: [x for x in row if len(x[0]) > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tok_lem_POS</th>\n",
       "      <th>misspelled_words</th>\n",
       "      <th>sents</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[(i, i, PRP), (met, meet, VBD), (my, my, PRP$)...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[I met my friend Nife while I was studying in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[(ten, ten, CD), (years, year, NNS), (ago, ago...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Ten years ago, I met a women on the train bet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[(in, in, IN), (my, my, PRP$), (country, count...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[In my country we usually don't use tea bags.,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[(i, i, PRP), (organized, organize, VBD), (the...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[I organized the instructions by time.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[(first, first, RB), (,, ,, ,), (prepare, prep...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[First, prepare a port, loose tea, and cup., S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tok_lem_POS misspelled_words  \\\n",
       "answer_id                                                                       \n",
       "1          [(i, i, PRP), (met, meet, VBD), (my, my, PRP$)...               []   \n",
       "2          [(ten, ten, CD), (years, year, NNS), (ago, ago...               []   \n",
       "3          [(in, in, IN), (my, my, PRP$), (country, count...               []   \n",
       "4          [(i, i, PRP), (organized, organize, VBD), (the...               []   \n",
       "5          [(first, first, RB), (,, ,, ,), (prepare, prep...               []   \n",
       "\n",
       "                                                       sents  \n",
       "answer_id                                                     \n",
       "1          [I met my friend Nife while I was studying in ...  \n",
       "2          [Ten years ago, I met a women on the train bet...  \n",
       "3          [In my country we usually don't use tea bags.,...  \n",
       "4                    [I organized the instructions by time.]  \n",
       "5          [First, prepare a port, loose tea, and cup., S...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_words.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And convert into more of a concordance format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_misspellings():\n",
    "    num = 1\n",
    "    collections = []\n",
    "\n",
    "    while num-1 < len(non_words):\n",
    "        misspelled_words = non_words['misspelled_words'].iloc[num-1]\n",
    "        sentences = non_words['sents'].iloc[num-1]\n",
    "        sentences = [sent.lower() for sent in sentences]\n",
    "    \n",
    "        index = 0\n",
    "        while index < len(sentences):\n",
    "            for word in misspelled_words:\n",
    "                if len(word[0]) == 1:\n",
    "                    pass\n",
    "                elif sentences[index][0] == '[':\n",
    "                    pass\n",
    "                elif word[0] in sentences[index]:\n",
    "                    collections.append((word, sentences[index], num))\n",
    "            index += 1\n",
    "        num += 1\n",
    "    \n",
    "    return collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tups = get_misspellings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tok_lem_POS</th>\n",
       "      <th>sentence</th>\n",
       "      <th>answer_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(beacause, beacause, NN)</td>\n",
       "      <td>i organized the instructions by time, beacause...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(wallmart, wallmart, NN)</td>\n",
       "      <td>next, you need to buy a box of tea in wallmart...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(dovn, dovn, NN)</td>\n",
       "      <td>first, you should take some hot water, you can...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(mircowave, mircowave, VBP)</td>\n",
       "      <td>first, you should take some hot water, you can...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(paragragh, paragragh, NN)</td>\n",
       "      <td>every paragragh's instructions depend on a mai...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   tok_lem_POS  \\\n",
       "0     (beacause, beacause, NN)   \n",
       "1     (wallmart, wallmart, NN)   \n",
       "2             (dovn, dovn, NN)   \n",
       "3  (mircowave, mircowave, VBP)   \n",
       "4   (paragragh, paragragh, NN)   \n",
       "\n",
       "                                            sentence  answer_id  \n",
       "0  i organized the instructions by time, beacause...          8  \n",
       "1  next, you need to buy a box of tea in wallmart...         11  \n",
       "2  first, you should take some hot water, you can...         13  \n",
       "3  first, you should take some hot water, you can...         13  \n",
       "4  every paragragh's instructions depend on a mai...         16  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rebuilding the non_words df\n",
    "\n",
    "non_words = pd.DataFrame(tups, columns = ['tok_lem_POS', 'sentence', 'answer_id'])\n",
    "non_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21368"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total number of non-words\n",
    "\n",
    "len(non_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a dataframe of misspellings\n",
    "In the `non-words` dataframe above, each row is an occurrence of a misspelling (i.e. _tokens_ ). We also want a dataframe where each row is a misspelling _type_ with frequency information attached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21368"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gathering the total misspellings\n",
    "\n",
    "total_misspellings = [x for x in non_words['tok_lem_POS']]\n",
    "len(total_misspellings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a dataframe of misspelled frequencies\n",
    "This dataframe will be what we apply the spell correction too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To keep an account of misspelling frequency, create a dictionary with frequencies\n",
    "\n",
    "misspell_freq_dict = {}\n",
    "for word in total_misspellings:\n",
    "    if word not in misspell_freq_dict:\n",
    "        misspell_freq_dict[word] = 1\n",
    "    else:\n",
    "        misspell_freq_dict[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('instand', 'instand', 'NN'), ('raeder', 'raeder', 'NN'), ('stuednts', 'stuednts', 'NNS'), ('instraction', 'instraction', 'NN'), ('chooesd', 'chooesd', 'VBP')]\n"
     ]
    }
   ],
   "source": [
    "print(random.sample(list(misspell_freq_dict),5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11084"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove duplicates\n",
    "\n",
    "final_misspellings = sorted(list(set(total_misspellings)))\n",
    "len(final_misspellings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aa</td>\n",
       "      <td>aa</td>\n",
       "      <td>VB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aabout</td>\n",
       "      <td>aabout</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aad</td>\n",
       "      <td>aad</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aain</td>\n",
       "      <td>aain</td>\n",
       "      <td>VBP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aare</td>\n",
       "      <td>aare</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0       1    2\n",
       "0      aa      aa   VB\n",
       "1  aabout  aabout   IN\n",
       "2     aad     aad   JJ\n",
       "3    aain    aain  VBP\n",
       "4    aare    aare   IN"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Constructing misspell_df\n",
    "\n",
    "misspell_df = pd.DataFrame(final_misspellings)\n",
    "misspell_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming columns to match other DataFrames in this notebook\n",
    "\n",
    "misspell_df.rename(columns = {0: 'misspelling',1:'lemma',2:'POS'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>misspelling</th>\n",
       "      <th>lemma</th>\n",
       "      <th>POS</th>\n",
       "      <th>tok_lem_POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aa</td>\n",
       "      <td>aa</td>\n",
       "      <td>VB</td>\n",
       "      <td>(aa, aa, VB)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aabout</td>\n",
       "      <td>aabout</td>\n",
       "      <td>IN</td>\n",
       "      <td>(aabout, aabout, IN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aad</td>\n",
       "      <td>aad</td>\n",
       "      <td>JJ</td>\n",
       "      <td>(aad, aad, JJ)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aain</td>\n",
       "      <td>aain</td>\n",
       "      <td>VBP</td>\n",
       "      <td>(aain, aain, VBP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aare</td>\n",
       "      <td>aare</td>\n",
       "      <td>IN</td>\n",
       "      <td>(aare, aare, IN)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  misspelling   lemma  POS           tok_lem_POS\n",
       "0          aa      aa   VB          (aa, aa, VB)\n",
       "1      aabout  aabout   IN  (aabout, aabout, IN)\n",
       "2         aad     aad   JJ        (aad, aad, JJ)\n",
       "3        aain    aain  VBP     (aain, aain, VBP)\n",
       "4        aare    aare   IN      (aare, aare, IN)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating recreating tok_lem_POS column to match dictionary\n",
    "\n",
    "misspell_df['tok_lem_POS'] = list(zip(misspell_df.misspelling, misspell_df.lemma, misspell_df.POS))\n",
    "misspell_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping dictionary to DataFrame\n",
    "\n",
    "misspell_df['freq'] = misspell_df['tok_lem_POS'].map(misspell_freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting by frequency\n",
    "\n",
    "misspell_df = misspell_df.sort_values(by=['freq'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### actually_ok\n",
    "The following is the basis for the 'acutally_ok' list used earlier. Here, errors with a frequency of 10 or more were manually checked, and if determined to be a real word, were added to the actually_ok list. There were originally 267 items which met this criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>misspelling</th>\n",
       "      <th>lemma</th>\n",
       "      <th>POS</th>\n",
       "      <th>tok_lem_POS</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>alot</td>\n",
       "      <td>alot</td>\n",
       "      <td>NN</td>\n",
       "      <td>(alot, alot, NN)</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2704</th>\n",
       "      <td>defition</td>\n",
       "      <td>defition</td>\n",
       "      <td>NN</td>\n",
       "      <td>(defition, defition, NN)</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6647</th>\n",
       "      <td>nepotizm</td>\n",
       "      <td>nepotizm</td>\n",
       "      <td>NN</td>\n",
       "      <td>(nepotizm, nepotizm, NN)</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>apartament</td>\n",
       "      <td>apartament</td>\n",
       "      <td>NN</td>\n",
       "      <td>(apartament, apartament, NN)</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11036</th>\n",
       "      <td>yut</td>\n",
       "      <td>yut</td>\n",
       "      <td>NN</td>\n",
       "      <td>(yut, yut, NN)</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6462</th>\n",
       "      <td>mounths</td>\n",
       "      <td>mounths</td>\n",
       "      <td>NNS</td>\n",
       "      <td>(mounths, mounths, NNS)</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7210</th>\n",
       "      <td>peaple</td>\n",
       "      <td>peaple</td>\n",
       "      <td>NN</td>\n",
       "      <td>(peaple, peaple, NN)</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2947</th>\n",
       "      <td>differnt</td>\n",
       "      <td>differnt</td>\n",
       "      <td>NN</td>\n",
       "      <td>(differnt, differnt, NN)</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3880</th>\n",
       "      <td>experiance</td>\n",
       "      <td>experiance</td>\n",
       "      <td>NN</td>\n",
       "      <td>(experiance, experiance, NN)</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4508</th>\n",
       "      <td>geul</td>\n",
       "      <td>geul</td>\n",
       "      <td>NN</td>\n",
       "      <td>(geul, geul, NN)</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>189 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      misspelling       lemma  POS                   tok_lem_POS  freq\n",
       "439          alot        alot   NN              (alot, alot, NN)   173\n",
       "2704     defition    defition   NN      (defition, defition, NN)   128\n",
       "6647     nepotizm    nepotizm   NN      (nepotizm, nepotizm, NN)   126\n",
       "619    apartament  apartament   NN  (apartament, apartament, NN)   120\n",
       "11036         yut         yut   NN                (yut, yut, NN)    96\n",
       "...           ...         ...  ...                           ...   ...\n",
       "6462      mounths     mounths  NNS       (mounths, mounths, NNS)    10\n",
       "7210       peaple      peaple   NN          (peaple, peaple, NN)    10\n",
       "2947     differnt    differnt   NN      (differnt, differnt, NN)    10\n",
       "3880   experiance  experiance   NN  (experiance, experiance, NN)    10\n",
       "4508         geul        geul   NN              (geul, geul, NN)    10\n",
       "\n",
       "[189 rows x 5 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(misspell_df.loc[misspell_df.freq >= 10]))\n",
    "misspell_df.loc[misspell_df.freq >= 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acording',\n",
       " 'addtion',\n",
       " 'adventages',\n",
       " 'advertisment',\n",
       " 'airplan',\n",
       " 'alfter',\n",
       " 'alot',\n",
       " 'analytes',\n",
       " 'anoise',\n",
       " 'apartament',\n",
       " 'apartement',\n",
       " 'appartment',\n",
       " 'aquired',\n",
       " 'ar',\n",
       " 'aupair',\n",
       " 'auther',\n",
       " 'axé',\n",
       " 'beacuse',\n",
       " 'beatiful',\n",
       " 'becasue',\n",
       " 'becaue',\n",
       " 'becaus',\n",
       " 'becouse',\n",
       " 'becuase',\n",
       " 'befor',\n",
       " 'begining',\n",
       " 'belived',\n",
       " 'benefites',\n",
       " 'benifits',\n",
       " 'beutifull',\n",
       " 'bousporus',\n",
       " 'bridege',\n",
       " 'bridg',\n",
       " 'brillient',\n",
       " 'caffiene',\n",
       " 'caral',\n",
       " 'celefone',\n",
       " 'ch',\n",
       " 'chiken',\n",
       " 'childern',\n",
       " 'childrens',\n",
       " 'chres',\n",
       " 'chuan',\n",
       " 'citys',\n",
       " 'coffe',\n",
       " 'comercials',\n",
       " 'conclution',\n",
       " 'confortable',\n",
       " 'contry',\n",
       " 'countery',\n",
       " 'ddui',\n",
       " 'defition',\n",
       " 'dementi',\n",
       " 'deprission',\n",
       " 'desicion',\n",
       " 'devorce',\n",
       " 'diferent',\n",
       " 'differents',\n",
       " 'differnt',\n",
       " 'diffirent',\n",
       " 'diffrent',\n",
       " 'dopperganger',\n",
       " 'eatting',\n",
       " 'etat',\n",
       " 'eventhough',\n",
       " 'everythings',\n",
       " 'everytime',\n",
       " 'evry',\n",
       " 'experiance',\n",
       " 'experince',\n",
       " 'familly',\n",
       " 'famos',\n",
       " 'fastfood',\n",
       " 'favorate',\n",
       " 'favorit',\n",
       " 'forigners',\n",
       " 'freind',\n",
       " 'freinds',\n",
       " 'friens',\n",
       " 'frind',\n",
       " 'frinds',\n",
       " 'frisby',\n",
       " 'geol',\n",
       " 'ger',\n",
       " 'geul',\n",
       " 'gi',\n",
       " 'gliese',\n",
       " 'goverment',\n",
       " 'grammer',\n",
       " 'hc',\n",
       " 'healty',\n",
       " 'helth',\n",
       " 'heum',\n",
       " 'htm',\n",
       " 'iam',\n",
       " 'impotant',\n",
       " 'improtant',\n",
       " 'iss',\n",
       " 'jop',\n",
       " 'ke',\n",
       " 'languge',\n",
       " 'lifes',\n",
       " 'ludwing',\n",
       " 'marrige',\n",
       " 'monthes',\n",
       " 'mounths',\n",
       " 'neccessary',\n",
       " 'nepotizm',\n",
       " 'nowdays',\n",
       " 'nowruz',\n",
       " 'occured',\n",
       " 'ordor',\n",
       " 'peaple',\n",
       " 'peeters',\n",
       " 'pleace',\n",
       " 'poeple',\n",
       " 'polution',\n",
       " 'popluar',\n",
       " 'postive',\n",
       " 'preson',\n",
       " 'pronounciation',\n",
       " 'rac',\n",
       " 'rades',\n",
       " 'realy',\n",
       " 'recomand',\n",
       " 'recommond',\n",
       " 'referes',\n",
       " 'researchs',\n",
       " 'resons',\n",
       " 'responsabilities',\n",
       " 'restorant',\n",
       " 'restraunt',\n",
       " 'resturant',\n",
       " 'resturants',\n",
       " 'rissotto',\n",
       " 'roomate',\n",
       " 'rosling',\n",
       " 'scool',\n",
       " 'scound',\n",
       " 'securaty',\n",
       " 'sen',\n",
       " 'seng',\n",
       " 'sentance',\n",
       " 'shose',\n",
       " 'shoud',\n",
       " 'skrable',\n",
       " 'sociaty',\n",
       " 'somthing',\n",
       " 'sould',\n",
       " 'specialis',\n",
       " 'sterotypes',\n",
       " 'stors',\n",
       " 'studing',\n",
       " 'studints',\n",
       " 'sucessful',\n",
       " 'suger',\n",
       " 'sukwonable',\n",
       " 'synethesia',\n",
       " 'tfgp',\n",
       " 'ther',\n",
       " 'thes',\n",
       " 'tions',\n",
       " 'togather',\n",
       " 'tution',\n",
       " 'ubicate',\n",
       " 'unversity',\n",
       " 'usualy',\n",
       " 'vegeterian',\n",
       " 'vist',\n",
       " 'voiling',\n",
       " 'webholic',\n",
       " 'wonderfull',\n",
       " 'yut'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_rows = 1000\n",
    "print(len(set(misspell_df.loc[misspell_df.freq >= 10].misspelling.to_list())))\n",
    "set(misspell_df.loc[misspell_df.freq >= 10].misspelling.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying spelling correction\n",
    "\n",
    "Selected spell check: SymSpell https://github.com/mammothb/symspellpy\n",
    "\n",
    "In some ways symspell is not ideal as sentence context is not considered, only general frequencies. However, other well-known spellcheckers (hunspell, pyspell, etc.) use the same strategy - frequency based criteria for suggestions, without considering immediate cotext. As such, we have followed this common practice, but it is important to remember that accuracy of corrected tokens will not be 100% and must be taken into consideration.\n",
    "\n",
    "There is a dictionary file which which needs to be installed (saved to repo):\n",
    "[frequency_dictionary_en_82_765.txt](https://symspellpy.readthedocs.io/en/latest/users/installing.html)\n",
    "\n",
    "To install symspellpy the first time, use pip in command line: `pip install -U symspellpy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 23135851162),\n",
       " ('of', 13151942776),\n",
       " ('and', 12997637966),\n",
       " ('to', 12136980858),\n",
       " ('a', 9081174698)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting up symspell\n",
    "\n",
    "from itertools import islice\n",
    "import pkg_resources\n",
    "from symspellpy import SymSpell\n",
    "from symspellpy import Verbosity\n",
    "sym_spell = SymSpell()\n",
    "dictionary_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "sym_spell.load_dictionary(dictionary_path, 0, 1)\n",
    "\n",
    "# Print out first 5 elements to demonstrate that dictionary is successfully loaded\n",
    "list(islice(sym_spell.words.items(), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "because, 1, 271323986\n"
     ]
    }
   ],
   "source": [
    "# Testing with 'becuase'\n",
    "\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "dictionary_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "# term_index is the column of the term and count_index is the column of the term frequency\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "\n",
    "input_term = \"becuase\"\n",
    "suggestions = sym_spell.lookup(input_term, Verbosity.CLOSEST, max_edit_distance=2, #Edit distance can be adjusted\n",
    "                               transfer_casing=True, #Optional argument set to ignore case\n",
    "                              include_unknown=True) #Return same word if unknown\n",
    "for suggestion in suggestions:\n",
    "    print(suggestion)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating function for applying the above code\n",
    "\n",
    "def get_suggestions(word):\n",
    "    if len(word) >= 4:\n",
    "        suggestions = sym_spell.lookup(word, Verbosity.CLOSEST,max_edit_distance=2, transfer_casing=True)\n",
    "    else:\n",
    "        suggestions = sym_spell.lookup(word, Verbosity.CLOSEST,max_edit_distance=1, transfer_casing=True)\n",
    "    return [str(x).split(',') for x in suggestions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: The function has a variable edit distance: words of length 4 or more get edit distance of 2, shorter words get edit distance of 1. These preferences can be adjusted in the function if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>misspelling</th>\n",
       "      <th>lemma</th>\n",
       "      <th>POS</th>\n",
       "      <th>tok_lem_POS</th>\n",
       "      <th>freq</th>\n",
       "      <th>suggestions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>alot</td>\n",
       "      <td>alot</td>\n",
       "      <td>NN</td>\n",
       "      <td>(alot, alot, NN)</td>\n",
       "      <td>173</td>\n",
       "      <td>[[lot,  1,  106405208], [slot,  1,  21602762],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2704</th>\n",
       "      <td>defition</td>\n",
       "      <td>defition</td>\n",
       "      <td>NN</td>\n",
       "      <td>(defition, defition, NN)</td>\n",
       "      <td>128</td>\n",
       "      <td>[[edition,  2,  110051463], [decision,  2,  71...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6647</th>\n",
       "      <td>nepotizm</td>\n",
       "      <td>nepotizm</td>\n",
       "      <td>NN</td>\n",
       "      <td>(nepotizm, nepotizm, NN)</td>\n",
       "      <td>126</td>\n",
       "      <td>[[nepotism,  1,  174108]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>apartament</td>\n",
       "      <td>apartament</td>\n",
       "      <td>NN</td>\n",
       "      <td>(apartament, apartament, NN)</td>\n",
       "      <td>120</td>\n",
       "      <td>[[apartment,  1,  30771172]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11036</th>\n",
       "      <td>yut</td>\n",
       "      <td>yut</td>\n",
       "      <td>NN</td>\n",
       "      <td>(yut, yut, NN)</td>\n",
       "      <td>96</td>\n",
       "      <td>[[but,  1,  999899654], [out,  1,  741601852],...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      misspelling       lemma POS                   tok_lem_POS  freq  \\\n",
       "439          alot        alot  NN              (alot, alot, NN)   173   \n",
       "2704     defition    defition  NN      (defition, defition, NN)   128   \n",
       "6647     nepotizm    nepotizm  NN      (nepotizm, nepotizm, NN)   126   \n",
       "619    apartament  apartament  NN  (apartament, apartament, NN)   120   \n",
       "11036         yut         yut  NN                (yut, yut, NN)    96   \n",
       "\n",
       "                                             suggestions  \n",
       "439    [[lot,  1,  106405208], [slot,  1,  21602762],...  \n",
       "2704   [[edition,  2,  110051463], [decision,  2,  71...  \n",
       "6647                           [[nepotism,  1,  174108]]  \n",
       "619                         [[apartment,  1,  30771172]]  \n",
       "11036  [[but,  1,  999899654], [out,  1,  741601852],...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying function to create new column\n",
    "\n",
    "misspell_df['suggestions'] =  misspell_df['misspelling'].apply(get_suggestions)\n",
    "misspell_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "736"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(misspell_df.loc[(misspell_df.suggestions.str.len() == 0),:])\n",
    "\n",
    "#Items with no suggestions - these will be left in their original form though manual corrections could be applied if desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new column with just the most likely correction (based on frequency)\n",
    "\n",
    "misspell_df['correction'] = [x[0][0] if len(x) != 0 else np.NaN for x in misspell_df['suggestions']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If no correction, use original word\n",
    "\n",
    "misspell_df.correction.fillna(misspell_df.misspelling, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>misspelling</th>\n",
       "      <th>lemma</th>\n",
       "      <th>POS</th>\n",
       "      <th>tok_lem_POS</th>\n",
       "      <th>freq</th>\n",
       "      <th>suggestions</th>\n",
       "      <th>correction</th>\n",
       "      <th>correction_POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>alot</td>\n",
       "      <td>alot</td>\n",
       "      <td>NN</td>\n",
       "      <td>(alot, alot, NN)</td>\n",
       "      <td>173</td>\n",
       "      <td>[[lot,  1,  106405208], [slot,  1,  21602762],...</td>\n",
       "      <td>lot</td>\n",
       "      <td>(lot, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2704</th>\n",
       "      <td>defition</td>\n",
       "      <td>defition</td>\n",
       "      <td>NN</td>\n",
       "      <td>(defition, defition, NN)</td>\n",
       "      <td>128</td>\n",
       "      <td>[[edition,  2,  110051463], [decision,  2,  71...</td>\n",
       "      <td>edition</td>\n",
       "      <td>(edition, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6647</th>\n",
       "      <td>nepotizm</td>\n",
       "      <td>nepotizm</td>\n",
       "      <td>NN</td>\n",
       "      <td>(nepotizm, nepotizm, NN)</td>\n",
       "      <td>126</td>\n",
       "      <td>[[nepotism,  1,  174108]]</td>\n",
       "      <td>nepotism</td>\n",
       "      <td>(nepotism, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>apartament</td>\n",
       "      <td>apartament</td>\n",
       "      <td>NN</td>\n",
       "      <td>(apartament, apartament, NN)</td>\n",
       "      <td>120</td>\n",
       "      <td>[[apartment,  1,  30771172]]</td>\n",
       "      <td>apartment</td>\n",
       "      <td>(apartment, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11036</th>\n",
       "      <td>yut</td>\n",
       "      <td>yut</td>\n",
       "      <td>NN</td>\n",
       "      <td>(yut, yut, NN)</td>\n",
       "      <td>96</td>\n",
       "      <td>[[but,  1,  999899654], [out,  1,  741601852],...</td>\n",
       "      <td>but</td>\n",
       "      <td>(but, NN)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      misspelling       lemma POS                   tok_lem_POS  freq  \\\n",
       "439          alot        alot  NN              (alot, alot, NN)   173   \n",
       "2704     defition    defition  NN      (defition, defition, NN)   128   \n",
       "6647     nepotizm    nepotizm  NN      (nepotizm, nepotizm, NN)   126   \n",
       "619    apartament  apartament  NN  (apartament, apartament, NN)   120   \n",
       "11036         yut         yut  NN                (yut, yut, NN)    96   \n",
       "\n",
       "                                             suggestions correction  \\\n",
       "439    [[lot,  1,  106405208], [slot,  1,  21602762],...        lot   \n",
       "2704   [[edition,  2,  110051463], [decision,  2,  71...    edition   \n",
       "6647                           [[nepotism,  1,  174108]]   nepotism   \n",
       "619                         [[apartment,  1,  30771172]]  apartment   \n",
       "11036  [[but,  1,  999899654], [out,  1,  741601852],...        but   \n",
       "\n",
       "        correction_POS  \n",
       "439          (lot, NN)  \n",
       "2704     (edition, NN)  \n",
       "6647    (nepotism, NN)  \n",
       "619    (apartment, NN)  \n",
       "11036        (but, NN)  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create correction_POS column\n",
    "\n",
    "misspell_df['correction_POS'] = list(zip(misspell_df.correction, misspell_df.POS))\n",
    "misspell_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>misspelling</th>\n",
       "      <th>lemma</th>\n",
       "      <th>POS</th>\n",
       "      <th>tok_lem_POS</th>\n",
       "      <th>freq</th>\n",
       "      <th>suggestions</th>\n",
       "      <th>correction</th>\n",
       "      <th>correction_POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4454</th>\n",
       "      <td>garaoke</td>\n",
       "      <td>garaoke</td>\n",
       "      <td>NN</td>\n",
       "      <td>(garaoke, garaoke, NN)</td>\n",
       "      <td>2</td>\n",
       "      <td>[[karaoke,  1,  7568648]]</td>\n",
       "      <td>karaoke</td>\n",
       "      <td>(karaoke, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9203</th>\n",
       "      <td>squerilhill</td>\n",
       "      <td>squerilhill</td>\n",
       "      <td>NN</td>\n",
       "      <td>(squerilhill, squerilhill, NN)</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>squerilhill</td>\n",
       "      <td>(squerilhill, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8558</th>\n",
       "      <td>schildren</td>\n",
       "      <td>schildren</td>\n",
       "      <td>NN</td>\n",
       "      <td>(schildren, schildren, NN)</td>\n",
       "      <td>1</td>\n",
       "      <td>[[children,  1,  206538107]]</td>\n",
       "      <td>children</td>\n",
       "      <td>(children, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7201</th>\n",
       "      <td>pdf</td>\n",
       "      <td>pdf</td>\n",
       "      <td>VB</td>\n",
       "      <td>(pdf, pdf, VB)</td>\n",
       "      <td>2</td>\n",
       "      <td>[[psf,  1,  950636]]</td>\n",
       "      <td>psf</td>\n",
       "      <td>(psf, VB)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9496</th>\n",
       "      <td>supermarke</td>\n",
       "      <td>supermarke</td>\n",
       "      <td>NN</td>\n",
       "      <td>(supermarke, supermarke, NN)</td>\n",
       "      <td>1</td>\n",
       "      <td>[[supermarket,  1,  2885844]]</td>\n",
       "      <td>supermarket</td>\n",
       "      <td>(supermarket, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1881</th>\n",
       "      <td>citicen</td>\n",
       "      <td>citicen</td>\n",
       "      <td>JJ</td>\n",
       "      <td>(citicen, citicen, JJ)</td>\n",
       "      <td>1</td>\n",
       "      <td>[[citizen,  1,  16245917]]</td>\n",
       "      <td>citizen</td>\n",
       "      <td>(citizen, JJ)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>assignement</td>\n",
       "      <td>assignement</td>\n",
       "      <td>NN</td>\n",
       "      <td>(assignement, assignement, NN)</td>\n",
       "      <td>1</td>\n",
       "      <td>[[assignment,  1,  14722710]]</td>\n",
       "      <td>assignment</td>\n",
       "      <td>(assignment, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8734</th>\n",
       "      <td>seris</td>\n",
       "      <td>seris</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>(seris, seris, VBZ)</td>\n",
       "      <td>1</td>\n",
       "      <td>[[series,  1,  161518557], [serif,  1,  210933...</td>\n",
       "      <td>series</td>\n",
       "      <td>(series, VBZ)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5350</th>\n",
       "      <td>inpiration</td>\n",
       "      <td>inpiration</td>\n",
       "      <td>NN</td>\n",
       "      <td>(inpiration, inpiration, NN)</td>\n",
       "      <td>1</td>\n",
       "      <td>[[inspiration,  1,  8460625]]</td>\n",
       "      <td>inspiration</td>\n",
       "      <td>(inspiration, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3516</th>\n",
       "      <td>enjouy</td>\n",
       "      <td>enjouy</td>\n",
       "      <td>VBP</td>\n",
       "      <td>(enjouy, enjouy, VBP)</td>\n",
       "      <td>2</td>\n",
       "      <td>[[enjoy,  1,  50141455]]</td>\n",
       "      <td>enjoy</td>\n",
       "      <td>(enjoy, VBP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>colleauges</td>\n",
       "      <td>colleauges</td>\n",
       "      <td>NNS</td>\n",
       "      <td>(colleauges, colleauges, NNS)</td>\n",
       "      <td>1</td>\n",
       "      <td>[[colleagues,  1,  9762059]]</td>\n",
       "      <td>colleagues</td>\n",
       "      <td>(colleagues, NNS)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1177</th>\n",
       "      <td>behaivior</td>\n",
       "      <td>behaivior</td>\n",
       "      <td>NN</td>\n",
       "      <td>(behaivior, behaivior, NN)</td>\n",
       "      <td>1</td>\n",
       "      <td>[[behaviour,  2,  14175567]]</td>\n",
       "      <td>behaviour</td>\n",
       "      <td>(behaviour, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2500</th>\n",
       "      <td>critisized</td>\n",
       "      <td>critisized</td>\n",
       "      <td>VBN</td>\n",
       "      <td>(critisized, critisized, VBN)</td>\n",
       "      <td>1</td>\n",
       "      <td>[[criticized,  1,  2022870]]</td>\n",
       "      <td>criticized</td>\n",
       "      <td>(criticized, VBN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>airplain</td>\n",
       "      <td>airplain</td>\n",
       "      <td>NN</td>\n",
       "      <td>(airplain, airplain, NN)</td>\n",
       "      <td>1</td>\n",
       "      <td>[[airplay,  2,  610851]]</td>\n",
       "      <td>airplay</td>\n",
       "      <td>(airplay, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1848</th>\n",
       "      <td>chul</td>\n",
       "      <td>chul</td>\n",
       "      <td>NN</td>\n",
       "      <td>(chul, chul, NN)</td>\n",
       "      <td>1</td>\n",
       "      <td>[[chum,  1,  578060], [chub,  1,  271882], [ch...</td>\n",
       "      <td>chum</td>\n",
       "      <td>(chum, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4619</th>\n",
       "      <td>grandfahter</td>\n",
       "      <td>grandfahter</td>\n",
       "      <td>NN</td>\n",
       "      <td>(grandfahter, grandfahter, NN)</td>\n",
       "      <td>4</td>\n",
       "      <td>[[grandfather,  1,  3976363]]</td>\n",
       "      <td>grandfather</td>\n",
       "      <td>(grandfather, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5942</th>\n",
       "      <td>linessed</td>\n",
       "      <td>linessed</td>\n",
       "      <td>VBD</td>\n",
       "      <td>(linessed, linessed, VBD)</td>\n",
       "      <td>1</td>\n",
       "      <td>[[finessed,  1,  20547]]</td>\n",
       "      <td>finessed</td>\n",
       "      <td>(finessed, VBD)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6762</th>\n",
       "      <td>nowruz</td>\n",
       "      <td>nowruz</td>\n",
       "      <td>JJ</td>\n",
       "      <td>(nowruz, nowruz, JJ)</td>\n",
       "      <td>17</td>\n",
       "      <td>[]</td>\n",
       "      <td>nowruz</td>\n",
       "      <td>(nowruz, JJ)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9749</th>\n",
       "      <td>terribl</td>\n",
       "      <td>terribl</td>\n",
       "      <td>JJ</td>\n",
       "      <td>(terribl, terribl, JJ)</td>\n",
       "      <td>1</td>\n",
       "      <td>[[terrible,  1,  8610277], [terribly,  1,  196...</td>\n",
       "      <td>terrible</td>\n",
       "      <td>(terrible, JJ)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10462</th>\n",
       "      <td>usus</td>\n",
       "      <td>usus</td>\n",
       "      <td>JJ</td>\n",
       "      <td>(usus, usus, JJ)</td>\n",
       "      <td>1</td>\n",
       "      <td>[[uses,  1,  55842334], [usu,  1,  421601], [u...</td>\n",
       "      <td>uses</td>\n",
       "      <td>(uses, JJ)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2156</th>\n",
       "      <td>compplex</td>\n",
       "      <td>compplex</td>\n",
       "      <td>JJ</td>\n",
       "      <td>(compplex, compplex, JJ)</td>\n",
       "      <td>1</td>\n",
       "      <td>[[complex,  1,  46865979]]</td>\n",
       "      <td>complex</td>\n",
       "      <td>(complex, JJ)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9049</th>\n",
       "      <td>somethink</td>\n",
       "      <td>somethink</td>\n",
       "      <td>VB</td>\n",
       "      <td>(somethink, somethink, VB)</td>\n",
       "      <td>1</td>\n",
       "      <td>[[something,  1,  131836210]]</td>\n",
       "      <td>something</td>\n",
       "      <td>(something, VB)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7118</th>\n",
       "      <td>paragragraph</td>\n",
       "      <td>paragragraph</td>\n",
       "      <td>NN</td>\n",
       "      <td>(paragragraph, paragragraph, NN)</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>paragragraph</td>\n",
       "      <td>(paragragraph, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8653</th>\n",
       "      <td>sedfood</td>\n",
       "      <td>sedfood</td>\n",
       "      <td>NN</td>\n",
       "      <td>(sedfood, sedfood, NN)</td>\n",
       "      <td>1</td>\n",
       "      <td>[[seafood,  1,  7525177]]</td>\n",
       "      <td>seafood</td>\n",
       "      <td>(seafood, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7731</th>\n",
       "      <td>professior</td>\n",
       "      <td>professior</td>\n",
       "      <td>NN</td>\n",
       "      <td>(professior, professior, NN)</td>\n",
       "      <td>1</td>\n",
       "      <td>[[professor,  1,  40572636], [profession,  1, ...</td>\n",
       "      <td>professor</td>\n",
       "      <td>(professor, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8473</th>\n",
       "      <td>sahimi</td>\n",
       "      <td>sahimi</td>\n",
       "      <td>NN</td>\n",
       "      <td>(sahimi, sahimi, NN)</td>\n",
       "      <td>1</td>\n",
       "      <td>[[tahiti,  2,  1715144], [swahili,  2,  113593...</td>\n",
       "      <td>tahiti</td>\n",
       "      <td>(tahiti, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>famili</td>\n",
       "      <td>famili</td>\n",
       "      <td>NN</td>\n",
       "      <td>(famili, famili, NN)</td>\n",
       "      <td>1</td>\n",
       "      <td>[[family,  1,  254164055]]</td>\n",
       "      <td>family</td>\n",
       "      <td>(family, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>advertasment</td>\n",
       "      <td>advertasment</td>\n",
       "      <td>NN</td>\n",
       "      <td>(advertasment, advertasment, NN)</td>\n",
       "      <td>1</td>\n",
       "      <td>[[advertisement,  2,  34681267]]</td>\n",
       "      <td>advertisement</td>\n",
       "      <td>(advertisement, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2550</th>\n",
       "      <td>cumbe</td>\n",
       "      <td>cumbe</td>\n",
       "      <td>NN</td>\n",
       "      <td>(cumbe, cumbe, NN)</td>\n",
       "      <td>1</td>\n",
       "      <td>[[cube,  1,  7200844], [combe,  1,  282561], [...</td>\n",
       "      <td>cube</td>\n",
       "      <td>(cube, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8430</th>\n",
       "      <td>rsource</td>\n",
       "      <td>rsource</td>\n",
       "      <td>NN</td>\n",
       "      <td>(rsource, rsource, NN)</td>\n",
       "      <td>1</td>\n",
       "      <td>[[source,  1,  179963886], [resource,  1,  999...</td>\n",
       "      <td>source</td>\n",
       "      <td>(source, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6815</th>\n",
       "      <td>obsorption</td>\n",
       "      <td>obsorption</td>\n",
       "      <td>NN</td>\n",
       "      <td>(obsorption, obsorption, NN)</td>\n",
       "      <td>1</td>\n",
       "      <td>[[absorption,  1,  5294724]]</td>\n",
       "      <td>absorption</td>\n",
       "      <td>(absorption, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5789</th>\n",
       "      <td>langurge</td>\n",
       "      <td>langurge</td>\n",
       "      <td>NN</td>\n",
       "      <td>(langurge, langurge, NN)</td>\n",
       "      <td>1</td>\n",
       "      <td>[[language,  1,  138517992]]</td>\n",
       "      <td>language</td>\n",
       "      <td>(language, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10731</th>\n",
       "      <td>wheather</td>\n",
       "      <td>wheather</td>\n",
       "      <td>NN</td>\n",
       "      <td>(wheather, wheather, NN)</td>\n",
       "      <td>1</td>\n",
       "      <td>[[whether,  1,  105014961], [weather,  1,  103...</td>\n",
       "      <td>whether</td>\n",
       "      <td>(whether, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6940</th>\n",
       "      <td>opivusly</td>\n",
       "      <td>opivusly</td>\n",
       "      <td>RB</td>\n",
       "      <td>(opivusly, opivusly, RB)</td>\n",
       "      <td>1</td>\n",
       "      <td>[[copiously,  2,  58356], [piously,  2,  42369]]</td>\n",
       "      <td>copiously</td>\n",
       "      <td>(copiously, RB)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>additition</td>\n",
       "      <td>additition</td>\n",
       "      <td>NN</td>\n",
       "      <td>(additition, additition, NN)</td>\n",
       "      <td>2</td>\n",
       "      <td>[[addition,  2,  73434482], [addiction,  2,  9...</td>\n",
       "      <td>addition</td>\n",
       "      <td>(addition, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>colesterol</td>\n",
       "      <td>colesterol</td>\n",
       "      <td>NN</td>\n",
       "      <td>(colesterol, colesterol, NN)</td>\n",
       "      <td>1</td>\n",
       "      <td>[[cholesterol,  1,  8620700]]</td>\n",
       "      <td>cholesterol</td>\n",
       "      <td>(cholesterol, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9126</th>\n",
       "      <td>spce</td>\n",
       "      <td>spce</td>\n",
       "      <td>NN</td>\n",
       "      <td>(spce, spce, NN)</td>\n",
       "      <td>1</td>\n",
       "      <td>[[space,  1,  121505269], [spec,  1,  9669417]...</td>\n",
       "      <td>space</td>\n",
       "      <td>(space, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1461</th>\n",
       "      <td>buld</td>\n",
       "      <td>buld</td>\n",
       "      <td>VB</td>\n",
       "      <td>(buld, buld, VB)</td>\n",
       "      <td>1</td>\n",
       "      <td>[[build,  1,  77203075], [bold,  1,  18244416]...</td>\n",
       "      <td>build</td>\n",
       "      <td>(build, VB)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9552</th>\n",
       "      <td>sutable</td>\n",
       "      <td>sutable</td>\n",
       "      <td>JJ</td>\n",
       "      <td>(sutable, sutable, JJ)</td>\n",
       "      <td>3</td>\n",
       "      <td>[[suitable,  1,  20824374], [stable,  1,  1822...</td>\n",
       "      <td>suitable</td>\n",
       "      <td>(suitable, JJ)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1978</th>\n",
       "      <td>cm</td>\n",
       "      <td>cm</td>\n",
       "      <td>JJ</td>\n",
       "      <td>(cm, cm, JJ)</td>\n",
       "      <td>4</td>\n",
       "      <td>[[pm,  1,  604577485], [am,  1,  576436203], [...</td>\n",
       "      <td>pm</td>\n",
       "      <td>(pm, JJ)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5451</th>\n",
       "      <td>interupt</td>\n",
       "      <td>interupt</td>\n",
       "      <td>VB</td>\n",
       "      <td>(interupt, interupt, VB)</td>\n",
       "      <td>2</td>\n",
       "      <td>[[interrupt,  1,  3979422]]</td>\n",
       "      <td>interrupt</td>\n",
       "      <td>(interrupt, VB)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2299</th>\n",
       "      <td>contenue</td>\n",
       "      <td>contenue</td>\n",
       "      <td>VB</td>\n",
       "      <td>(contenue, contenue, VB)</td>\n",
       "      <td>2</td>\n",
       "      <td>[[continue,  1,  76659163]]</td>\n",
       "      <td>continue</td>\n",
       "      <td>(continue, VB)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5017</th>\n",
       "      <td>humborger</td>\n",
       "      <td>humborger</td>\n",
       "      <td>NN</td>\n",
       "      <td>(humborger, humborger, NN)</td>\n",
       "      <td>1</td>\n",
       "      <td>[[hamburger,  2,  1453555]]</td>\n",
       "      <td>hamburger</td>\n",
       "      <td>(hamburger, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7674</th>\n",
       "      <td>presusre</td>\n",
       "      <td>presusre</td>\n",
       "      <td>NN</td>\n",
       "      <td>(presusre, presusre, NN)</td>\n",
       "      <td>2</td>\n",
       "      <td>[[pressure,  1,  51407261]]</td>\n",
       "      <td>pressure</td>\n",
       "      <td>(pressure, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3896</th>\n",
       "      <td>expiences</td>\n",
       "      <td>expiences</td>\n",
       "      <td>NNS</td>\n",
       "      <td>(expiences, expiences, NNS)</td>\n",
       "      <td>2</td>\n",
       "      <td>[[expenses,  2,  24251012], [experiences,  2, ...</td>\n",
       "      <td>expenses</td>\n",
       "      <td>(expenses, NNS)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4764</th>\n",
       "      <td>happent</td>\n",
       "      <td>happent</td>\n",
       "      <td>NN</td>\n",
       "      <td>(happent, happent, NN)</td>\n",
       "      <td>1</td>\n",
       "      <td>[[happen,  1,  27207887], [happens,  1,  20031...</td>\n",
       "      <td>happen</td>\n",
       "      <td>(happen, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3611</th>\n",
       "      <td>espacial</td>\n",
       "      <td>espacial</td>\n",
       "      <td>JJ</td>\n",
       "      <td>(espacial, espacial, JJ)</td>\n",
       "      <td>1</td>\n",
       "      <td>[[especial,  1,  557190], [spacial,  1,  99568]]</td>\n",
       "      <td>especial</td>\n",
       "      <td>(especial, JJ)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1659</th>\n",
       "      <td>celver</td>\n",
       "      <td>celver</td>\n",
       "      <td>NN</td>\n",
       "      <td>(celver, celver, NN)</td>\n",
       "      <td>1</td>\n",
       "      <td>[[clever,  1,  4865097], [culver,  1,  1376258...</td>\n",
       "      <td>clever</td>\n",
       "      <td>(clever, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5077</th>\n",
       "      <td>ichi</td>\n",
       "      <td>ichi</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>(ichi, ichi, VBZ)</td>\n",
       "      <td>1</td>\n",
       "      <td>[[chi,  1,  10981165]]</td>\n",
       "      <td>chi</td>\n",
       "      <td>(chi, VBZ)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1855</th>\n",
       "      <td>ciega</td>\n",
       "      <td>ciega</td>\n",
       "      <td>JJ</td>\n",
       "      <td>(ciega, ciega, JJ)</td>\n",
       "      <td>1</td>\n",
       "      <td>[[diego,  2,  32238555], [cinema,  2,  1817938...</td>\n",
       "      <td>diego</td>\n",
       "      <td>(diego, JJ)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        misspelling         lemma  POS                       tok_lem_POS  \\\n",
       "4454        garaoke       garaoke   NN            (garaoke, garaoke, NN)   \n",
       "9203    squerilhill   squerilhill   NN    (squerilhill, squerilhill, NN)   \n",
       "8558      schildren     schildren   NN        (schildren, schildren, NN)   \n",
       "7201            pdf           pdf   VB                    (pdf, pdf, VB)   \n",
       "9496     supermarke    supermarke   NN      (supermarke, supermarke, NN)   \n",
       "1881        citicen       citicen   JJ            (citicen, citicen, JJ)   \n",
       "810     assignement   assignement   NN    (assignement, assignement, NN)   \n",
       "8734          seris         seris  VBZ               (seris, seris, VBZ)   \n",
       "5350     inpiration    inpiration   NN      (inpiration, inpiration, NN)   \n",
       "3516         enjouy        enjouy  VBP             (enjouy, enjouy, VBP)   \n",
       "2016     colleauges    colleauges  NNS     (colleauges, colleauges, NNS)   \n",
       "1177      behaivior     behaivior   NN        (behaivior, behaivior, NN)   \n",
       "2500     critisized    critisized  VBN     (critisized, critisized, VBN)   \n",
       "367        airplain      airplain   NN          (airplain, airplain, NN)   \n",
       "1848           chul          chul   NN                  (chul, chul, NN)   \n",
       "4619    grandfahter   grandfahter   NN    (grandfahter, grandfahter, NN)   \n",
       "5942       linessed      linessed  VBD         (linessed, linessed, VBD)   \n",
       "6762         nowruz        nowruz   JJ              (nowruz, nowruz, JJ)   \n",
       "9749        terribl       terribl   JJ            (terribl, terribl, JJ)   \n",
       "10462          usus          usus   JJ                  (usus, usus, JJ)   \n",
       "2156       compplex      compplex   JJ          (compplex, compplex, JJ)   \n",
       "9049      somethink     somethink   VB        (somethink, somethink, VB)   \n",
       "7118   paragragraph  paragragraph   NN  (paragragraph, paragragraph, NN)   \n",
       "8653        sedfood       sedfood   NN            (sedfood, sedfood, NN)   \n",
       "7731     professior    professior   NN      (professior, professior, NN)   \n",
       "8473         sahimi        sahimi   NN              (sahimi, sahimi, NN)   \n",
       "3995         famili        famili   NN              (famili, famili, NN)   \n",
       "272    advertasment  advertasment   NN  (advertasment, advertasment, NN)   \n",
       "2550          cumbe         cumbe   NN                (cumbe, cumbe, NN)   \n",
       "8430        rsource       rsource   NN            (rsource, rsource, NN)   \n",
       "6815     obsorption    obsorption   NN      (obsorption, obsorption, NN)   \n",
       "5789       langurge      langurge   NN          (langurge, langurge, NN)   \n",
       "10731      wheather      wheather   NN          (wheather, wheather, NN)   \n",
       "6940       opivusly      opivusly   RB          (opivusly, opivusly, RB)   \n",
       "213      additition    additition   NN      (additition, additition, NN)   \n",
       "2011     colesterol    colesterol   NN      (colesterol, colesterol, NN)   \n",
       "9126           spce          spce   NN                  (spce, spce, NN)   \n",
       "1461           buld          buld   VB                  (buld, buld, VB)   \n",
       "9552        sutable       sutable   JJ            (sutable, sutable, JJ)   \n",
       "1978             cm            cm   JJ                      (cm, cm, JJ)   \n",
       "5451       interupt      interupt   VB          (interupt, interupt, VB)   \n",
       "2299       contenue      contenue   VB          (contenue, contenue, VB)   \n",
       "5017      humborger     humborger   NN        (humborger, humborger, NN)   \n",
       "7674       presusre      presusre   NN          (presusre, presusre, NN)   \n",
       "3896      expiences     expiences  NNS       (expiences, expiences, NNS)   \n",
       "4764        happent       happent   NN            (happent, happent, NN)   \n",
       "3611       espacial      espacial   JJ          (espacial, espacial, JJ)   \n",
       "1659         celver        celver   NN              (celver, celver, NN)   \n",
       "5077           ichi          ichi  VBZ                 (ichi, ichi, VBZ)   \n",
       "1855          ciega         ciega   JJ                (ciega, ciega, JJ)   \n",
       "\n",
       "       freq                                        suggestions     correction  \\\n",
       "4454      2                          [[karaoke,  1,  7568648]]        karaoke   \n",
       "9203      1                                                 []    squerilhill   \n",
       "8558      1                       [[children,  1,  206538107]]       children   \n",
       "7201      2                               [[psf,  1,  950636]]            psf   \n",
       "9496      1                      [[supermarket,  1,  2885844]]    supermarket   \n",
       "1881      1                         [[citizen,  1,  16245917]]        citizen   \n",
       "810       1                      [[assignment,  1,  14722710]]     assignment   \n",
       "8734      1  [[series,  1,  161518557], [serif,  1,  210933...         series   \n",
       "5350      1                      [[inspiration,  1,  8460625]]    inspiration   \n",
       "3516      2                           [[enjoy,  1,  50141455]]          enjoy   \n",
       "2016      1                       [[colleagues,  1,  9762059]]     colleagues   \n",
       "1177      1                       [[behaviour,  2,  14175567]]      behaviour   \n",
       "2500      1                       [[criticized,  1,  2022870]]     criticized   \n",
       "367       1                           [[airplay,  2,  610851]]        airplay   \n",
       "1848      1  [[chum,  1,  578060], [chub,  1,  271882], [ch...           chum   \n",
       "4619      4                      [[grandfather,  1,  3976363]]    grandfather   \n",
       "5942      1                           [[finessed,  1,  20547]]       finessed   \n",
       "6762     17                                                 []         nowruz   \n",
       "9749      1  [[terrible,  1,  8610277], [terribly,  1,  196...       terrible   \n",
       "10462     1  [[uses,  1,  55842334], [usu,  1,  421601], [u...           uses   \n",
       "2156      1                         [[complex,  1,  46865979]]        complex   \n",
       "9049      1                      [[something,  1,  131836210]]      something   \n",
       "7118      1                                                 []   paragragraph   \n",
       "8653      1                          [[seafood,  1,  7525177]]        seafood   \n",
       "7731      1  [[professor,  1,  40572636], [profession,  1, ...      professor   \n",
       "8473      1  [[tahiti,  2,  1715144], [swahili,  2,  113593...         tahiti   \n",
       "3995      1                         [[family,  1,  254164055]]         family   \n",
       "272       1                   [[advertisement,  2,  34681267]]  advertisement   \n",
       "2550      1  [[cube,  1,  7200844], [combe,  1,  282561], [...           cube   \n",
       "8430      1  [[source,  1,  179963886], [resource,  1,  999...         source   \n",
       "6815      1                       [[absorption,  1,  5294724]]     absorption   \n",
       "5789      1                       [[language,  1,  138517992]]       language   \n",
       "10731     1  [[whether,  1,  105014961], [weather,  1,  103...        whether   \n",
       "6940      1   [[copiously,  2,  58356], [piously,  2,  42369]]      copiously   \n",
       "213       2  [[addition,  2,  73434482], [addiction,  2,  9...       addition   \n",
       "2011      1                      [[cholesterol,  1,  8620700]]    cholesterol   \n",
       "9126      1  [[space,  1,  121505269], [spec,  1,  9669417]...          space   \n",
       "1461      1  [[build,  1,  77203075], [bold,  1,  18244416]...          build   \n",
       "9552      3  [[suitable,  1,  20824374], [stable,  1,  1822...       suitable   \n",
       "1978      4  [[pm,  1,  604577485], [am,  1,  576436203], [...             pm   \n",
       "5451      2                        [[interrupt,  1,  3979422]]      interrupt   \n",
       "2299      2                        [[continue,  1,  76659163]]       continue   \n",
       "5017      1                        [[hamburger,  2,  1453555]]      hamburger   \n",
       "7674      2                        [[pressure,  1,  51407261]]       pressure   \n",
       "3896      2  [[expenses,  2,  24251012], [experiences,  2, ...       expenses   \n",
       "4764      1  [[happen,  1,  27207887], [happens,  1,  20031...         happen   \n",
       "3611      1   [[especial,  1,  557190], [spacial,  1,  99568]]       especial   \n",
       "1659      1  [[clever,  1,  4865097], [culver,  1,  1376258...         clever   \n",
       "5077      1                             [[chi,  1,  10981165]]            chi   \n",
       "1855      1  [[diego,  2,  32238555], [cinema,  2,  1817938...          diego   \n",
       "\n",
       "            correction_POS  \n",
       "4454         (karaoke, NN)  \n",
       "9203     (squerilhill, NN)  \n",
       "8558        (children, NN)  \n",
       "7201             (psf, VB)  \n",
       "9496     (supermarket, NN)  \n",
       "1881         (citizen, JJ)  \n",
       "810       (assignment, NN)  \n",
       "8734         (series, VBZ)  \n",
       "5350     (inspiration, NN)  \n",
       "3516          (enjoy, VBP)  \n",
       "2016     (colleagues, NNS)  \n",
       "1177       (behaviour, NN)  \n",
       "2500     (criticized, VBN)  \n",
       "367          (airplay, NN)  \n",
       "1848            (chum, NN)  \n",
       "4619     (grandfather, NN)  \n",
       "5942       (finessed, VBD)  \n",
       "6762          (nowruz, JJ)  \n",
       "9749        (terrible, JJ)  \n",
       "10462           (uses, JJ)  \n",
       "2156         (complex, JJ)  \n",
       "9049       (something, VB)  \n",
       "7118    (paragragraph, NN)  \n",
       "8653         (seafood, NN)  \n",
       "7731       (professor, NN)  \n",
       "8473          (tahiti, NN)  \n",
       "3995          (family, NN)  \n",
       "272    (advertisement, NN)  \n",
       "2550            (cube, NN)  \n",
       "8430          (source, NN)  \n",
       "6815      (absorption, NN)  \n",
       "5789        (language, NN)  \n",
       "10731        (whether, NN)  \n",
       "6940       (copiously, RB)  \n",
       "213         (addition, NN)  \n",
       "2011     (cholesterol, NN)  \n",
       "9126           (space, NN)  \n",
       "1461           (build, VB)  \n",
       "9552        (suitable, JJ)  \n",
       "1978              (pm, JJ)  \n",
       "5451       (interrupt, VB)  \n",
       "2299        (continue, VB)  \n",
       "5017       (hamburger, NN)  \n",
       "7674        (pressure, NN)  \n",
       "3896       (expenses, NNS)  \n",
       "4764          (happen, NN)  \n",
       "3611        (especial, JJ)  \n",
       "1659          (clever, NN)  \n",
       "5077            (chi, VBZ)  \n",
       "1855           (diego, JJ)  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misspell_df.sort_values(by=['freq'], ascending=False).sample(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incorporating corrections into `answer_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11084\n",
      "10239\n"
     ]
    }
   ],
   "source": [
    "# First removing items from misspell_df where no correction will take place\n",
    "\n",
    "print(len(misspell_df))\n",
    "misspell_df = misspell_df.loc[misspell_df.misspelling != misspell_df.correction]\n",
    "print(len(misspell_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>misspelling</th>\n",
       "      <th>lemma</th>\n",
       "      <th>POS</th>\n",
       "      <th>tok_lem_POS</th>\n",
       "      <th>freq</th>\n",
       "      <th>suggestions</th>\n",
       "      <th>correction</th>\n",
       "      <th>correction_POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>alot</td>\n",
       "      <td>alot</td>\n",
       "      <td>NN</td>\n",
       "      <td>(alot, alot, NN)</td>\n",
       "      <td>173</td>\n",
       "      <td>[[lot,  1,  106405208], [slot,  1,  21602762],...</td>\n",
       "      <td>lot</td>\n",
       "      <td>(lot, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2704</th>\n",
       "      <td>defition</td>\n",
       "      <td>defition</td>\n",
       "      <td>NN</td>\n",
       "      <td>(defition, defition, NN)</td>\n",
       "      <td>128</td>\n",
       "      <td>[[edition,  2,  110051463], [decision,  2,  71...</td>\n",
       "      <td>edition</td>\n",
       "      <td>(edition, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6647</th>\n",
       "      <td>nepotizm</td>\n",
       "      <td>nepotizm</td>\n",
       "      <td>NN</td>\n",
       "      <td>(nepotizm, nepotizm, NN)</td>\n",
       "      <td>126</td>\n",
       "      <td>[[nepotism,  1,  174108]]</td>\n",
       "      <td>nepotism</td>\n",
       "      <td>(nepotism, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>apartament</td>\n",
       "      <td>apartament</td>\n",
       "      <td>NN</td>\n",
       "      <td>(apartament, apartament, NN)</td>\n",
       "      <td>120</td>\n",
       "      <td>[[apartment,  1,  30771172]]</td>\n",
       "      <td>apartment</td>\n",
       "      <td>(apartment, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11036</th>\n",
       "      <td>yut</td>\n",
       "      <td>yut</td>\n",
       "      <td>NN</td>\n",
       "      <td>(yut, yut, NN)</td>\n",
       "      <td>96</td>\n",
       "      <td>[[but,  1,  999899654], [out,  1,  741601852],...</td>\n",
       "      <td>but</td>\n",
       "      <td>(but, NN)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      misspelling       lemma POS                   tok_lem_POS  freq  \\\n",
       "439          alot        alot  NN              (alot, alot, NN)   173   \n",
       "2704     defition    defition  NN      (defition, defition, NN)   128   \n",
       "6647     nepotizm    nepotizm  NN      (nepotizm, nepotizm, NN)   126   \n",
       "619    apartament  apartament  NN  (apartament, apartament, NN)   120   \n",
       "11036         yut         yut  NN                (yut, yut, NN)    96   \n",
       "\n",
       "                                             suggestions correction  \\\n",
       "439    [[lot,  1,  106405208], [slot,  1,  21602762],...        lot   \n",
       "2704   [[edition,  2,  110051463], [decision,  2,  71...    edition   \n",
       "6647                           [[nepotism,  1,  174108]]   nepotism   \n",
       "619                         [[apartment,  1,  30771172]]  apartment   \n",
       "11036  [[but,  1,  999899654], [out,  1,  741601852],...        but   \n",
       "\n",
       "        correction_POS  \n",
       "439          (lot, NN)  \n",
       "2704     (edition, NN)  \n",
       "6647    (nepotism, NN)  \n",
       "619    (apartment, NN)  \n",
       "11036        (but, NN)  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misspell_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dictionary for mappying - key = incorrect spelling, value = correct spelling\n",
    "\n",
    "misspell_dict = pd.Series(misspell_df.correction_POS.values,misspell_df.tok_lem_POS).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('alot', 'alot', 'NN'): ('lot', 'NN'),\n",
       " ('defition', 'defition', 'NN'): ('edition', 'NN'),\n",
       " ('nepotizm', 'nepotizm', 'NN'): ('nepotism', 'NN'),\n",
       " ('apartament', 'apartament', 'NN'): ('apartment', 'NN'),\n",
       " ('yut', 'yut', 'NN'): ('but', 'NN'),\n",
       " ('studing', 'studing', 'VBG'): ('studying', 'VBG'),\n",
       " ('frisby', 'frisby', 'NN'): ('frisky', 'NN'),\n",
       " ('grammer', 'grammer', 'NN'): ('grammar', 'NN'),\n",
       " ('brillient', 'brillient', 'NN'): ('brilliant', 'NN'),\n",
       " ('brillient', 'brillient', 'VBN'): ('brilliant', 'VBN'),\n",
       " ('tution', 'tution', 'NN'): ('tuition', 'NN'),\n",
       " ('goverment', 'goverment', 'NN'): ('government', 'NN'),\n",
       " ('ludwing', 'ludwing', 'VBG'): ('ludwig', 'VBG'),\n",
       " ('fastfood', 'fastfood', 'NN'): ('eastwood', 'NN'),\n",
       " ('becuase', 'becuase', 'NN'): ('because', 'NN'),\n",
       " ('appartment', 'appartment', 'NN'): ('apartment', 'NN'),\n",
       " ('marrige', 'marrige', 'NN'): ('marriage', 'NN'),\n",
       " ('axé', 'axé', 'JJ'): ('axe', 'JJ'),\n",
       " ('etat', 'etat', 'NN'): ('eat', 'NN'),\n",
       " ('beatiful', 'beatiful', 'JJ'): ('beautiful', 'JJ'),\n",
       " ('recommond', 'recommond', 'VBP'): ('recommend', 'VBP'),\n",
       " ('lifes', 'life', 'NN'): ('life', 'NN'),\n",
       " ('recomand', 'recomand', 'VBP'): ('recommend', 'VBP'),\n",
       " ('devorce', 'devorce', 'NN'): ('divorce', 'NN'),\n",
       " ('resturant', 'resturant', 'NN'): ('restaurant', 'NN'),\n",
       " ('alfter', 'alfter', 'NN'): ('after', 'NN'),\n",
       " ('postive', 'postive', 'JJ'): ('positive', 'JJ'),\n",
       " ('sould', 'sould', 'VBP'): ('would', 'VBP'),\n",
       " ('becouse', 'becouse', 'IN'): ('because', 'IN'),\n",
       " ('befor', 'befor', 'NN'): ('before', 'NN'),\n",
       " ('becouse', 'becouse', 'NN'): ('because', 'NN'),\n",
       " ('tfgp', 'tfgp', 'IN'): ('top', 'IN'),\n",
       " ('frind', 'frind', 'NN'): ('find', 'NN'),\n",
       " ('benefites', 'benefites', 'NNS'): ('benefits', 'NNS'),\n",
       " ('thes', 'thes', 'NNS'): ('the', 'NNS'),\n",
       " ('hc', 'hc', 'NN'): ('he', 'NN'),\n",
       " ('apartement', 'apartement', 'NN'): ('apartment', 'NN'),\n",
       " ('ch', 'ch', 'NN'): ('cd', 'NN'),\n",
       " ('differents', 'differents', 'NNS'): ('different', 'NNS'),\n",
       " ('aupair', 'aupair', 'NN'): ('repair', 'NN'),\n",
       " ('specialis', 'specialis', 'NN'): ('specials', 'NN'),\n",
       " ('shoud', 'shoud', 'VBP'): ('should', 'VBP'),\n",
       " ('securaty', 'securaty', 'NN'): ('security', 'NN'),\n",
       " ('yut', 'yut', 'VBN'): ('but', 'VBN'),\n",
       " ('freind', 'freind', 'NN'): ('friend', 'NN'),\n",
       " ('somthing', 'somthing', 'VBG'): ('something', 'VBG'),\n",
       " ('airplan', 'airplan', 'NN'): ('airplay', 'NN'),\n",
       " ('htm', 'htm', 'NN'): ('him', 'NN'),\n",
       " ('sociaty', 'sociaty', 'NN'): ('society', 'NN'),\n",
       " ('gi', 'gi', 'NN'): ('i', 'NN'),\n",
       " ('confortable', 'confortable', 'JJ'): ('comfortable', 'JJ'),\n",
       " ('befor', 'befor', 'IN'): ('before', 'IN'),\n",
       " ('jop', 'jop', 'NN'): ('top', 'NN'),\n",
       " ('familly', 'familly', 'RB'): ('family', 'RB'),\n",
       " ('monthes', 'monthes', 'NNS'): ('months', 'NNS'),\n",
       " ('bridg', 'bridg', 'NN'): ('bring', 'NN'),\n",
       " ('chres', 'chres', 'NNS'): ('chris', 'NNS'),\n",
       " ('becasue', 'becasue', 'NN'): ('because', 'NN'),\n",
       " ('nowdays', 'nowdays', 'NNS'): ('nowadays', 'NNS'),\n",
       " ('suger', 'suger', 'NN'): ('super', 'NN'),\n",
       " ('synethesia', 'synethesia', 'NN'): ('synthesis', 'NN'),\n",
       " ('heum', 'heum', 'NN'): ('hum', 'NN'),\n",
       " ('researchs', 'researchs', 'NN'): ('research', 'NN'),\n",
       " ('chiken', 'chiken', 'NN'): ('chicken', 'NN'),\n",
       " ('rac', 'rac', 'NN'): ('mac', 'NN'),\n",
       " ('deprission', 'deprission', 'NN'): ('depression', 'NN'),\n",
       " ('hc', 'hc', 'VB'): ('he', 'VB'),\n",
       " ('freinds', 'freinds', 'NNS'): ('friends', 'NNS'),\n",
       " ('wonderfull', 'wonderfull', 'JJ'): ('wonderful', 'JJ'),\n",
       " ('ordor', 'ordor', 'SYM'): ('order', 'SYM'),\n",
       " ('resons', 'resons', 'NNS'): ('reasons', 'NNS'),\n",
       " ('analytes', 'analyte', 'NNS'): ('analyses', 'NNS'),\n",
       " ('bridege', 'bridege', 'NN'): ('bridge', 'NN'),\n",
       " ('referes', 'referes', 'VBZ'): ('refers', 'VBZ'),\n",
       " ('everytime', 'everytime', 'NN'): ('overtime', 'NN'),\n",
       " ('polution', 'polution', 'NN'): ('solution', 'NN'),\n",
       " ('sterotypes', 'sterotypes', 'NNS'): ('stereotypes', 'NNS'),\n",
       " ('begining', 'begining', 'NN'): ('beginning', 'NN'),\n",
       " ('favorate', 'favorate', 'NN'): ('favourite', 'NN'),\n",
       " ('improtant', 'improtant', 'JJ'): ('important', 'JJ'),\n",
       " ('forigners', 'forigners', 'NNS'): ('foreigners', 'NNS'),\n",
       " ('ddui', 'ddui', 'NN'): ('due', 'NN'),\n",
       " ('chiken', 'chiken', 'JJ'): ('chicken', 'JJ'),\n",
       " ('iss', 'iss', 'NN'): ('is', 'NN'),\n",
       " ('voiling', 'voiling', 'VBG'): ('boiling', 'VBG'),\n",
       " ('helth', 'helth', 'NN'): ('health', 'NN'),\n",
       " ('roomate', 'roomate', 'NN'): ('roommate', 'NN'),\n",
       " ('comercials', 'comercials', 'NNS'): ('commercials', 'NNS'),\n",
       " ('desicion', 'desicion', 'NN'): ('decision', 'NN'),\n",
       " ('stors', 'stors', 'NNPS'): ('store', 'NNPS'),\n",
       " ('addtion', 'addtion', 'NN'): ('addition', 'NN'),\n",
       " ('caffiene', 'caffiene', 'NN'): ('caffeine', 'NN'),\n",
       " ('shose', 'shose', 'NN'): ('those', 'NN'),\n",
       " ('healty', 'healty', 'NN'): ('health', 'NN'),\n",
       " ('adventages', 'adventages', 'NNS'): ('advantages', 'NNS'),\n",
       " ('famos', 'famos', 'JJ'): ('famous', 'JJ'),\n",
       " ('childern', 'childern', 'NN'): ('children', 'NN'),\n",
       " ('seng', 'seng', 'NN'): ('send', 'NN'),\n",
       " ('dopperganger', 'dopperganger', 'NN'): ('doppelganger', 'NN'),\n",
       " ('countery', 'countery', 'NN'): ('country', 'NN'),\n",
       " ('aquired', 'aquired', 'JJ'): ('acquired', 'JJ'),\n",
       " ('rissotto', 'rissotto', 'NN'): ('risotto', 'NN'),\n",
       " ('diferent', 'diferent', 'JJ'): ('different', 'JJ'),\n",
       " ('occured', 'occur', 'VBN'): ('occurred', 'VBN'),\n",
       " ('coffe', 'coffe', 'NN'): ('coffee', 'NN'),\n",
       " ('tions', 'tions', 'NNS'): ('tons', 'NNS'),\n",
       " ('nepotizm', 'nepotizm', 'RB'): ('nepotism', 'RB'),\n",
       " ('rosling', 'rosling', 'VBG'): ('rolling', 'VBG'),\n",
       " ('nepotizm', 'nepotizm', 'JJ'): ('nepotism', 'JJ'),\n",
       " ('sentance', 'sentance', 'NN'): ('sentence', 'NN'),\n",
       " ('neccessary', 'neccessary', 'JJ'): ('necessary', 'JJ'),\n",
       " ('poeple', 'poeple', 'NN'): ('people', 'NN'),\n",
       " ('dementi', 'dementi', 'NN'): ('dementia', 'NN'),\n",
       " ('devorce', 'devorce', 'VB'): ('divorce', 'VB'),\n",
       " ('differnt', 'differnt', 'JJ'): ('different', 'JJ'),\n",
       " ('ke', 'ke', 'NN'): ('be', 'NN'),\n",
       " ('diffirent', 'diffirent', 'JJ'): ('different', 'JJ'),\n",
       " ('researchs', 'researchs', 'NNS'): ('research', 'NNS'),\n",
       " ('becuase', 'becuase', 'VBP'): ('because', 'VBP'),\n",
       " ('auther', 'auther', 'NN'): ('author', 'NN'),\n",
       " ('experince', 'experince', 'NN'): ('experience', 'NN'),\n",
       " ('responsabilities', 'responsabilities', 'NNS'): ('responsibilities', 'NNS'),\n",
       " ('citys', 'citys', 'NN'): ('city', 'NN'),\n",
       " ('preson', 'preson', 'NN'): ('person', 'NN'),\n",
       " ('unversity', 'unversity', 'NN'): ('university', 'NN'),\n",
       " ('eatting', 'eatting', 'VBG'): ('eating', 'VBG'),\n",
       " ('iam', 'iam', 'NN'): ('am', 'NN'),\n",
       " ('caral', 'caral', 'JJ'): ('carl', 'JJ'),\n",
       " ('beutifull', 'beutifull', 'JJ'): ('beautiful', 'JJ'),\n",
       " ('everythings', 'everythings', 'NNS'): ('everything', 'NNS'),\n",
       " ('anoise', 'anoise', 'JJ'): ('noise', 'JJ'),\n",
       " ('vegeterian', 'vegeterian', 'JJ'): ('vegetarian', 'JJ'),\n",
       " ('rac', 'rac', 'VB'): ('mac', 'VB'),\n",
       " ('ther', 'ther', 'NN'): ('the', 'NN'),\n",
       " ('bousporus', 'bousporus', 'NN'): ('bosporus', 'NN'),\n",
       " ('rades', 'rades', 'NNS'): ('rates', 'NNS'),\n",
       " ('gliese', 'gliese', 'JJ'): ('lies', 'JJ'),\n",
       " ('chuan', 'chuan', 'NN'): ('chan', 'NN'),\n",
       " ('studints', 'studints', 'NNS'): ('students', 'NNS'),\n",
       " ('languge', 'languge', 'NN'): ('language', 'NN'),\n",
       " ('beacuse', 'beacuse', 'IN'): ('because', 'IN'),\n",
       " ('frinds', 'frinds', 'NNS'): ('friends', 'NNS'),\n",
       " ('resturants', 'resturants', 'NNS'): ('restaurants', 'NNS'),\n",
       " ('restorant', 'restorant', 'NN'): ('restaurant', 'NN'),\n",
       " ('diffrent', 'diffrent', 'JJ'): ('different', 'JJ'),\n",
       " ('ar', 'ar', 'VBP'): ('a', 'VBP'),\n",
       " ('popluar', 'popluar', 'JJ'): ('popular', 'JJ'),\n",
       " ('contry', 'contry', 'NN'): ('country', 'NN'),\n",
       " ('belived', 'belived', 'VBN'): ('believed', 'VBN'),\n",
       " ('pronounciation', 'pronounciation', 'NN'): ('pronunciation', 'NN'),\n",
       " ('scool', 'scool', 'NN'): ('school', 'NN'),\n",
       " ('favorit', 'favorit', 'JJ'): ('favourite', 'JJ'),\n",
       " ('favorit', 'favorit', 'NN'): ('favourite', 'NN'),\n",
       " ('togather', 'togather', 'NN'): ('together', 'NN'),\n",
       " ('realy', 'realy', 'JJ'): ('real', 'JJ'),\n",
       " ('ubicate', 'ubicate', 'JJ'): ('urinate', 'JJ'),\n",
       " ('advertisment', 'advertisment', 'NN'): ('advertisement', 'NN'),\n",
       " ('becaue', 'becaue', 'NN'): ('because', 'NN'),\n",
       " ('restraunt', 'restraunt', 'NN'): ('restraint', 'NN'),\n",
       " ('ther', 'ther', 'CC'): ('the', 'CC'),\n",
       " ('begining', 'begining', 'VBG'): ('beginning', 'VBG'),\n",
       " ('pleace', 'pleace', 'NN'): ('please', 'NN'),\n",
       " ('peeters', 'peeters', 'VBZ'): ('peters', 'VBZ'),\n",
       " ('vist', 'vist', 'VB'): ('list', 'VB'),\n",
       " ('childrens', 'child', 'NNS'): ('children', 'NNS'),\n",
       " ('impotant', 'impotant', 'JJ'): ('important', 'JJ'),\n",
       " ('friens', 'friens', 'NNS'): ('friend', 'NNS'),\n",
       " ('becaus', 'becaus', 'VBP'): ('because', 'VBP'),\n",
       " ('scound', 'scound', 'NN'): ('sound', 'NN'),\n",
       " ('evry', 'evry', 'JJ'): ('very', 'JJ'),\n",
       " ('benifits', 'benifits', 'NNS'): ('benefits', 'NNS'),\n",
       " ('acording', 'acording', 'VBG'): ('according', 'VBG'),\n",
       " ('sucessful', 'sucessful', 'JJ'): ('successful', 'JJ'),\n",
       " ('skrable', 'skrable', 'JJ'): ('skiable', 'JJ'),\n",
       " ('conclution', 'conclution', 'NN'): ('conclusion', 'NN'),\n",
       " ('usualy', 'usualy', 'JJ'): ('usually', 'JJ'),\n",
       " ('mounths', 'mounths', 'NNS'): ('months', 'NNS'),\n",
       " ('peaple', 'peaple', 'NN'): ('people', 'NN'),\n",
       " ('differnt', 'differnt', 'NN'): ('different', 'NN'),\n",
       " ('experiance', 'experiance', 'NN'): ('experience', 'NN'),\n",
       " ('geul', 'geul', 'NN'): ('gel', 'NN'),\n",
       " ('grammer', 'grammer', 'JJ'): ('grammar', 'JJ'),\n",
       " ('goverments', 'goverments', 'NNS'): ('governments', 'NNS'),\n",
       " ('milion', 'milion', 'NN'): ('million', 'NN'),\n",
       " ('immuno', 'immuno', 'NN'): ('immune', 'NN'),\n",
       " ('avoide', 'avoide', 'VB'): ('avoid', 'VB'),\n",
       " ('servies', 'servies', 'NNS'): ('services', 'NNS'),\n",
       " ('staeke', 'staeke', 'NN'): ('stake', 'NN'),\n",
       " ('ordor', 'ordor', 'NN'): ('order', 'NN'),\n",
       " ('rainning', 'rainning', 'VBG'): ('raining', 'VBG'),\n",
       " ('secients', 'secients', 'NNS'): ('secrets', 'NNS'),\n",
       " ('poors', 'poors', 'NNS'): ('poor', 'NNS'),\n",
       " ('detaily', 'detaily', 'RB'): ('details', 'RB'),\n",
       " ('becuse', 'becuse', 'IN'): ('because', 'IN'),\n",
       " ('pinao', 'pinao', 'NN'): ('piano', 'NN'),\n",
       " ('imformation', 'imformation', 'NN'): ('information', 'NN'),\n",
       " ('countreis', 'countreis', 'NN'): ('countries', 'NN'),\n",
       " ('acadimic', 'acadimic', 'JJ'): ('academic', 'JJ'),\n",
       " ('whe', 'whe', 'NN'): ('the', 'NN'),\n",
       " ('activites', 'activites', 'NNS'): ('activities', 'NNS'),\n",
       " ('sences', 'sences', 'NNS'): ('senses', 'NNS'),\n",
       " ('enghlish', 'enghlish', 'JJ'): ('english', 'JJ'),\n",
       " ('mounth', 'mounth', 'NN'): ('month', 'NN'),\n",
       " ('kuk', 'kuk', 'NN'): ('kun', 'NN'),\n",
       " ('becase', 'becase', 'NN'): ('because', 'NN'),\n",
       " ('genero', 'genero', 'NN'): ('genera', 'NN'),\n",
       " ('whos', 'whos', 'NN'): ('who', 'NN'),\n",
       " ('morden', 'morden', 'JJ'): ('borden', 'JJ'),\n",
       " ('mussaman', 'mussaman', 'JJ'): ('mussulman', 'JJ'),\n",
       " ('attoney', 'attoney', 'NN'): ('attorney', 'NN'),\n",
       " ('occured', 'occur', 'VBD'): ('occurred', 'VBD'),\n",
       " ('vo', 'vo', 'NN'): ('to', 'NN'),\n",
       " ('mussaman', 'mussaman', 'NN'): ('mussulman', 'NN'),\n",
       " ('achive', 'achive', 'VB'): ('archive', 'VB'),\n",
       " ('bulding', 'bulding', 'NN'): ('building', 'NN'),\n",
       " ('analyte', 'analyte', 'JJ'): ('analyse', 'JJ'),\n",
       " ('rumore', 'rumore', 'NN'): ('more', 'NN'),\n",
       " ('aprtment', 'aprtment', 'NN'): ('apartment', 'NN'),\n",
       " ('copsule', 'copsule', 'NN'): ('capsule', 'NN'),\n",
       " ('shek', 'shek', 'NN'): ('she', 'NN'),\n",
       " ('trodition', 'trodition', 'NN'): ('tradition', 'NN'),\n",
       " ('mahram', 'mahram', 'NN'): ('madam', 'NN'),\n",
       " ('duu', 'duu', 'NN'): ('due', 'NN'),\n",
       " ('enlish', 'enlish', 'JJ'): ('english', 'JJ'),\n",
       " ('wintter', 'wintter', 'NN'): ('winter', 'NN'),\n",
       " ('plause', 'plause', 'NN'): ('clause', 'NN'),\n",
       " ('imporatnt', 'imporatnt', 'JJ'): ('important', 'JJ'),\n",
       " ('happend', 'happend', 'VBP'): ('happen', 'VBP'),\n",
       " ('saleries', 'saleries', 'NNS'): ('salaries', 'NNS'),\n",
       " ('htm', 'htm', 'VB'): ('him', 'VB'),\n",
       " ('realy', 'realy', 'VBP'): ('real', 'VBP'),\n",
       " ('youself', 'youself', 'PRP'): ('yourself', 'PRP'),\n",
       " ('dwen', 'dwen', 'NN'): ('den', 'NN'),\n",
       " ('photogragh', 'photogragh', 'NN'): ('photograph', 'NN'),\n",
       " ('advisces', 'advisces', 'NNS'): ('advises', 'NNS'),\n",
       " ('trademen', 'trademen', 'NNS'): ('tradesmen', 'NNS'),\n",
       " ('firecrakers', 'firecrakers', 'NNS'): ('firecrackers', 'NNS'),\n",
       " ('finshed', 'finshed', 'VBD'): ('finished', 'VBD'),\n",
       " ('womens', 'womens', 'NNS'): ('women', 'NNS'),\n",
       " ('disipline', 'disipline', 'NN'): ('discipline', 'NN'),\n",
       " ('unforgetable', 'unforgetable', 'JJ'): ('unforgettable', 'JJ'),\n",
       " ('shoul', 'shoul', 'VBP'): ('should', 'VBP'),\n",
       " ('somestr', 'somestr', 'NN'): ('semester', 'NN'),\n",
       " ('feter', 'feter', 'NN'): ('peter', 'NN'),\n",
       " ('festifal', 'festifal', 'NN'): ('festival', 'NN'),\n",
       " ('alot', 'alot', 'VBN'): ('lot', 'VBN'),\n",
       " ('axé', 'axé', 'NN'): ('axe', 'NN'),\n",
       " ('porgrame', 'porgrame', 'NN'): ('program', 'NN'),\n",
       " ('dirsor', 'dirsor', 'NN'): ('mirror', 'NN'),\n",
       " ('daegu', 'daegu', 'PDT'): ('taegu', 'PDT'),\n",
       " ('customes', 'customes', 'NNS'): ('customer', 'NNS'),\n",
       " ('stadying', 'stadying', 'VBG'): ('studying', 'VBG'),\n",
       " ('becuase', 'becuase', 'IN'): ('because', 'IN'),\n",
       " ('studint', 'studint', 'NN'): ('student', 'NN'),\n",
       " ('suv', 'suv', 'NN'): ('sun', 'NN'),\n",
       " ('earthquak', 'earthquak', 'NN'): ('earthquake', 'NN'),\n",
       " ('intersting', 'intersting', 'VBG'): ('interesting', 'VBG'),\n",
       " ('tesh', 'tesh', 'JJ'): ('test', 'JJ'),\n",
       " ('vedio', 'vedio', 'NN'): ('vedic', 'NN'),\n",
       " ('choosed', 'choosed', 'VBD'): ('choose', 'VBD'),\n",
       " ('lamen', 'lamen', 'NNS'): ('laden', 'NNS'),\n",
       " ('bikinies', 'bikinies', 'NNS'): ('bikinis', 'NNS'),\n",
       " ('vtr', 'vtr', 'JJ'): ('var', 'JJ'),\n",
       " ('espacially', 'espacially', 'RB'): ('especially', 'RB'),\n",
       " ('ther', 'ther', 'PRP'): ('the', 'PRP'),\n",
       " ('melo', 'melo', 'NN'): ('memo', 'NN'),\n",
       " ('melo', 'melo', 'NNS'): ('memo', 'NNS'),\n",
       " ('jang', 'jang', 'NN'): ('jan', 'NN'),\n",
       " ('fter', 'fter', 'NN'): ('after', 'NN'),\n",
       " ('transporation', 'transporation', 'NN'): ('transportation', 'NN'),\n",
       " ('aquire', 'aquire', 'VB'): ('acquire', 'VB'),\n",
       " ('knowlege', 'knowlege', 'NN'): ('knowledge', 'NN'),\n",
       " ('acommodations', 'acommodations', 'NNS'): ('accommodations', 'NNS'),\n",
       " ('hemsphere', 'hemsphere', 'RB'): ('hemisphere', 'RB'),\n",
       " ('trainning', 'trainning', 'VBG'): ('training', 'VBG'),\n",
       " ('exessive', 'exessive', 'JJ'): ('excessive', 'JJ'),\n",
       " ('exective', 'exective', 'JJ'): ('executive', 'JJ'),\n",
       " ('becuse', 'becuse', 'NN'): ('because', 'NN'),\n",
       " ('intersted', 'intersted', 'VBN'): ('interested', 'VBN'),\n",
       " ('ap', 'ap', 'JJ'): ('a', 'JJ'),\n",
       " ('inconfidence', 'inconfidence', 'NN'): ('confidence', 'NN'),\n",
       " ('guntis', 'guntis', 'NN'): ('until', 'NN'),\n",
       " ('heared', 'heared', 'VBN'): ('heard', 'VBN'),\n",
       " ('easly', 'easly', 'RB'): ('easy', 'RB'),\n",
       " ('advantege', 'advantege', 'NN'): ('advantage', 'NN'),\n",
       " ('problame', 'problame', 'NN'): ('problem', 'NN'),\n",
       " ('lik', 'lik', 'VB'): ('like', 'VB'),\n",
       " ('befor', 'befor', 'JJ'): ('before', 'JJ'),\n",
       " ('zaki', 'zaki', 'NN'): ('saki', 'NN'),\n",
       " ('socail', 'socail', 'JJ'): ('social', 'JJ'),\n",
       " ('zaki', 'zaki', 'JJ'): ('saki', 'JJ'),\n",
       " ('contries', 'contries', 'NNS'): ('countries', 'NNS'),\n",
       " ('moniter', 'moniter', 'NN'): ('monitor', 'NN'),\n",
       " ('recieve', 'recieve', 'VB'): ('receive', 'VB'),\n",
       " ('redsox', 'redsox', 'NN'): ('reason', 'NN'),\n",
       " ('gramatical', 'gramatical', 'JJ'): ('grammatical', 'JJ'),\n",
       " ('adivices', 'adivices', 'NNS'): ('advices', 'NNS'),\n",
       " ('performence', 'performence', 'NN'): ('performance', 'NN'),\n",
       " ('adivces', 'adivces', 'NNS'): ('advices', 'NNS'),\n",
       " ('pepole', 'pepole', 'NN'): ('people', 'NN'),\n",
       " ('beleive', 'beleive', 'VBP'): ('believe', 'VBP'),\n",
       " ('ture', 'ture', 'NN'): ('sure', 'NN'),\n",
       " ('grammer', 'grammer', 'NNS'): ('grammar', 'NNS'),\n",
       " ('nyuh', 'nyuh', 'NN'): ('nut', 'NN'),\n",
       " ('alfeter', 'alfeter', 'NN'): ('after', 'NN'),\n",
       " ('differnt', 'differnt', 'VBN'): ('different', 'VBN'),\n",
       " ('bacause', 'bacause', 'NN'): ('because', 'NN'),\n",
       " ('persone', 'persone', 'NN'): ('person', 'NN'),\n",
       " ('baket', 'baket', 'NN'): ('basket', 'NN'),\n",
       " ('beleive', 'beleive', 'VB'): ('believe', 'VB'),\n",
       " ('kakike', 'kakike', 'VB'): ('katie', 'VB'),\n",
       " ('kakike', 'kakike', 'JJ'): ('katie', 'JJ'),\n",
       " ('jeju', 'jeju', 'NN'): ('jesu', 'NN'),\n",
       " ('thr', 'thr', 'JJ'): ('the', 'JJ'),\n",
       " ('transpotation', 'transpotation', 'NN'): ('transportation', 'NN'),\n",
       " ('centr', 'centr', 'NN'): ('centre', 'NN'),\n",
       " ('thoub', 'thoub', 'NN'): ('thou', 'NN'),\n",
       " ('openion', 'openion', 'NN'): ('opinion', 'NN'),\n",
       " ('changings', 'changings', 'NNS'): ('changing', 'NNS'),\n",
       " ('childern', 'childern', 'JJ'): ('children', 'JJ'),\n",
       " ('chres', 'chres', 'NNPS'): ('chris', 'NNPS'),\n",
       " ('cm', 'cm', 'NN'): ('pm', 'NN'),\n",
       " ('belived', 'belived', 'VBD'): ('believed', 'VBD'),\n",
       " ('convserve', 'convserve', 'VB'): ('conserve', 'VB'),\n",
       " ('studiying', 'studiying', 'VBG'): ('studying', 'VBG'),\n",
       " ('analytes', 'analyte', 'VBZ'): ('analyses', 'VBZ'),\n",
       " ('tzu', 'tzu', 'NN'): ('thu', 'NN'),\n",
       " ('amircan', 'amircan', 'JJ'): ('american', 'JJ'),\n",
       " ('maried', 'maried', 'JJ'): ('married', 'JJ'),\n",
       " ('hyun', 'hyun', 'NN'): ('hun', 'NN'),\n",
       " ('nyu', 'nyu', 'NN'): ('nyx', 'NN'),\n",
       " ('decieded', 'decieded', 'VBD'): ('decided', 'VBD'),\n",
       " ('whome', 'whome', 'NN'): ('home', 'NN'),\n",
       " ('lelan', 'lelan', 'NN'): ('lean', 'NN'),\n",
       " ('usally', 'usally', 'RB'): ('usually', 'RB'),\n",
       " ('ipods', 'ipod', 'NNS'): ('ipod', 'NNS'),\n",
       " ('sause', 'sause', 'NN'): ('cause', 'NN'),\n",
       " ('agressive', 'agressive', 'JJ'): ('aggressive', 'JJ'),\n",
       " ('reson', 'reson', 'NN'): ('reason', 'NN'),\n",
       " ('frisby', 'frisby', 'NNS'): ('frisky', 'NNS'),\n",
       " ('frisby', 'frisby', 'RB'): ('frisky', 'RB'),\n",
       " ('heared', 'heared', 'VBD'): ('heard', 'VBD'),\n",
       " ('similiar', 'similiar', 'JJ'): ('similar', 'JJ'),\n",
       " ('puplic', 'puplic', 'JJ'): ('public', 'JJ'),\n",
       " ('everthing', 'everthing', 'VBG'): ('everything', 'VBG'),\n",
       " ('safa', 'safa', 'NN'): ('safe', 'NN'),\n",
       " ('esl', 'esl', 'JJ'): ('est', 'JJ'),\n",
       " ('enjoied', 'enjoied', 'VBD'): ('enjoyed', 'VBD'),\n",
       " ('goverment', 'goverment', 'JJ'): ('government', 'JJ'),\n",
       " ('writting', 'writting', 'VBG'): ('writing', 'VBG'),\n",
       " ('eatten', 'eatten', 'VBN'): ('eaten', 'VBN'),\n",
       " ('doughter', 'doughter', 'NN'): ('daughter', 'NN'),\n",
       " ('habilis', 'habilis', 'NN'): ('habits', 'NN'),\n",
       " ('dvds', 'dvds', 'NN'): ('dds', 'NN'),\n",
       " ('researchs', 'researchs', 'JJ'): ('research', 'JJ'),\n",
       " ('geting', 'geting', 'VBG'): ('getting', 'VBG'),\n",
       " ('feild', 'feild', 'NN'): ('field', 'NN'),\n",
       " ('falled', 'falled', 'VBD'): ('called', 'VBD'),\n",
       " ('completly', 'completly', 'RB'): ('completely', 'RB'),\n",
       " ('infront', 'infront', 'NN'): ('front', 'NN'),\n",
       " ('majong', 'majong', 'JJ'): ('mahjong', 'JJ'),\n",
       " ('majers', 'majers', 'NNS'): ('makers', 'NNS'),\n",
       " ('emotibob', 'emotibob', 'NN'): ('emotion', 'NN'),\n",
       " ('finacial', 'finacial', 'JJ'): ('financial', 'JJ'),\n",
       " ('comercial', 'comercial', 'NN'): ('commercial', 'NN'),\n",
       " ('attintion', 'attintion', 'NN'): ('attention', 'NN'),\n",
       " ('pck', 'pck', 'VB'): ('pc', 'VB'),\n",
       " ('schmid', 'schmid', 'JJ'): ('schmidt', 'JJ'),\n",
       " ('schedual', 'schedual', 'JJ'): ('schedule', 'JJ'),\n",
       " ('patien', 'patien', 'NN'): ('patient', 'NN'),\n",
       " ('mahy', 'mahy', 'NN'): ('may', 'NN'),\n",
       " ('interet', 'interet', 'NNS'): ('internet', 'NNS'),\n",
       " ('interst', 'interst', 'NN'): ('interest', 'NN'),\n",
       " ('enlgish', 'enlgish', 'JJ'): ('english', 'JJ'),\n",
       " ('sc', 'sc', 'NN'): ('so', 'NN'),\n",
       " ('parede', 'parede', 'NN'): ('parade', 'NN'),\n",
       " ('chopstics', 'chopstics', 'NNS'): ('chopsticks', 'NNS'),\n",
       " ('anoise', 'anoise', 'VB'): ('noise', 'VB'),\n",
       " ('compus', 'compus', 'NN'): ('campus', 'NN'),\n",
       " ('salam', 'salam', 'NN'): ('salem', 'NN'),\n",
       " ('programe', 'programe', 'NN'): ('program', 'NN'),\n",
       " ('worring', 'worring', 'VBG'): ('working', 'VBG'),\n",
       " ('immuno', 'immuno', 'VB'): ('immune', 'VB'),\n",
       " ('photograghs', 'photograghs', 'NNS'): ('photographs', 'NNS'),\n",
       " ('flashlighter', 'flashlighter', 'NN'): ('flashlight', 'NN'),\n",
       " ('firends', 'firends', 'NNS'): ('friends', 'NNS'),\n",
       " ('habbits', 'habbits', 'NNS'): ('habits', 'NNS'),\n",
       " ('seng', 'seng', 'JJ'): ('send', 'JJ'),\n",
       " ('studyed', 'studyed', 'VBN'): ('studied', 'VBN'),\n",
       " ('ligh', 'ligh', 'JJ'): ('high', 'JJ'),\n",
       " ('importent', 'importent', 'JJ'): ('important', 'JJ'),\n",
       " ('milions', 'milions', 'NNS'): ('millions', 'NNS'),\n",
       " ('finishin', 'finishin', 'JJ'): ('finishing', 'JJ'),\n",
       " ('convinient', 'convinient', 'NN'): ('convenient', 'NN'),\n",
       " ('adolescen', 'adolescen', 'NNS'): ('adolescent', 'NNS'),\n",
       " ('converient', 'converient', 'JJ'): ('convenient', 'JJ'),\n",
       " ('weekand', 'weekand', 'NN'): ('weekend', 'NN'),\n",
       " ('millitary', 'millitary', 'NN'): ('military', 'NN'),\n",
       " ('eldery', 'eldery', 'NN'): ('elderly', 'NN'),\n",
       " ('indipendent', 'indipendent', 'JJ'): ('independent', 'JJ'),\n",
       " ('indivisuals', 'indivisuals', 'NNS'): ('individuals', 'NNS'),\n",
       " ('supose', 'supose', 'VBP'): ('suppose', 'VBP'),\n",
       " ('nighbor', 'nighbor', 'NN'): ('neighbour', 'NN'),\n",
       " ('becaue', 'becaue', 'IN'): ('because', 'IN'),\n",
       " ('lb', 'lb', 'NN'): ('la', 'NN'),\n",
       " ('ther', 'ther', 'EX'): ('the', 'EX'),\n",
       " ('longman', 'longman', 'JJ'): ('longan', 'JJ'),\n",
       " ('expencive', 'expencive', 'JJ'): ('expensive', 'JJ'),\n",
       " ('ftur', 'ftur', 'NN'): ('four', 'NN'),\n",
       " ('kony', 'kony', 'NN'): ('sony', 'NN'),\n",
       " ('qutra', 'qutra', 'NN'): ('sutra', 'NN'),\n",
       " ('qutra', 'qutra', 'NNS'): ('sutra', 'NNS'),\n",
       " ('resoures', 'resoures', 'NNS'): ('resources', 'NNS'),\n",
       " ('beduoins', 'beduoins', 'NNS'): ('bedouins', 'NNS'),\n",
       " ('littele', 'littele', 'NN'): ('little', 'NN'),\n",
       " ('tring', 'tring', 'VBG'): ('thing', 'VBG'),\n",
       " ('familys', 'family', 'NN'): ('family', 'NN'),\n",
       " ('explaination', 'explaination', 'NN'): ('explanation', 'NN'),\n",
       " ('frisby', 'frisby', 'JJ'): ('frisky', 'JJ'),\n",
       " ('famely', 'famely', 'RB'): ('family', 'RB'),\n",
       " ('yueh', 'yueh', 'SYM'): ('such', 'SYM'),\n",
       " ('bestfriend', 'bestfriend', 'NN'): ('befriend', 'NN'),\n",
       " ('befor', 'befor', 'VBP'): ('before', 'VBP'),\n",
       " ('totaly', 'totaly', 'NNS'): ('total', 'NNS'),\n",
       " ('usualy', 'usualy', 'VBP'): ('usually', 'VBP'),\n",
       " ('pyoung', 'pyoung', 'NN'): ('young', 'NN'),\n",
       " ('marrige', 'marrige', 'JJ'): ('marriage', 'JJ'),\n",
       " ('ther', 'ther', 'JJR'): ('the', 'JJR'),\n",
       " ('especialy', 'especialy', 'NN'): ('especially', 'NN'),\n",
       " ('girlfrind', 'girlfrind', 'NN'): ('girlfriend', 'NN'),\n",
       " ('charactors', 'charactors', 'NNS'): ('characters', 'NNS'),\n",
       " ('abju', 'abju', 'NN'): ('about', 'NN'),\n",
       " ('ith', 'ith', 'VB'): ('with', 'VB'),\n",
       " ('iwas', 'iwas', 'NN'): ('was', 'NN'),\n",
       " ('iwork', 'iwork', 'NN'): ('work', 'NN'),\n",
       " ('mla', 'mla', 'NN'): ('la', 'NN'),\n",
       " ('aquired', 'aquired', 'VBN'): ('acquired', 'VBN'),\n",
       " ('tian', 'tian', 'JJ'): ('than', 'JJ'),\n",
       " ('luctures', 'luctures', 'NNS'): ('lectures', 'NNS'),\n",
       " ('archaelogists', 'archaelogists', 'NNS'): ('archaeologists', 'NNS'),\n",
       " ('evrything', 'evrything', 'VBG'): ('everything', 'VBG'),\n",
       " ('becuase', 'becuase', 'VB'): ('because', 'VB'),\n",
       " ('toei', 'toei', 'JJ'): ('toe', 'JJ'),\n",
       " ('calss', 'calss', 'NN'): ('class', 'NN'),\n",
       " ('coutry', 'coutry', 'NN'): ('country', 'NN'),\n",
       " ('diffierent', 'diffierent', 'JJ'): ('different', 'JJ'),\n",
       " ('mypyramid', 'mypyramid', 'NN'): ('pyramid', 'NN'),\n",
       " ('sevral', 'sevral', 'JJ'): ('several', 'JJ'),\n",
       " ('dammam', 'dammam', 'NN'): ('gamma', 'NN'),\n",
       " ('huspand', 'huspand', 'VBP'): ('husband', 'VBP'),\n",
       " ('woreship', 'woreship', 'VB'): ('worship', 'VB'),\n",
       " ('humen', 'humen', 'NNS'): ('human', 'NNS'),\n",
       " ('dolsot', 'dolsot', 'NN'): ('dorset', 'NN'),\n",
       " ('diverent', 'diverent', 'NN'): ('divergent', 'NN'),\n",
       " ('afther', 'afther', 'RB'): ('after', 'RB'),\n",
       " ('soving', 'soving', 'VBG'): ('moving', 'VBG'),\n",
       " ('sould', 'sould', 'JJ'): ('would', 'JJ'),\n",
       " ('homwork', 'homwork', 'NN'): ('homework', 'NN'),\n",
       " ('dissapoint', 'dissapoint', 'NN'): ('disappoint', 'NN'),\n",
       " ('prefering', 'prefering', 'VBG'): ('preferring', 'VBG'),\n",
       " ('someome', 'someome', 'NN'): ('someone', 'NN'),\n",
       " ('hm', 'hm', 'NN'): ('he', 'NN'),\n",
       " ('alfter', 'alfter', 'RB'): ('after', 'RB'),\n",
       " ('dgree', 'dgree', 'NN'): ('degree', 'NN'),\n",
       " ('diaphragmic', 'diaphragmic', 'JJ'): ('diaphragm', 'JJ'),\n",
       " ('sities', 'sities', 'NNS'): ('sites', 'NNS'),\n",
       " ('differant', 'differant', 'JJ'): ('different', 'JJ'),\n",
       " ('differe', 'differe', 'RB'): ('differ', 'RB'),\n",
       " ('helpfull', 'helpfull', 'JJ'): ('helpful', 'JJ'),\n",
       " ('skyrush', 'skyrush', 'NN'): ('skyros', 'NN'),\n",
       " ('diffrent', 'diffrent', 'NN'): ('different', 'NN'),\n",
       " ('diffrences', 'diffrences', 'NNS'): ('differences', 'NNS'),\n",
       " ('dallah', 'dallah', 'NN'): ('dallas', 'NN'),\n",
       " ('huspand', 'huspand', 'NN'): ('husband', 'NN'),\n",
       " ('culteres', 'culteres', 'NNS'): ('cultures', 'NNS'),\n",
       " ('excuting', 'excuting', 'VBG'): ('exciting', 'VBG'),\n",
       " ('kyung', 'kyung', 'NN'): ('young', 'NN'),\n",
       " ('scholl', 'scholl', 'NN'): ('school', 'NN'),\n",
       " ('nadal', 'nadal', 'NN'): ('naval', 'NN'),\n",
       " ('humman', 'humman', 'JJ'): ('human', 'JJ'),\n",
       " ('drowing', 'drowing', 'VBG'): ('growing', 'VBG'),\n",
       " ('mythes', 'mythes', 'NNS'): ('myths', 'NNS'),\n",
       " ('vegtables', 'vegtables', 'NNS'): ('vegetables', 'NNS'),\n",
       " ('experiances', 'experiances', 'NNS'): ('experiences', 'NNS'),\n",
       " ('befor', 'befor', 'VBD'): ('before', 'VBD'),\n",
       " ('diffcult', 'diffcult', 'NN'): ('difficult', 'NN'),\n",
       " ('dissapointment', 'dissapointment', 'NN'): ('disappointment', 'NN'),\n",
       " ('thoes', 'thoes', 'JJ'): ('those', 'JJ'),\n",
       " ('voil', 'voil', 'NN'): ('oil', 'NN'),\n",
       " ('neighbohood', 'neighbohood', 'NN'): ('neighbourhood', 'NN'),\n",
       " ('psycological', 'psycological', 'JJ'): ('psychological', 'JJ'),\n",
       " ('spirite', 'spirite', 'NN'): ('spirit', 'NN'),\n",
       " ('tatal', 'tatal', 'JJ'): ('total', 'JJ'),\n",
       " ('remeber', 'remeber', 'VB'): ('remember', 'VB'),\n",
       " ('feets', 'feets', 'NNS'): ('feet', 'NNS'),\n",
       " ('peopl', 'peopl', 'NN'): ('people', 'NN'),\n",
       " ('writin', 'writin', 'JJ'): ('writing', 'JJ'),\n",
       " ('abou', 'abou', 'IN'): ('about', 'IN'),\n",
       " ('synethesia', 'synethesia', 'JJ'): ('synthesis', 'JJ'),\n",
       " ('aquired', 'aquired', 'VBD'): ('acquired', 'VBD'),\n",
       " ('governement', 'governement', 'NN'): ('government', 'NN'),\n",
       " ('transportaion', 'transportaion', 'NN'): ('transportation', 'NN'),\n",
       " ('strenght', 'strenght', 'NN'): ('strength', 'NN'),\n",
       " ('pouses', 'pouses', 'NNS'): ('houses', 'NNS'),\n",
       " ('npr', 'npr', 'NN'): ('apr', 'NN'),\n",
       " ('collegues', 'collegues', 'NNS'): ('colleges', 'NNS'),\n",
       " ('temparature', 'temparature', 'NN'): ('temperature', 'NN'),\n",
       " ('beginnig', 'beginnig', 'NN'): ('beginning', 'NN'),\n",
       " ('dificult', 'dificult', 'NN'): ('difficult', 'NN'),\n",
       " ('msn', 'msn', 'NN'): ('man', 'NN'),\n",
       " ('chiken', 'chiken', 'JJR'): ('chicken', 'JJR'),\n",
       " ('tryed', 'tryed', 'VBD'): ('tried', 'VBD'),\n",
       " ('postion', 'postion', 'NN'): ('position', 'NN'),\n",
       " ('themo', 'themo', 'EX'): ('them', 'EX'),\n",
       " ('themo', 'themo', 'NN'): ('them', 'NN'),\n",
       " ('beteween', 'beteween', 'VB'): ('between', 'VB'),\n",
       " ('dicision', 'dicision', 'NN'): ('decision', 'NN'),\n",
       " ('wondring', 'wondring', 'VBG'): ('wondering', 'VBG'),\n",
       " ('preperation', 'preperation', 'NN'): ('preparation', 'NN'),\n",
       " ('troble', 'troble', 'JJ'): ('trouble', 'JJ'),\n",
       " ('sp', 'sp', 'NN'): ('up', 'NN'),\n",
       " ('phychic', 'phychic', 'JJ'): ('psychic', 'JJ'),\n",
       " ('poeple', 'poeple', 'NNS'): ('people', 'NNS'),\n",
       " ('beutiful', 'beutiful', 'JJ'): ('beautiful', 'JJ'),\n",
       " ('wnat', 'wnat', 'VBP'): ('what', 'VBP'),\n",
       " ('benifit', 'benifit', 'NN'): ('benefit', 'NN'),\n",
       " ('nigative', 'nigative', 'JJ'): ('negative', 'JJ'),\n",
       " ('paragragh', 'paragragh', 'NN'): ('paragraph', 'NN'),\n",
       " ('saulty', 'saulty', 'NN'): ('faulty', 'NN'),\n",
       " ('enought', 'enought', 'JJ'): ('enough', 'JJ'),\n",
       " ('responsability', 'responsability', 'NN'): ('responsibility', 'NN'),\n",
       " ('accaunting', 'accaunting', 'VBG'): ('accounting', 'VBG'),\n",
       " ('reserch', 'reserch', 'NN'): ('research', 'NN'),\n",
       " ('extereme', 'extereme', 'JJ'): ('extreme', 'JJ'),\n",
       " ('belivers', 'belivers', 'NNS'): ('delivers', 'NNS'),\n",
       " ('chilhood', 'chilhood', 'NN'): ('childhood', 'NN'),\n",
       " ('postive', 'postive', 'VBP'): ('positive', 'VBP'),\n",
       " ('larg', 'larg', 'JJ'): ('large', 'JJ'),\n",
       " ('didnot', 'didnot', 'VBP'): ('idiot', 'VBP'),\n",
       " ('theather', 'theather', 'NN'): ('heather', 'NN'),\n",
       " ('intuitives', 'intuitives', 'NNS'): ('intuitive', 'NNS'),\n",
       " ('wav', 'wav', 'NN'): ('was', 'NN'),\n",
       " ('beutifull', 'beutifull', 'NN'): ('beautiful', 'NN'),\n",
       " ('transfomation', 'transfomation', 'NN'): ('transformation', 'NN'),\n",
       " ('desease', 'desease', 'NN'): ('disease', 'NN'),\n",
       " ('ssshh', 'ssshh', 'NN'): ('ssh', 'NN'),\n",
       " ('regulary', 'regulary', 'JJ'): ('regular', 'JJ'),\n",
       " ('quistion', 'quistion', 'NN'): ('question', 'NN'),\n",
       " ('suger', 'suger', 'VBN'): ('super', 'VBN'),\n",
       " ('exampe', 'exampe', 'NN'): ('example', 'NN'),\n",
       " ('sentece', 'sentece', 'NN'): ('sentence', 'NN'),\n",
       " ('firest', 'firest', 'JJS'): ('first', 'JJS'),\n",
       " ('iam', 'iam', 'JJ'): ('am', 'JJ'),\n",
       " ('manag', 'manag', 'VBP'): ('manage', 'VBP'),\n",
       " ('coun', 'coun', 'NN'): ('count', 'NN'),\n",
       " ('bady', 'bady', 'NN'): ('body', 'NN'),\n",
       " ('unfortunatly', 'unfortunatly', 'RB'): ('unfortunately', 'RB'),\n",
       " ('badroom', 'badroom', 'NN'): ('bedroom', 'NN'),\n",
       " ('finshed', 'finshed', 'VBN'): ('finished', 'VBN'),\n",
       " ('tradional', 'tradional', 'JJ'): ('traditional', 'JJ'),\n",
       " ('contruction', 'contruction', 'NN'): ('construction', 'NN'),\n",
       " ('bacause', 'bacause', 'IN'): ('because', 'IN'),\n",
       " ('uniqe', 'uniqe', 'JJ'): ('unique', 'JJ'),\n",
       " ('recieved', 'recieved', 'VBD'): ('received', 'VBD'),\n",
       " ('drived', 'drived', 'VBD'): ('drive', 'VBD'),\n",
       " ('fineshed', 'fineshed', 'VBD'): ('finished', 'VBD'),\n",
       " ('unkown', 'unkown', 'JJ'): ('unknown', 'JJ'),\n",
       " ('lerning', 'lerning', 'VBG'): ('learning', 'VBG'),\n",
       " ('expecially', 'expecially', 'RB'): ('especially', 'RB'),\n",
       " ('jilbab', 'jilbab', 'NN'): ('bilbao', 'NN'),\n",
       " ('sheeps', 'sheep', 'NNS'): ('sheets', 'NNS'),\n",
       " ('becaue', 'becaue', 'VBP'): ('because', 'VBP'),\n",
       " ('becaue', 'becaue', 'VB'): ('because', 'VB'),\n",
       " ('somthing', 'somthing', 'NN'): ('something', 'NN'),\n",
       " ('businese', 'businese', 'NN'): ('business', 'NN'),\n",
       " ('unsucced', 'unsucced', 'VBD'): ('unsuited', 'VBD'),\n",
       " ('uesd', 'uesd', 'VBP'): ('used', 'VBP'),\n",
       " ('usefull', 'usefull', 'JJ'): ('useful', 'JJ'),\n",
       " ('sinc', 'sinc', 'NN'): ('since', 'NN'),\n",
       " ('discribe', 'discribe', 'VB'): ('describe', 'VB'),\n",
       " ('coutries', 'coutries', 'NNS'): ('countries', 'NNS'),\n",
       " ('tiji', 'tiji', 'JJ'): ('fiji', 'JJ'),\n",
       " ('ancxious', 'ancxious', 'JJ'): ('anxious', 'JJ'),\n",
       " ('jik', 'jik', 'NN'): ('jim', 'NN'),\n",
       " ('exective', 'exective', 'NN'): ('executive', 'NN'),\n",
       " ('cuold', 'cuold', 'NN'): ('could', 'NN'),\n",
       " ('beacuse', 'beacuse', 'NN'): ('because', 'NN'),\n",
       " ('kitche', 'kitche', 'NN'): ('kitchen', 'NN'),\n",
       " ('dopperganger', 'dopperganger', 'JJ'): ('doppelganger', 'JJ'),\n",
       " ('comunity', 'comunity', 'NN'): ('community', 'NN'),\n",
       " ('forign', 'forign', 'JJ'): ('foreign', 'JJ'),\n",
       " ('bycicle', 'bycicle', 'NN'): ('bicycle', 'NN'),\n",
       " ('furnuture', 'furnuture', 'NN'): ('furniture', 'NN'),\n",
       " ('exercices', 'exercices', 'NNS'): ('exercises', 'NNS'),\n",
       " ('sisiter', 'sisiter', 'NN'): ('sister', 'NN'),\n",
       " ('prblems', 'prblems', 'NNS'): ('problems', 'NNS'),\n",
       " ('buatiful', 'buatiful', 'JJ'): ('beautiful', 'JJ'),\n",
       " ('fustis', 'fustis', 'NN'): ('austin', 'NN'),\n",
       " ('stly', 'stly', 'RB'): ('stay', 'RB'),\n",
       " ('socaily', 'socaily', 'RB'): ('social', 'RB'),\n",
       " ('existance', 'existance', 'NN'): ('existence', 'NN'),\n",
       " ('brocolli', 'brocolli', 'NN'): ('broccoli', 'NN'),\n",
       " ('trainning', 'trainning', 'NN'): ('training', 'NN'),\n",
       " ('tranportation', 'tranportation', 'NN'): ('transportation', 'NN'),\n",
       " ('translat', 'translat', 'VB'): ('translate', 'VB'),\n",
       " ('diciplined', 'diciplined', 'VBN'): ('disciplined', 'VBN'),\n",
       " ('traning', 'traning', 'VBG'): ('training', 'VBG'),\n",
       " ('experenice', 'experenice', 'NN'): ('experience', 'NN'),\n",
       " ('konw', 'konw', 'VB'): ('know', 'VB'),\n",
       " ('devolop', 'devolop', 'VB'): ('develop', 'VB'),\n",
       " ('destory', 'destory', 'VB'): ('destroy', 'VB'),\n",
       " ('trainning', 'trainning', 'JJ'): ('training', 'JJ'),\n",
       " ('restuarant', 'restuarant', 'NN'): ('restaurant', 'NN'),\n",
       " ('healty', 'healty', 'JJ'): ('health', 'JJ'),\n",
       " ('breadfast', 'breadfast', 'NN'): ('breakfast', 'NN'),\n",
       " ('oppotunity', 'oppotunity', 'NN'): ('opportunity', 'NN'),\n",
       " ('treament', 'treament', 'NN'): ('treatment', 'NN'),\n",
       " ('exmple', 'exmple', 'NN'): ('example', 'NN'),\n",
       " ('borns', 'borns', 'NNS'): ('born', 'NNS'),\n",
       " ('biger', 'biger', 'JJR'): ('tiger', 'JJR'),\n",
       " ('oppsite', 'oppsite', 'NN'): ('opposite', 'NN'),\n",
       " ('dreem', 'dreem', 'NN'): ('dream', 'NN'),\n",
       " ('quesion', 'quesion', 'NN'): ('question', 'NN'),\n",
       " ('softwears', 'softwears', 'NNS'): ('software', 'NNS'),\n",
       " ('ducation', 'ducation', 'NN'): ('education', 'NN'),\n",
       " ('languaje', 'languaje', 'NN'): ('language', 'NN'),\n",
       " ('ie', 'ie', 'NN'): ('in', 'NN'),\n",
       " ('libary', 'libary', 'JJ'): ('library', 'JJ'),\n",
       " ('finaly', 'finaly', 'NN'): ('final', 'NN'),\n",
       " ('libarary', 'libarary', 'NN'): ('library', 'NN'),\n",
       " ('pr', 'pr', 'NN'): ('or', 'NN'),\n",
       " ('reaveling', 'reaveling', 'VBG'): ('revealing', 'VBG'),\n",
       " ('basical', 'basical', 'JJ'): ('basic', 'JJ'),\n",
       " ('basterds', 'basterds', 'NNS'): ('bastards', 'NNS'),\n",
       " ('occitane', 'occitane', 'NN'): ('octane', 'NN'),\n",
       " ('lenght', 'lenght', 'NN'): ('length', 'NN'),\n",
       " ('relagion', 'relagion', 'NN'): ('religion', 'NN'),\n",
       " ('feter', 'feter', 'VBP'): ('peter', 'VBP'),\n",
       " ('lefting', 'lefting', 'VBG'): ('letting', 'VBG'),\n",
       " ('seung', 'seung', 'VBD'): ('sung', 'VBD'),\n",
       " ('becasue', 'becasue', 'IN'): ('because', 'IN'),\n",
       " ('balad', 'balad', 'NN'): ('salad', 'NN'),\n",
       " ('lifes', 'life', 'JJ'): ('life', 'JJ'),\n",
       " ('stil', 'stil', 'VBP'): ('still', 'VBP'),\n",
       " ('recomendation', 'recomendation', 'NN'): ('recommendation', 'NN'),\n",
       " ('redsox', 'redsox', 'JJ'): ('reason', 'JJ'),\n",
       " ('aviod', 'aviod', 'VB'): ('avoid', 'VB'),\n",
       " ('helthy', 'helthy', 'JJ'): ('healthy', 'JJ'),\n",
       " ('flashligh', 'flashligh', 'NN'): ('flashlight', 'NN'),\n",
       " ('redsox', 'redsox', 'VBZ'): ('reason', 'VBZ'),\n",
       " ('recived', 'recived', 'VBN'): ('received', 'VBN'),\n",
       " ('receve', 'receve', 'VBP'): ('receive', 'VBP'),\n",
       " ('identfy', 'identfy', 'VB'): ('identify', 'VB'),\n",
       " ('hemspheres', 'hemspheres', 'NNS'): ('hemispheres', 'NNS'),\n",
       " ('universitey', 'universitey', 'NN'): ('university', 'NN'),\n",
       " ('universites', 'universites', 'NNS'): ('universities', 'NNS'),\n",
       " ('univerisities', 'univerisities', 'NNS'): ('universities', 'NNS'),\n",
       " ('reaserch', 'reaserch', 'NN'): ('research', 'NN'),\n",
       " ('skillt', 'skillt', 'NNS'): ('skills', 'NNS'),\n",
       " ('skillt', 'skillt', 'NN'): ('skills', 'NN'),\n",
       " ('facinated', 'facinated', 'VBN'): ('fascinated', 'VBN'),\n",
       " ('diferents', 'diferents', 'NNS'): ('different', 'NNS'),\n",
       " ('oishii', 'oishii', 'FW'): ('rishi', 'FW'),\n",
       " ('beginging', 'beginging', 'NN'): ('beginning', 'NN'),\n",
       " ('buisness', 'buisness', 'NN'): ('business', 'NN'),\n",
       " ('fainally', 'fainally', 'RB'): ('finally', 'RB'),\n",
       " ('frienship', 'frienship', 'NN'): ('friendship', 'NN'),\n",
       " ('fellings', 'fellings', 'NNS'): ('feelings', 'NNS'),\n",
       " ('sms', 'sms', 'NNS'): ('sims', 'NNS'),\n",
       " ('smth', 'smth', 'NN'): ('smith', 'NN'),\n",
       " ('tryin', 'try', 'VBN'): ('trying', 'VBN'),\n",
       " ('benfits', 'benfits', 'NNS'): ('benefits', 'NNS'),\n",
       " ('frindes', 'frindes', 'NNS'): ('fringes', 'NNS'),\n",
       " ('frickles', 'frickles', 'NNS'): ('freckles', 'NNS'),\n",
       " ('befor', 'befor', 'VB'): ('before', 'VB'),\n",
       " ('typeing', 'typeing', 'VBG'): ('typing', 'VBG'),\n",
       " ('difrent', 'difrent', 'JJ'): ('different', 'JJ'),\n",
       " ('freind', 'freind', 'VBN'): ('friend', 'VBN'),\n",
       " ('famus', 'famus', 'JJ'): ('famous', 'JJ'),\n",
       " ('remenber', 'remenber', 'VBP'): ('remember', 'VBP'),\n",
       " ('hamberge', 'hamberge', 'NN'): ('hamburg', 'NN'),\n",
       " ('bedwins', 'bedwins', 'NNS'): ('begins', 'NNS'),\n",
       " ('seung', 'seung', 'SYM'): ('sung', 'SYM'),\n",
       " ('freetime', 'freetime', 'NN'): ('freebie', 'NN'),\n",
       " ('fantasys', 'fantasys', 'NN'): ('fantasy', 'NN'),\n",
       " ('farme', 'farme', 'NN'): ('farm', 'NN'),\n",
       " ('relized', 'relized', 'VBD'): ('realized', 'VBD'),\n",
       " ('pratice', 'pratice', 'VB'): ('practice', 'VB'),\n",
       " ('favorate', 'favorate', 'JJ'): ('favourite', 'JJ'),\n",
       " ('ucia', 'ucia', 'NN'): ('lucia', 'NN'),\n",
       " ('jewelery', 'jewelery', 'NN'): ('jewellery', 'NN'),\n",
       " ('execirses', 'execirses', 'NNS'): ('exercises', 'NNS'),\n",
       " ('complet', 'complet', 'NN'): ('complete', 'NN'),\n",
       " ('grandfahter', 'grandfahter', 'NN'): ('grandfather', 'NN'),\n",
       " ('swiming', 'swiming', 'VBG'): ('swimming', 'VBG'),\n",
       " ('inglourious', 'inglourious', 'JJ'): ('inglorious', 'JJ'),\n",
       " ('companys', 'company', 'NN'): ('company', 'NN'),\n",
       " ('embarassed', 'embarassed', 'VBN'): ('embarrassed', 'VBN'),\n",
       " ('pepers', 'pepers', 'NNS'): ('papers', 'NNS'),\n",
       " ('embacy', 'embacy', 'NN'): ('ebay', 'NN'),\n",
       " ('competion', 'competion', 'NN'): ('completion', 'NN'),\n",
       " ('dc', 'dc', 'NN'): ('do', 'NN'),\n",
       " ('huumii', 'huumii', 'NN'): ('humid', 'NN'),\n",
       " ('inforamtion', 'inforamtion', 'NN'): ('information', 'NN'),\n",
       " ('emam', 'emam', 'NN'): ('exam', 'NN'),\n",
       " ('dautgher', 'dautgher', 'NN'): ('daughter', 'NN'),\n",
       " ('comption', 'comption', 'NN'): ('compton', 'NN'),\n",
       " ('compurter', 'compurter', 'NN'): ('computer', 'NN'),\n",
       " ('comunication', 'comunication', 'NN'): ('communication', 'NN'),\n",
       " ('counrty', 'counrty', 'NN'): ('county', 'NN'),\n",
       " ('concl', 'concl', 'NN'): ('conc', 'NN'),\n",
       " ('peole', 'peole', 'NN'): ('people', 'NN'),\n",
       " ('comming', 'comming', 'VBG'): ('coming', 'VBG'),\n",
       " ('phybe', 'phybe', 'NNPS'): ('phebe', 'NNPS'),\n",
       " ('sponsership', 'sponsership', 'NN'): ('sponsorship', 'NN'),\n",
       " ('interet', 'interet', 'JJ'): ('internet', 'JJ'),\n",
       " ('patiens', 'patiens', 'NNS'): ('patients', 'NNS'),\n",
       " ('patner', 'patner', 'NN'): ('partner', 'NN'),\n",
       " ('cm', 'cm', 'JJ'): ('pm', 'JJ'),\n",
       " ('scholership', 'scholership', 'NN'): ('scholarship', 'NN'),\n",
       " ('happend', 'happend', 'VBN'): ('happen', 'VBN'),\n",
       " ('cohee', 'cohee', 'NN'): ('cohen', 'NN'),\n",
       " ('peautiful', 'peautiful', 'JJ'): ('beautiful', 'JJ'),\n",
       " ('donot', 'donot', 'NN'): ('donor', 'NN'),\n",
       " ('sciencetist', 'sciencetist', 'NN'): ('scientist', 'NN'),\n",
       " ('taht', 'taht', 'NN'): ('that', 'NN'),\n",
       " ('taht', 'taht', 'IN'): ('that', 'IN'),\n",
       " ('encourge', 'encourge', 'VB'): ('encourage', 'VB'),\n",
       " ('decaprio', 'decaprio', 'NN'): ('dicaprio', 'NN'),\n",
       " ('pencreas', 'pencreas', 'NNS'): ('pancreas', 'NNS'),\n",
       " ('deases', 'deases', 'NNS'): ('leases', 'NNS'),\n",
       " ('huumii', 'huumii', 'JJ'): ('humid', 'JJ'),\n",
       " ('indivisual', 'indivisual', 'JJ'): ('individual', 'JJ'),\n",
       " ('propetic', 'propetic', 'JJ'): ('prophetic', 'JJ'),\n",
       " ('senteces', 'senteces', 'NNS'): ('sentences', 'NNS'),\n",
       " ('convinient', 'convinient', 'JJ'): ('convenient', 'JJ'),\n",
       " ('prodactive', 'prodactive', 'JJ'): ('productive', 'JJ'),\n",
       " ('hapiness', 'hapiness', 'NN'): ('happiness', 'NN'),\n",
       " ('shaour', 'shaour', 'NN'): ('hour', 'NN'),\n",
       " ('douctor', 'douctor', 'NN'): ('doctor', 'NN'),\n",
       " ('studnts', 'studnts', 'NNS'): ('students', 'NNS'),\n",
       " ('guk', 'guk', 'NN'): ('guy', 'NN'),\n",
       " ('downloader', 'downloader', 'NN'): ('downloaded', 'NN'),\n",
       " ('elementry', 'elementry', 'NN'): ('elementary', 'NN'),\n",
       " ('cupon', 'cupon', 'NN'): ('upon', 'NN'),\n",
       " ('seperated', 'seperated', 'VBD'): ('separated', 'VBD'),\n",
       " ('dvice', 'dvice', 'NN'): ('advice', 'NN'),\n",
       " ('studi', 'studi', 'NN'): ('study', 'NN'),\n",
       " ('dvd', 'dvd', 'NN'): ('did', 'NN'),\n",
       " ('cosco', 'cosco', 'VB'): ('cisco', 'VB'),\n",
       " ('strss', 'strss', 'NN'): ('stress', 'NN'),\n",
       " ('sharities', 'sharities', 'NNS'): ('charities', 'NNS'),\n",
       " ('duaghter', 'duaghter', 'NN'): ('daughter', 'NN'),\n",
       " ('convenint', 'convenint', 'NN'): ('convenient', 'NN'),\n",
       " ('ediger', 'ediger', 'NN'): ('edger', 'NN'),\n",
       " ('seccion', 'seccion', 'NN'): ('section', 'NN'),\n",
       " ('huur', 'huur', 'NN'): ('hour', 'NN'),\n",
       " ('plase', 'plase', 'NN'): ('please', 'NN'),\n",
       " ('proffesional', 'proffesional', 'JJ'): ('professional', 'JJ'),\n",
       " ('dangerouse', 'dangerouse', 'RB'): ('dangerous', 'RB'),\n",
       " ('secound', 'secound', 'NN'): ('second', 'NN'),\n",
       " ('conteiner', 'conteiner', 'NN'): ('container', 'NN'),\n",
       " ('inbabitant', 'inbabitant', 'NN'): ('inhabitant', 'NN'),\n",
       " ('continously', 'continously', 'RB'): ('continuously', 'RB'),\n",
       " ('inadition', 'inadition', 'NN'): ('tradition', 'NN'),\n",
       " ('contradiccion', 'contradiccion', 'NN'): ('contradiction', 'NN'),\n",
       " ('ee', 'ee', 'NN'): ('be', 'NN'),\n",
       " ('improvment', 'improvment', 'NN'): ('improvement', 'NN'),\n",
       " ('improvies', 'improvies', 'NNS'): ('improves', 'NNS'),\n",
       " ('educationers', 'educationers', 'NNS'): ('educations', 'NNS'),\n",
       " ('immegration', 'immegration', 'NN'): ('immigration', 'NN'),\n",
       " ('techology', 'techology', 'NN'): ('technology', 'NN'),\n",
       " ('roomates', 'roomates', 'NNS'): ('roommates', 'NNS'),\n",
       " ('riscos', 'riscos', 'NN'): ('discos', 'NN'),\n",
       " ('countrys', 'countrys', 'NN'): ('country', 'NN'),\n",
       " ('rissoto', 'rissoto', 'NN'): ('risotto', 'NN'),\n",
       " ('dishs', 'dishs', 'NN'): ('dish', 'NN'),\n",
       " ('ourown', 'ourown', 'JJ'): ('brown', 'JJ'),\n",
       " ('countrys', 'countrys', 'JJ'): ('country', 'JJ'),\n",
       " ('gaviotas', 'gaviotas', 'NNPS'): ('gravitas', 'NNPS'),\n",
       " ('everytime', 'everytime', 'JJ'): ('overtime', 'JJ'),\n",
       " ('pittsburh', 'pittsburh', 'NN'): ('pittsburgh', 'NN'),\n",
       " ('chado', 'chado', 'NN'): ('chad', 'NN'),\n",
       " ('rsa', 'rsa', 'JJ'): ('rosa', 'JJ'),\n",
       " ('ruels', 'ruels', 'NNS'): ('rules', 'NNS'),\n",
       " ('runing', 'runing', 'VBG'): ('running', 'VBG'),\n",
       " ('thoses', 'thoses', 'NNS'): ('those', 'NNS'),\n",
       " ('thoes', 'thoes', 'NNS'): ('those', 'NNS'),\n",
       " ('cermony', 'cermony', 'NN'): ('ceremony', 'NN'),\n",
       " ('iddition', 'iddition', 'NN'): ('addition', 'NN'),\n",
       " ('precussionist', 'precussionist', 'NN'): ('percussionist', 'NN'),\n",
       " ('caractor', 'caractor', 'NN'): ('character', 'NN'),\n",
       " ('garrage', 'garrage', 'NN'): ('garage', 'NN'),\n",
       " ('qintian', 'qintian', 'JJ'): ('indian', 'JJ'),\n",
       " ('retreived', 'retreived', 'VBN'): ('retrieved', 'VBN'),\n",
       " ('togther', 'togther', 'NN'): ('together', 'NN'),\n",
       " ('cabenet', 'cabenet', 'NN'): ('cabinet', 'NN'),\n",
       " ('caffienated', 'caffienated', 'JJ'): ('caffeinated', 'JJ'),\n",
       " ('retrived', 'retrived', 'VBD'): ('retrieved', 'VBD'),\n",
       " ('porgram', 'porgram', 'NN'): ('program', 'NN'),\n",
       " ('tofel', 'tofel', 'JJ'): ('towel', 'JJ'),\n",
       " ('gamt', 'gamt', 'NN'): ('game', 'NN'),\n",
       " ('deside', 'deside', 'VBP'): ('decide', 'VBP'),\n",
       " ('similer', 'similer', 'JJ'): ('similar', 'JJ'),\n",
       " ('judu', 'judu', 'NN'): ('judy', 'NN'),\n",
       " ('juares', 'juares', 'NNS'): ('juarez', 'NNS'),\n",
       " ('tobacoo', 'tobacoo', 'NN'): ('tobacco', 'NN'),\n",
       " ('campony', 'campony', 'NN'): ('company', 'NN'),\n",
       " ('countrise', 'countrise', 'NN'): ('countries', 'NN'),\n",
       " ('overspeeding', 'overspeeding', 'VBG'): ('overspending', 'VBG'),\n",
       " ('cleanning', 'cleanning', 'VBG'): ('cleaning', 'VBG'),\n",
       " ('saterday', 'saterday', 'JJ'): ('saturday', 'JJ'),\n",
       " ('choosen', 'choosen', 'VBN'): ('choose', 'VBN'),\n",
       " ('speacial', 'speacial', 'JJ'): ('special', 'JJ'),\n",
       " ('saudia', 'saudia', 'JJ'): ('saudi', 'JJ'),\n",
       " ('servise', 'servise', 'NN'): ('service', 'NN'),\n",
       " ('diffirences', 'diffirences', 'NNS'): ('differences', 'NNS'),\n",
       " ('shoping', 'shoping', 'VBG'): ('shopping', 'VBG'),\n",
       " ('sould', 'sould', 'MD'): ('would', 'MD'),\n",
       " ('inthe', 'inthe', 'JJ'): ('the', 'JJ'),\n",
       " ('tenis', 'tenis', 'NN'): ('tennis', 'NN'),\n",
       " ('doctore', 'doctore', 'NN'): ('doctor', 'NN'),\n",
       " ('intersting', 'intersting', 'JJ'): ('interesting', 'JJ'),\n",
       " ('proplems', 'proplems', 'NNS'): ('problems', 'NNS'),\n",
       " ('shold', 'shold', 'VBP'): ('should', 'VBP'),\n",
       " ('dokalah', 'dokalah', 'NN'): ('douala', 'NN'),\n",
       " ('defferent', 'defferent', 'JJ'): ('different', 'JJ'),\n",
       " ('enviromental', 'enviromental', 'JJ'): ('environmental', 'JJ'),\n",
       " ('gloval', 'gloval', 'JJ'): ('global', 'JJ'),\n",
       " ('equipament', 'equipament', 'NN'): ('equipment', 'NN'),\n",
       " ('erarchy', 'erarchy', 'NN'): ('eparchy', 'NN'),\n",
       " ('shuld', 'shuld', 'MD'): ('should', 'MD'),\n",
       " ('espicially', 'espicially', 'RB'): ('especially', 'RB'),\n",
       " ('demis', 'demis', 'NN'): ('demos', 'NN'),\n",
       " ('harmfull', 'harmfull', 'NN'): ('harmful', 'NN'),\n",
       " ('thesedays', 'thesedays', 'NNS'): ('tuesdays', 'NNS'),\n",
       " ('girlfreind', 'girlfreind', 'NN'): ('girlfriend', 'NN'),\n",
       " ('palyful', 'palyful', 'JJ'): ('playful', 'JJ'),\n",
       " ('ipots', 'ipots', 'VB'): ('spots', 'VB'),\n",
       " ('ipots', 'ipots', 'NNS'): ('spots', 'NNS'),\n",
       " ('ther', 'ther', 'RB'): ('the', 'RB'),\n",
       " ('esl', 'esl', 'NN'): ('est', 'NN'),\n",
       " ('droped', 'droped', 'VBD'): ('dropped', 'VBD'),\n",
       " ('hourse', 'hourse', 'NN'): ('house', 'NN'),\n",
       " ('housband', 'housband', 'NN'): ('husband', 'NN'),\n",
       " ('chidren', 'chidren', 'NN'): ('children', 'NN'),\n",
       " ('giudlines', 'giudlines', 'NNS'): ('guidelines', 'NNS'),\n",
       " ('draems', 'draems', 'NNS'): ('dreams', 'NNS'),\n",
       " ('swimed', 'swimed', 'VBD'): ('swiped', 'VBD'),\n",
       " ('lllinois', 'lllinois', 'NN'): ('illinois', 'NN'),\n",
       " ('nto', 'nto', 'IN'): ('to', 'IN'),\n",
       " ('whoes', 'whoes', 'NNS'): ('shoes', 'NNS'),\n",
       " ('waiteress', 'waiteress', 'NN'): ('waitress', 'NN'),\n",
       " ('zhong', 'zhong', 'NN'): ('hong', 'NN'),\n",
       " ('wors', 'wors', 'NNS'): ('work', 'NNS'),\n",
       " ('negatif', 'negatif', 'JJ'): ('negative', 'JJ'),\n",
       " ('vaiety', 'vaiety', 'NN'): ('variety', 'NN'),\n",
       " ('wel', 'wel', 'VB'): ('we', 'VB'),\n",
       " ('mmonitored', 'mmonitored', 'VBN'): ('monitored', 'VBN'),\n",
       " ('wais', 'wais', 'NN'): ('was', 'NN'),\n",
       " ('aouther', 'aouther', 'NN'): ('souther', 'NN'),\n",
       " ('whe', 'whe', 'WDT'): ('the', 'WDT'),\n",
       " ('yurika', 'yurika', 'FW'): ('eureka', 'FW'),\n",
       " ('nutrional', 'nutrional', 'JJ'): ('national', 'JJ'),\n",
       " ('adition', 'adition', 'NN'): ('edition', 'NN'),\n",
       " ('vaction', 'vaction', 'NN'): ('action', 'NN'),\n",
       " ('angery', 'angery', 'RB'): ('angry', 'RB'),\n",
       " ('agaist', 'agaist', 'VBP'): ('against', 'VBP'),\n",
       " ('lockar', 'lockar', 'NN'): ('locker', 'NN'),\n",
       " ('littel', 'littel', 'NN'): ('little', 'NN'),\n",
       " ('vistoer', 'vistoer', 'NN'): ('sister', 'NN'),\n",
       " ('wheater', 'wheater', 'NN'): ('heater', 'NN'),\n",
       " ('agro', 'agro', 'NN'): ('ago', 'NN'),\n",
       " ('weaknes', 'weaknes', 'NNS'): ('weakness', 'NNS'),\n",
       " ('artical', 'artical', 'JJ'): ('article', 'JJ'),\n",
       " ('adviced', 'adviced', 'VBD'): ('advice', 'VBD'),\n",
       " ('mitting', 'mitting', 'VBG'): ('sitting', 'VBG'),\n",
       " ('wel', 'wel', 'SYM'): ('we', 'SYM'),\n",
       " ('xiang', 'xiang', 'JJ'): ('xian', 'JJ'),\n",
       " ('accross', 'accross', 'IN'): ('across', 'IN'),\n",
       " ('advertisments', 'advertisments', 'NNS'): ('advertisements', 'NNS'),\n",
       " ('nute', 'nute', 'JJ'): ('note', 'JJ'),\n",
       " ('masjed', 'masjed', 'NN'): ('masked', 'NN'),\n",
       " ('ususally', 'ususally', 'RB'): ('usually', 'RB'),\n",
       " ('ysanne', 'ysanne', 'FW'): ('anne', 'FW'),\n",
       " ('musium', 'musium', 'NN'): ('museum', 'NN'),\n",
       " ('motocycles', 'motocycles', 'NNS'): ('motorcycles', 'NNS'),\n",
       " ('wirting', 'wirting', 'NN'): ('writing', 'NN'),\n",
       " ('actriss', 'actriss', 'NN'): ('actress', 'NN'),\n",
       " ('abillity', 'abillity', 'NN'): ('ability', 'NN'),\n",
       " ('attendence', 'attendence', 'NN'): ('attendance', 'NN'),\n",
       " ('mahn', 'mahn', 'NN'): ('main', 'NN'),\n",
       " ('aplliances', 'aplliances', 'NNS'): ('appliances', 'NNS'),\n",
       " ('yei', 'yei', 'NN'): ('yes', 'NN'),\n",
       " ('meate', 'meate', 'NN'): ('meat', 'NN'),\n",
       " ('whever', 'whever', 'WRB'): ('whoever', 'WRB'),\n",
       " ('aggs', 'aggs', 'NN'): ('ages', 'NN'),\n",
       " ('nuturition', 'nuturition', 'NN'): ('nutrition', 'NN'),\n",
       " ('aod', 'aod', 'NN'): ('and', 'NN'),\n",
       " ('armee', 'armee', 'NN'): ('armed', 'NN'),\n",
       " ('moslim', 'moslim', 'NN'): ('muslim', 'NN'),\n",
       " ('mojito', 'mojito', 'NN'): ('monitor', 'NN'),\n",
       " ('assisstent', 'assisstent', 'NN'): ('assessment', 'NN'),\n",
       " ('minitues', 'minitues', 'NNS'): ('minutes', 'NNS'),\n",
       " ('lt', 'lt', 'JJ'): ('it', 'JJ'),\n",
       " ('manho', 'manho', 'NNS'): ('mango', 'NNS'),\n",
       " ('lke', 'lke', 'VBP'): ('like', 'VBP'),\n",
       " ('maked', 'maked', 'VBD'): ('make', 'VBD'),\n",
       " ('whith', 'whith', 'JJ'): ('with', 'JJ'),\n",
       " ('manho', 'manho', 'NN'): ('mango', 'NN'),\n",
       " ('samll', 'samll', 'JJ'): ('small', 'JJ'),\n",
       " ('dificult', 'dificult', 'JJ'): ('difficult', 'JJ'),\n",
       " ('spaice', 'spaice', 'NN'): ('space', 'NN'),\n",
       " ('naitve', 'naitve', 'JJ'): ('native', 'JJ'),\n",
       " ('vitual', 'vitual', 'JJ'): ('virtual', 'JJ'),\n",
       " ('imortant', 'imortant', 'JJ'): ('important', 'JJ'),\n",
       " ('opinon', 'opinon', 'NN'): ('opinion', 'NN'),\n",
       " ('grabed', 'grabed', 'VBD'): ('graded', 'VBD'),\n",
       " ('addittion', 'addittion', 'NN'): ('addition', 'NN'),\n",
       " ('architechure', 'architechure', 'NN'): ('architecture', 'NN'),\n",
       " ('resoures', 'resoures', 'VBZ'): ('resources', 'VBZ'),\n",
       " ('ghoat', 'ghoat', 'NN'): ('ghost', 'NN'),\n",
       " ('othere', 'othere', 'JJ'): ('other', 'JJ'),\n",
       " ('bodeis', 'bodeis', 'NN'): ('bodies', 'NN'),\n",
       " ('boild', 'boild', 'JJ'): ('build', 'JJ'),\n",
       " ('scholl', 'scholl', 'VB'): ('school', 'VB'),\n",
       " ('chirpings', 'chirpings', 'NNS'): ('chirping', 'NNS'),\n",
       " ('interduce', 'interduce', 'VB'): ('interface', 'VB'),\n",
       " ('whtch', 'whtch', 'VBP'): ('which', 'VBP'),\n",
       " ('kony', 'kony', 'JJ'): ('sony', 'JJ'),\n",
       " ('trasic', 'trasic', 'JJ'): ('tragic', 'JJ'),\n",
       " ('bofeah', 'bofeah', 'JJ'): ('boreal', 'JJ'),\n",
       " ('chinses', 'chinses', 'VBZ'): ('chines', 'VBZ'),\n",
       " ('carlories', 'carlories', 'NNS'): ('calories', 'NNS'),\n",
       " ('paragraphes', 'paragraphes', 'NNS'): ('paragraphs', 'NNS'),\n",
       " ('helpfull', 'helpfull', 'NN'): ('helpful', 'NN'),\n",
       " ('kowen', 'kowen', 'VBZ'): ('owen', 'VBZ'),\n",
       " ('poeple', 'poeple', 'JJ'): ('people', 'JJ'),\n",
       " ('aboout', 'aboout', 'IN'): ('about', 'IN'),\n",
       " ('paragrah', 'paragrah', 'NN'): ('paragraph', 'NN'),\n",
       " ('blieve', 'blieve', 'VBP'): ('believe', 'VBP'),\n",
       " ('skiny', 'skiny', 'JJ'): ('skin', 'JJ'),\n",
       " ('clothers', 'clothers', 'NNS'): ('clothes', 'NNS'),\n",
       " ('verious', 'verious', 'JJ'): ('various', 'JJ'),\n",
       " ('teath', 'teath', 'NN'): ('death', 'NN'),\n",
       " ('costom', 'costom', 'NN'): ('custom', 'NN'),\n",
       " ('marabinos', 'marabinos', 'NNS'): ('arabinose', 'NNS'),\n",
       " ('biggist', 'biggist', 'NN'): ('biggest', 'NN'),\n",
       " ('movment', 'movment', 'NN'): ('moment', 'NN'),\n",
       " ('heenes', 'heenes', 'NNPS'): ('teens', 'NNPS'),\n",
       " ('langage', 'langage', 'NN'): ('language', 'NN'),\n",
       " ('coporations', 'coporations', 'NNS'): ('corporations', 'NNS'),\n",
       " ('castel', 'castel', 'NN'): ('castle', 'NN'),\n",
       " ('disapeared', 'disapeared', 'VBN'): ('disappeared', 'VBN'),\n",
       " ('adecision', 'adecision', 'NN'): ('decision', 'NN'),\n",
       " ('assalam', 'assalam', 'NN'): ('assam', 'NN'),\n",
       " ('healt', 'healt', 'NN'): ('health', 'NN'),\n",
       " ('publi', 'publi', 'JJ'): ('public', 'JJ'),\n",
       " ('vatiety', 'vatiety', 'NN'): ('variety', 'NN'),\n",
       " ('jaya', 'jaya', 'NN'): ('java', 'NN'),\n",
       " ('luctures', 'luctures', 'VBZ'): ('lectures', 'VBZ'),\n",
       " ('throgh', 'throgh', 'RB'): ('through', 'RB'),\n",
       " ('bettery', 'bettery', 'NN'): ('better', 'NN'),\n",
       " ('similarties', 'similarties', 'NNS'): ('similarities', 'NNS'),\n",
       " ('studyin', 'studyin', 'JJ'): ('studying', 'JJ'),\n",
       " ('studyin', 'studyin', 'VBN'): ('studying', 'VBN'),\n",
       " ('everytime', 'everytime', 'RB'): ('overtime', 'RB'),\n",
       " ('prohibe', 'prohibe', 'VB'): ('profile', 'VB'),\n",
       " ('llth', 'llth', 'NN'): ('lith', 'NN'),\n",
       " ('bestfriend', 'bestfriend', 'JJ'): ('befriend', 'JJ'),\n",
       " ('insted', 'insted', 'VBN'): ('instead', 'VBN'),\n",
       " ('maind', 'maind', 'VB'): ('main', 'VB'),\n",
       " ('japanses', 'japanses', 'NNPS'): ('japaneses', 'NNPS'),\n",
       " ('colleage', 'colleage', 'NN'): ('college', 'NN'),\n",
       " ('negetive', 'negetive', 'JJ'): ('negative', 'JJ'),\n",
       " ('tamim', 'tamim', 'VB'): ('tamil', 'VB'),\n",
       " ('haveing', 'haveing', 'VBG'): ('having', 'VBG'),\n",
       " ('importnat', 'importnat', 'JJ'): ('important', 'JJ'),\n",
       " ('pcked', 'pcked', 'VBD'): ('picked', 'VBD'),\n",
       " ('jeddah', 'jeddah', 'NN'): ('judah', 'NN'),\n",
       " ('vegatables', 'vegatables', 'NNS'): ('vegetables', 'NNS'),\n",
       " ('woked', 'woked', 'VBD'): ('worked', 'VBD'),\n",
       " ('pluged', 'pluged', 'VBN'): ('plugged', 'VBN'),\n",
       " ('ar', 'ar', 'VBD'): ('a', 'VBD'),\n",
       " ('feathres', 'feathres', 'NNS'): ('features', 'NNS'),\n",
       " ('houres', 'houres', 'NNS'): ('hours', 'NNS'),\n",
       " ('treeflights', 'treeflights', 'NNS'): ('streetlights', 'NNS'),\n",
       " ('amune', 'amune', 'NN'): ('amine', 'NN'),\n",
       " ('emt', 'emt', 'NN'): ('est', 'NN'),\n",
       " ('delecious', 'delecious', 'JJ'): ('delicious', 'JJ'),\n",
       " ('jennnifer', 'jennnifer', 'RB'): ('jennifer', 'RB'),\n",
       " ('laguages', 'laguages', 'NNS'): ('languages', 'NNS'),\n",
       " ('cmfortable', 'cmfortable', 'JJ'): ('comfortable', 'JJ'),\n",
       " ('factores', 'factores', 'NNS'): ('factors', 'NNS'),\n",
       " ('nimono', 'nimono', 'FW'): ('kimono', 'FW'),\n",
       " ('studing', 'studing', 'NN'): ('studying', 'NN'),\n",
       " ('treval', 'treval', 'NN'): ('reval', 'NN'),\n",
       " ('emportant', 'emportant', 'JJ'): ('important', 'JJ'),\n",
       " ('trevel', 'trevel', 'NN'): ('travel', 'NN'),\n",
       " ('bigest', 'bigest', 'JJS'): ('biggest', 'JJS'),\n",
       " ...}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misspell_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incorporating back into pelic_df\n",
    "\n",
    "pelic_df['tok_POS_corrected'] = pelic_df['tok_lem_POS'].apply\\\n",
    "(lambda row: [misspell_dict[(x[0].lower(),x[1],x[2])] if (x[0].lower(),x[1],x[2]) in misspell_dict else (x[0],x[2]) for x in row])\n",
    "\n",
    "# One minor issue is that this will make misspelled items lower case when originally upper case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('My', 'my', 'PRP$'), ('friend', 'friend', 'NN'), ('is', 'be', 'VBZ'), ('realy', 'realy', 'JJ'), ('nise', 'nise', 'RB'), ('guy', 'guy', 'NN'), ('.', '.', '.'), ('I', 'i', 'PRP'), ('like', 'like', 'VBP'), ('hem', 'hem', 'JJ'), ('becuase', 'becuase', 'NN'), ('he', 'he', 'PRP'), ('is', 'be', 'VBZ'), ('friendlly', 'friendlly', 'RB'), ('and', 'and', 'CC'), ('lovliy', 'lovliy', 'NN'), ('.', '.', '.'))\n",
      "[('My', 'PRP$'), ('friend', 'NN'), ('is', 'VBZ'), ('real', 'JJ'), ('nice', 'RB'), ('guy', 'NN'), ('.', '.'), ('I', 'PRP'), ('like', 'VBP'), ('hem', 'JJ'), ('because', 'NN'), ('he', 'PRP'), ('is', 'VBZ'), ('friendly', 'RB'), ('and', 'CC'), ('lovely', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Checking with 'becuase'\n",
    "\n",
    "print(pelic_df.loc[pelic_df.text.str.contains('becuase')].iloc[1,11]) #uncorrected\n",
    "print(pelic_df.loc[pelic_df.text.str.contains('becuase')].iloc[1,12]) #corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that many approrpriate corrections have been made, including _beccuase_ -> _because_ , _nise_ -> _nice_ , and _lovily_ -> _lovely_ .  \n",
    "Importantly, incorrect spellings that are actual words, e.g. _hem_ (should be _him_ in this case) are not corrected. In addition, if the POS is incorrectly tagged, often due to the learner language, then the result may not be correct, e.g. _realy_ (marked as an adj) -> _real_ rather than _really_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anon_id</th>\n",
       "      <th>L1</th>\n",
       "      <th>gender</th>\n",
       "      <th>course_id</th>\n",
       "      <th>level_id</th>\n",
       "      <th>class_id</th>\n",
       "      <th>question_id</th>\n",
       "      <th>version</th>\n",
       "      <th>text_len</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tok_lem_POS</th>\n",
       "      <th>tok_POS_corrected</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eq0</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>Male</td>\n",
       "      <td>149</td>\n",
       "      <td>4</td>\n",
       "      <td>g</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>177</td>\n",
       "      <td>I met my friend Nife while I was studying in a...</td>\n",
       "      <td>[I, met, my, friend, Nife, while, I, was, stud...</td>\n",
       "      <td>((I, i, PRP), (met, meet, VBD), (my, my, PRP$)...</td>\n",
       "      <td>[(I, PRP), (met, VBD), (my, PRP$), (friend, NN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>am8</td>\n",
       "      <td>Thai</td>\n",
       "      <td>Female</td>\n",
       "      <td>149</td>\n",
       "      <td>4</td>\n",
       "      <td>g</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>137</td>\n",
       "      <td>Ten years ago, I met a women on the train betw...</td>\n",
       "      <td>[Ten, years, ago, ,, I, met, a, women, on, the...</td>\n",
       "      <td>((Ten, ten, CD), (years, year, NNS), (ago, ago...</td>\n",
       "      <td>[(Ten, CD), (years, NNS), (ago, RB), (,, ,), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dk5</td>\n",
       "      <td>Turkish</td>\n",
       "      <td>Female</td>\n",
       "      <td>115</td>\n",
       "      <td>4</td>\n",
       "      <td>w</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>In my country we usually don't use tea bags. F...</td>\n",
       "      <td>[In, my, country, we, usually, do, n't, use, t...</td>\n",
       "      <td>((In, in, IN), (my, my, PRP$), (country, count...</td>\n",
       "      <td>[(In, IN), (my, PRP$), (country, NN), (we, PRP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dk5</td>\n",
       "      <td>Turkish</td>\n",
       "      <td>Female</td>\n",
       "      <td>115</td>\n",
       "      <td>4</td>\n",
       "      <td>w</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>I organized the instructions by time.</td>\n",
       "      <td>[I, organized, the, instructions, by, time, .]</td>\n",
       "      <td>((I, i, PRP), (organized, organize, VBD), (the...</td>\n",
       "      <td>[(I, PRP), (organized, VBD), (the, DT), (instr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ad1</td>\n",
       "      <td>Korean</td>\n",
       "      <td>Female</td>\n",
       "      <td>115</td>\n",
       "      <td>4</td>\n",
       "      <td>w</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>59</td>\n",
       "      <td>First, prepare a port, loose tea, and cup.\\nSe...</td>\n",
       "      <td>[First, ,, prepare, a, port, ,, loose, tea, ,,...</td>\n",
       "      <td>((First, first, RB), (,, ,, ,), (prepare, prep...</td>\n",
       "      <td>[(First, RB), (,, ,), (prepare, VB), (a, DT), ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          anon_id       L1  gender course_id level_id class_id question_id  \\\n",
       "answer_id                                                                    \n",
       "1             eq0   Arabic    Male       149        4        g           5   \n",
       "2             am8     Thai  Female       149        4        g           5   \n",
       "3             dk5  Turkish  Female       115        4        w          12   \n",
       "4             dk5  Turkish  Female       115        4        w          13   \n",
       "5             ad1   Korean  Female       115        4        w          12   \n",
       "\n",
       "          version  text_len  \\\n",
       "answer_id                     \n",
       "1               1       177   \n",
       "2               1       137   \n",
       "3               1        64   \n",
       "4               1         6   \n",
       "5               1        59   \n",
       "\n",
       "                                                        text  \\\n",
       "answer_id                                                      \n",
       "1          I met my friend Nife while I was studying in a...   \n",
       "2          Ten years ago, I met a women on the train betw...   \n",
       "3          In my country we usually don't use tea bags. F...   \n",
       "4                      I organized the instructions by time.   \n",
       "5          First, prepare a port, loose tea, and cup.\\nSe...   \n",
       "\n",
       "                                                      tokens  \\\n",
       "answer_id                                                      \n",
       "1          [I, met, my, friend, Nife, while, I, was, stud...   \n",
       "2          [Ten, years, ago, ,, I, met, a, women, on, the...   \n",
       "3          [In, my, country, we, usually, do, n't, use, t...   \n",
       "4             [I, organized, the, instructions, by, time, .]   \n",
       "5          [First, ,, prepare, a, port, ,, loose, tea, ,,...   \n",
       "\n",
       "                                                 tok_lem_POS  \\\n",
       "answer_id                                                      \n",
       "1          ((I, i, PRP), (met, meet, VBD), (my, my, PRP$)...   \n",
       "2          ((Ten, ten, CD), (years, year, NNS), (ago, ago...   \n",
       "3          ((In, in, IN), (my, my, PRP$), (country, count...   \n",
       "4          ((I, i, PRP), (organized, organize, VBD), (the...   \n",
       "5          ((First, first, RB), (,, ,, ,), (prepare, prep...   \n",
       "\n",
       "                                           tok_POS_corrected  \n",
       "answer_id                                                     \n",
       "1          [(I, PRP), (met, VBD), (my, PRP$), (friend, NN...  \n",
       "2          [(Ten, CD), (years, NNS), (ago, RB), (,, ,), (...  \n",
       "3          [(In, IN), (my, PRP$), (country, NN), (we, PRP...  \n",
       "4          [(I, PRP), (organized, VBD), (the, DT), (instr...  \n",
       "5          [(First, RB), (,, ,), (prepare, VB), (a, DT), ...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pelic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out new PELIC_compiled.csv\n",
    "\n",
    "pelic_df.to_csv('PELIC_compiled_spellcorrected.csv', encoding='utf-8', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle new pelic_df dataframe\n",
    "pelic_df.to_pickle('pelic_spellcorrected.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If preferred, this entire spelling correctin process can also be applied to [`answer.csv`]() instead of `PELIC_compiled`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Corrected-spelling)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
