{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PELIC spelling\n",
    "\n",
    "This notebook adds further processing to `PELIC_compiled.csv`  in the [`PELIC-dataset`](https://github.com/ELI-Data-Mining-Group/PELIC-dataset) repo by creating a column of tok_POS whose spelling has been automatically corrected.\n",
    "\n",
    "**Notebook contents:**\n",
    "- [Building `non_words_df`](#Building-non_words_df)\n",
    "- [Building `misspell_df`](#Building-misspell_df)\n",
    "- [Possible segmentation](#Applying-segmentation)\n",
    "- [Applying spelling correction](#Applying-spelling-correction)\n",
    "- [Incorporating corrections into `pelic_df`](#Incorporating-corrections-into-pelic_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building non_words_df\n",
    "In this section, we build a dataframe, `non_words_df`, which collects all of the non-words from the PELIC dataset (in `PELIC_compiled.csv`). The final dataframe has the following columns:\n",
    "- `non_word`: tuples with the non-words and their parts of speech\n",
    "- `sentence`: the complete sentence containing the non-word to provide context\n",
    "- `answer_id`: the id of the text they come from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anon_id</th>\n",
       "      <th>L1</th>\n",
       "      <th>gender</th>\n",
       "      <th>course_id</th>\n",
       "      <th>level_id</th>\n",
       "      <th>class_id</th>\n",
       "      <th>question_id</th>\n",
       "      <th>version</th>\n",
       "      <th>text_len</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tok_lem_POS</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eq0</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>Male</td>\n",
       "      <td>149</td>\n",
       "      <td>4</td>\n",
       "      <td>g</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>177</td>\n",
       "      <td>I met my friend Nife while I was studying in a...</td>\n",
       "      <td>[I, met, my, friend, Nife, while, I, was, stud...</td>\n",
       "      <td>((I, i, PRP), (met, meet, VBD), (my, my, PRP$)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>am8</td>\n",
       "      <td>Thai</td>\n",
       "      <td>Female</td>\n",
       "      <td>149</td>\n",
       "      <td>4</td>\n",
       "      <td>g</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>137</td>\n",
       "      <td>Ten years ago, I met a women on the train betw...</td>\n",
       "      <td>[Ten, years, ago, ,, I, met, a, women, on, the...</td>\n",
       "      <td>((Ten, ten, CD), (years, year, NNS), (ago, ago...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dk5</td>\n",
       "      <td>Turkish</td>\n",
       "      <td>Female</td>\n",
       "      <td>115</td>\n",
       "      <td>4</td>\n",
       "      <td>w</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>In my country we usually don't use tea bags. F...</td>\n",
       "      <td>[In, my, country, we, usually, do, n't, use, t...</td>\n",
       "      <td>((In, in, IN), (my, my, PRP$), (country, count...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dk5</td>\n",
       "      <td>Turkish</td>\n",
       "      <td>Female</td>\n",
       "      <td>115</td>\n",
       "      <td>4</td>\n",
       "      <td>w</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>I organized the instructions by time.</td>\n",
       "      <td>[I, organized, the, instructions, by, time, .]</td>\n",
       "      <td>((I, i, PRP), (organized, organize, VBD), (the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ad1</td>\n",
       "      <td>Korean</td>\n",
       "      <td>Female</td>\n",
       "      <td>115</td>\n",
       "      <td>4</td>\n",
       "      <td>w</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>59</td>\n",
       "      <td>First, prepare a port, loose tea, and cup.\\nSe...</td>\n",
       "      <td>[First, ,, prepare, a, port, ,, loose, tea, ,,...</td>\n",
       "      <td>((First, first, RB), (,, ,, ,), (prepare, prep...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          anon_id       L1  gender course_id level_id class_id question_id  \\\n",
       "answer_id                                                                    \n",
       "1             eq0   Arabic    Male       149        4        g           5   \n",
       "2             am8     Thai  Female       149        4        g           5   \n",
       "3             dk5  Turkish  Female       115        4        w          12   \n",
       "4             dk5  Turkish  Female       115        4        w          13   \n",
       "5             ad1   Korean  Female       115        4        w          12   \n",
       "\n",
       "          version  text_len  \\\n",
       "answer_id                     \n",
       "1               1       177   \n",
       "2               1       137   \n",
       "3               1        64   \n",
       "4               1         6   \n",
       "5               1        59   \n",
       "\n",
       "                                                        text  \\\n",
       "answer_id                                                      \n",
       "1          I met my friend Nife while I was studying in a...   \n",
       "2          Ten years ago, I met a women on the train betw...   \n",
       "3          In my country we usually don't use tea bags. F...   \n",
       "4                      I organized the instructions by time.   \n",
       "5          First, prepare a port, loose tea, and cup.\\nSe...   \n",
       "\n",
       "                                                      tokens  \\\n",
       "answer_id                                                      \n",
       "1          [I, met, my, friend, Nife, while, I, was, stud...   \n",
       "2          [Ten, years, ago, ,, I, met, a, women, on, the...   \n",
       "3          [In, my, country, we, usually, do, n't, use, t...   \n",
       "4             [I, organized, the, instructions, by, time, .]   \n",
       "5          [First, ,, prepare, a, port, ,, loose, tea, ,,...   \n",
       "\n",
       "                                                 tok_lem_POS  \n",
       "answer_id                                                     \n",
       "1          ((I, i, PRP), (met, meet, VBD), (my, my, PRP$)...  \n",
       "2          ((Ten, ten, CD), (years, year, NNS), (ago, ago...  \n",
       "3          ((In, in, IN), (my, my, PRP$), (country, count...  \n",
       "4          ((I, i, PRP), (organized, organize, VBD), (the...  \n",
       "5          ((First, first, RB), (,, ,, ,), (prepare, prep...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in PELIC_compiled.csv\n",
    "\n",
    "pelic_df = pd.read_csv(\"../PELIC-dataset/PELIC_compiled.csv\", index_col = 'answer_id', # answer_id is unique\n",
    "                      dtype = {'level_id':'object','question_id':'object','version':'object','course_id':'object'}, # str not ints\n",
    "                               converters={'tokens':literal_eval,'tok_lem_POS':literal_eval}) # read in as lists\n",
    "pelic_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The focus here is the `tok_lem_POS` column, but all columns will be kept as the entire df will be written out at the end of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating small dataframe to be used for finding non-words\n",
    "\n",
    "non_words = pelic_df[['text','tok_lem_POS']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** For spelling correction, it is necessary to decide what list of words will be used for determining if a word is real or not.\n",
    "\n",
    "Here, we use the [`SCOWL_condensed.txt`](https://github.com/ELI-Data-Mining-Group/PELIC-spelling/blob/master/SCOWL_condensed.txt) file which is a combination of wordlists available for download at http://wordlist.aspell.net/. We include items from all the dictionaries _except_ the abbreviations dictionary. For a detailed look at the compilation of this dictionary, please see the [SCOWL_wordlist](https://github.com/ELI-Data-Mining-Group/PELIC-spelling/blob/master/SCOWL_wordlist.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['uberrima', 'integrates', 'brid', 'separable', 'ftnerr']\n"
     ]
    }
   ],
   "source": [
    "#Reading in SCOWL_condensed as a set as a lookup list for spelling (500k words)\n",
    "\n",
    "scowl = set(open(\"SCOWL_condensed.txt\", \"r\").read().split('\\n'))\n",
    "print(random.sample(scowl,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a list of words which should be considered words but which were previously being labelled as non-words. These items have been manually added to this list based on output later in this notebook. Most of these items are food items, names, or abbreviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n",
      "['adha', 'adj', 'ahamed', 'alaikum', 'anonurlpage', 'antiretroviral', 'arpa', 'beyonce', 'bibimbap', 'bio', 'biodiesel', 'bioethanol', 'bulgogi', 'bundang', 'cafe', 'carnaval', 'cds', 'cf', 'co', 'comscore', 'cyber', 'ddukboggi', 'def', 'dr', 'eg', 'eid', 'electrospray', 'entrees', 'erectus', 'etc', 'fiance', 'fiancee', 'fiter', 'fitir', 'fitr', 'fl', 'freediving', 'fukubukuro', 'geolinguist', 'hikikomori', 'hp', 'ibt', 'iq', 'iriver', 'jetta', 'jul', 'kabsa', 'kaled', 'kawader', 'km', 'leisureville', 'll', 'maamool', 'mayumi', 'mcdonalds', 'min', 'mongongo', 'nc', 'neuro', 'nian', 'notting', 'okroshka', 'onsen', 'pajeon', 'pbt', 'pc', 'pcs', 'pp', 'pudim', 'puket', 'samear', 'shui', 'sq', 'st', 'staycation', 'sth', 'taoyuan', 'toefl', 'trans', 'transgene', 'tv', 'unsub', 'va', 'vol', 'vs', 'webaholic', 'webaholics', 'webaholism', 'wenjing', 'woong', 'yaoming', 'ying', 'yingdong', 'yugong', 'yuval', 'zi', '']\n"
     ]
    }
   ],
   "source": [
    "scowl_supp = open(\"scowl_supp.txt\", \"r\").read().split(',')\n",
    "scowl_supp = [x[2:-1] for x in scowl_supp]\n",
    "print(len(scowl_supp))\n",
    "print(scowl_supp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py:5303: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "# Lower case all toks\n",
    "\n",
    "non_words.tok_lem_POS = non_words.tok_lem_POS.apply(lambda row: [(x[0].lower(),x[1],x[2]) for x in row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find non-words\n",
    "\n",
    "def spell_check(tok_lem_POS_list):\n",
    "    word_list = scowl # Choose word_list here. Default is scowl described above.\n",
    "    not_in_word_list = []\n",
    "    for tok_lem_POS in tok_lem_POS_list:\n",
    "        if tok_lem_POS[0] not in word_list and tok_lem_POS[0] not in scowl_supp:\n",
    "            not_in_word_list.append(tok_lem_POS)\n",
    "    return not_in_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# Apply spell check function to find all misspelled-words. \n",
    "\n",
    "non_words['misspelled_words'] = non_words.tok_lem_POS.apply(spell_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tok_lem_POS</th>\n",
       "      <th>misspelled_words</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I met my friend Nife while I was studying in a...</td>\n",
       "      <td>[(i, i, PRP), (met, meet, VBD), (my, my, PRP$)...</td>\n",
       "      <td>[(., ., .), (., ., .), (., ., .), (;, ;, :), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ten years ago, I met a women on the train betw...</td>\n",
       "      <td>[(ten, ten, CD), (years, year, NNS), (ago, ago...</td>\n",
       "      <td>[(,, ,, ,), (,, ,, ,), (., ., .), (;, ;, :), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In my country we usually don't use tea bags. F...</td>\n",
       "      <td>[(in, in, IN), (my, my, PRP$), (country, count...</td>\n",
       "      <td>[(., ., .), (,, ,, ,), (., ., .), (., ., .), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I organized the instructions by time.</td>\n",
       "      <td>[(i, i, PRP), (organized, organize, VBD), (the...</td>\n",
       "      <td>[(., ., .)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>First, prepare a port, loose tea, and cup.\\nSe...</td>\n",
       "      <td>[(first, first, RB), (,, ,, ,), (prepare, prep...</td>\n",
       "      <td>[(,, ,, ,), (,, ,, ,), (,, ,, ,), (., ., .), (...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        text  \\\n",
       "answer_id                                                      \n",
       "1          I met my friend Nife while I was studying in a...   \n",
       "2          Ten years ago, I met a women on the train betw...   \n",
       "3          In my country we usually don't use tea bags. F...   \n",
       "4                      I organized the instructions by time.   \n",
       "5          First, prepare a port, loose tea, and cup.\\nSe...   \n",
       "\n",
       "                                                 tok_lem_POS  \\\n",
       "answer_id                                                      \n",
       "1          [(i, i, PRP), (met, meet, VBD), (my, my, PRP$)...   \n",
       "2          [(ten, ten, CD), (years, year, NNS), (ago, ago...   \n",
       "3          [(in, in, IN), (my, my, PRP$), (country, count...   \n",
       "4          [(i, i, PRP), (organized, organize, VBD), (the...   \n",
       "5          [(first, first, RB), (,, ,, ,), (prepare, prep...   \n",
       "\n",
       "                                            misspelled_words  \n",
       "answer_id                                                     \n",
       "1          [(., ., .), (., ., .), (., ., .), (;, ;, :), (...  \n",
       "2          [(,, ,, ,), (,, ,, ,), (., ., .), (;, ;, :), (...  \n",
       "3          [(., ., .), (,, ,, ,), (., ., .), (., ., .), (...  \n",
       "4                                                [(., ., .)]  \n",
       "5          [(,, ,, ,), (,, ,, ,), (,, ,, ,), (., ., .), (...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_words.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding context to the dataframe\n",
    "Seeing the mistakes in the context of a sentence will allow for better manual checking if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# Sent-tokenizing the text\n",
    "\n",
    "non_words['sentence'] = non_words['text'].apply(lambda x: nltk.sent_tokenize(x))\n",
    "\n",
    "# And delete text column which is no longer needed\n",
    "\n",
    "del non_words['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1182\n",
      "['well-educated', 'one-bedroom', 'record-keeping', 'air-conditioning', 'low-calorie', 'long-distance', 'sixty-year-old', 're-read', 'oscar-worthy', 'white-haired']\n"
     ]
    }
   ],
   "source": [
    "# Checking for hyphenated words tagged as misspellings because SCOWL doesn't contain hypenated words\n",
    "\n",
    "hyphenated = set([x[0] for x in [x for y in non_words.misspelled_words.to_list() for x in y] if '-' in x[0]])\n",
    "print(len(hyphenated))\n",
    "print(list(hyphenated)[:10])\n",
    "\n",
    "# These need to be removed from the non-words dataframe if composed of valid words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['', \"'\"],\n",
       " ['', '***', '****'],\n",
       " ['', '+'],\n",
       " ['', '.'],\n",
       " ['',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  ''],\n",
       " [\"'\", ''],\n",
       " ['.', ''],\n",
       " ['/', ''],\n",
       " ['\\\\\\\\', ''],\n",
       " ['^', '^'],\n",
       " ['al', 'qaida'],\n",
       " ['austro', 'hungarian'],\n",
       " ['cd', 'rom'],\n",
       " ['co', 'authored'],\n",
       " ['co', 'ed'],\n",
       " ['co', 'educational'],\n",
       " ['co', 'exist'],\n",
       " ['co', 'existence'],\n",
       " ['co', 'founded'],\n",
       " ['co', 'founder'],\n",
       " ['co', 'founders'],\n",
       " ['co', 'host'],\n",
       " ['co', 'op'],\n",
       " ['co', 'operate'],\n",
       " ['co', 'operation'],\n",
       " ['co', 'pay'],\n",
       " ['co', 'pilot'],\n",
       " ['co', 'sleeping'],\n",
       " ['co', 'star'],\n",
       " ['co', 'worker'],\n",
       " ['co', 'workers'],\n",
       " ['co', 'written'],\n",
       " ['co', 'wrote'],\n",
       " ['mah', 'jong'],\n",
       " ['mid', '80s'],\n",
       " ['pay', 'tv'],\n",
       " ['roly', 'poly'],\n",
       " ['socio', 'cultural'],\n",
       " ['socio', 'economic'],\n",
       " ['trans', 'fat'],\n",
       " ['vis', 'a', 'vis'],\n",
       " ['wal', 'mart']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyphenated items whose components are not in scowl - possible misspellings or punctuation strings\n",
    "\n",
    "sorted([y for y in [x.split('-') for x in hyphenated] if y[0] not in scowl or y[1] not in scowl])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After manual checking, all the hypenated words are punctuation, real words (or true productive use of affixes) and can be removed from the non-words df.\n",
    "\n",
    "The following two cells \n",
    "1. remove all the hypenated words from the dataframe\n",
    "2. remove all words that don't contain a letter\n",
    "\n",
    "However, as all hyphenated word are fine, we will instead just eliminate all words that are not purely composed of letters. This will have the effect of removing the following categories from the dataframe:\n",
    "- punctuation\n",
    "- hyphenated words (e.g. well-known)\n",
    "- contractions (e.g. 'll, 've)\n",
    "- years (e.g. 1950s)\n",
    "- ordinals (e.g. 1st, 2nd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing hypenated words\n",
    "\n",
    "# non_words.misspelled_words = non_words.misspelled_words.apply(lambda row: [x for x in row if x[0] not in hyphenated])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing items that are only numbers or punctuation\n",
    "# .isalpha() cannot be used without 'any' as this also removes hyphenated words\n",
    "\n",
    "# non_words.misspelled_words = non_words.misspelled_words.apply(lambda row: [x for x in row if any(y.isalpha() for y in x[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "684453\n",
      "20599\n"
     ]
    }
   ],
   "source": [
    "# Checking initial length of non_words list\n",
    "\n",
    "print(len([x for y in non_words.misspelled_words.to_list() for x in y]))\n",
    "print(len(set([x for y in non_words.misspelled_words.to_list() for x in y])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py:5303: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "# Removing items that are not purely alpha\n",
    "\n",
    "non_words.misspelled_words = non_words.misspelled_words.apply(lambda row: [x for x in row if x[0].isalpha()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30205\n",
      "15993\n"
     ]
    }
   ],
   "source": [
    "# Checking affect of removal\n",
    "\n",
    "print(len([x for y in non_words.misspelled_words.to_list() for x in y]))\n",
    "print(len(set([x for y in non_words.misspelled_words.to_list() for x in y])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing proper names - NNP, NNPS\n",
    "\n",
    "non_words.misspelled_words = non_words.misspelled_words.apply(lambda row: [x for x in row if x[2] != 'NNP' and x[1] != 'NNPS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16832\n",
      "11094\n"
     ]
    }
   ],
   "source": [
    "# Checking affect of removal\n",
    "\n",
    "print(len([x for y in non_words.misspelled_words.to_list() for x in y]))\n",
    "print(len(set([x for y in non_words.misspelled_words.to_list() for x in y])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing all words with length 1\n",
    "\n",
    "non_words.misspelled_words = non_words.misspelled_words.apply(lambda row: [x for x in row if len(x[0]) > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16826\n",
      "11090\n"
     ]
    }
   ],
   "source": [
    "# Checking affect of removal\n",
    "\n",
    "print(len([x for y in non_words.misspelled_words.to_list() for x in y]))\n",
    "print(len(set([x for y in non_words.misspelled_words.to_list() for x in y])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tok_lem_POS</th>\n",
       "      <th>misspelled_words</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[(i, i, PRP), (met, meet, VBD), (my, my, PRP$)...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[I met my friend Nife while I was studying in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[(ten, ten, CD), (years, year, NNS), (ago, ago...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Ten years ago, I met a women on the train bet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[(in, in, IN), (my, my, PRP$), (country, count...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[In my country we usually don't use tea bags.,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[(i, i, PRP), (organized, organize, VBD), (the...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[I organized the instructions by time.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[(first, first, RB), (,, ,, ,), (prepare, prep...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[First, prepare a port, loose tea, and cup., S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tok_lem_POS misspelled_words  \\\n",
       "answer_id                                                                       \n",
       "1          [(i, i, PRP), (met, meet, VBD), (my, my, PRP$)...               []   \n",
       "2          [(ten, ten, CD), (years, year, NNS), (ago, ago...               []   \n",
       "3          [(in, in, IN), (my, my, PRP$), (country, count...               []   \n",
       "4          [(i, i, PRP), (organized, organize, VBD), (the...               []   \n",
       "5          [(first, first, RB), (,, ,, ,), (prepare, prep...               []   \n",
       "\n",
       "                                                    sentence  \n",
       "answer_id                                                     \n",
       "1          [I met my friend Nife while I was studying in ...  \n",
       "2          [Ten years ago, I met a women on the train bet...  \n",
       "3          [In my country we usually don't use tea bags.,...  \n",
       "4                    [I organized the instructions by time.]  \n",
       "5          [First, prepare a port, loose tea, and cup., S...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_words.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new dataframe so that each misspelling token is a separate row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing rows with no misspellings\n",
    "\n",
    "non_words2 = non_words.loc[non_words.misspelled_words.str.len() > 0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploding the lists in misspelled words so that each misspelling gets its own row\n",
    "\n",
    "non_words2 = non_words2.explode('misspelled_words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping the answer_id (which is no longer unique) as a separate column\n",
    "\n",
    "non_words2 = non_words2.reset_index(drop = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer_id</th>\n",
       "      <th>tok_lem_POS</th>\n",
       "      <th>misspelled_words</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>[(i, i, PRP), (organized, organize, VBD), (the...</td>\n",
       "      <td>(beacause, beacause, NN)</td>\n",
       "      <td>[I organized the instructions by time, beacaus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>[(to, to, TO), (make, make, VB), (tea, tea, NN...</td>\n",
       "      <td>(wallmart, wallmart, NN)</td>\n",
       "      <td>[To make tea, nothing is easier, even if somet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>[(first, first, RB), (,, ,, ,), (you, you, PRP...</td>\n",
       "      <td>(dovn, dovn, NN)</td>\n",
       "      <td>[First, you should take some hot water, you ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>[(first, first, RB), (,, ,, ,), (you, you, PRP...</td>\n",
       "      <td>(mircowave, mircowave, VBP)</td>\n",
       "      <td>[First, you should take some hot water, you ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "      <td>[(every, every, DT), (paragragh, paragragh, NN...</td>\n",
       "      <td>(paragragh, paragragh, NN)</td>\n",
       "      <td>[Every paragragh's instructions depend on a ma...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   answer_id                                        tok_lem_POS  \\\n",
       "0          8  [(i, i, PRP), (organized, organize, VBD), (the...   \n",
       "1         11  [(to, to, TO), (make, make, VB), (tea, tea, NN...   \n",
       "2         13  [(first, first, RB), (,, ,, ,), (you, you, PRP...   \n",
       "3         13  [(first, first, RB), (,, ,, ,), (you, you, PRP...   \n",
       "4         16  [(every, every, DT), (paragragh, paragragh, NN...   \n",
       "\n",
       "              misspelled_words  \\\n",
       "0     (beacause, beacause, NN)   \n",
       "1     (wallmart, wallmart, NN)   \n",
       "2             (dovn, dovn, NN)   \n",
       "3  (mircowave, mircowave, VBP)   \n",
       "4   (paragragh, paragragh, NN)   \n",
       "\n",
       "                                            sentence  \n",
       "0  [I organized the instructions by time, beacaus...  \n",
       "1  [To make tea, nothing is easier, even if somet...  \n",
       "2  [First, you should take some hot water, you ca...  \n",
       "3  [First, you should take some hot water, you ca...  \n",
       "4  [Every paragragh's instructions depend on a ma...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking final non_words2 dataframe\n",
    "\n",
    "non_words2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16826\n",
      "11090\n"
     ]
    }
   ],
   "source": [
    "# Total number of non-words (tokens)\n",
    "print(len(non_words2))\n",
    "\n",
    "# Total number of non-words (types)\n",
    "print(non_words2.misspelled_words.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a dataframe of misspellings\n",
    "In the `non-words2` dataframe above, each row is an occurrence of a misspelling (i.e. _tokens_ ). We also want a dataframe where each row is a misspelling _type_ with frequency information attached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gathering the total misspellings\n",
    "\n",
    "total_misspellings = [x for x in non_words2['misspelled_words']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To keep an account of misspelling frequency, create a dictionary with frequencies\n",
    "\n",
    "misspell_freq_dict = {}\n",
    "for word in total_misspellings:\n",
    "    if word not in misspell_freq_dict:\n",
    "        misspell_freq_dict[word] = 1\n",
    "    else:\n",
    "        misspell_freq_dict[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('eperience', 'eperience', 'NN'), ('goodisms', 'goodisms', 'NN'), ('difrenet', 'difrenet', 'NN'), ('specaly', 'specaly', 'NN'), ('instrcture', 'instrcture', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "print(random.sample(list(misspell_freq_dict),5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11090"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove duplicates\n",
    "\n",
    "final_misspellings = sorted(list(set(total_misspellings)))\n",
    "len(final_misspellings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aa</td>\n",
       "      <td>aa</td>\n",
       "      <td>VB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aabout</td>\n",
       "      <td>aabout</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aad</td>\n",
       "      <td>aad</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aain</td>\n",
       "      <td>aain</td>\n",
       "      <td>VBP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aare</td>\n",
       "      <td>aare</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0       1    2\n",
       "0      aa      aa   VB\n",
       "1  aabout  aabout   IN\n",
       "2     aad     aad   JJ\n",
       "3    aain    aain  VBP\n",
       "4    aare    aare   IN"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Constructing misspell_df\n",
    "\n",
    "misspell_df = pd.DataFrame(final_misspellings)\n",
    "misspell_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming columns to match other DataFrames in this notebook\n",
    "\n",
    "misspell_df.rename(columns = {0: 'misspelling',1:'lemma',2:'POS'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>misspelling</th>\n",
       "      <th>lemma</th>\n",
       "      <th>POS</th>\n",
       "      <th>tok_lem_POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aa</td>\n",
       "      <td>aa</td>\n",
       "      <td>VB</td>\n",
       "      <td>(aa, aa, VB)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aabout</td>\n",
       "      <td>aabout</td>\n",
       "      <td>IN</td>\n",
       "      <td>(aabout, aabout, IN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aad</td>\n",
       "      <td>aad</td>\n",
       "      <td>JJ</td>\n",
       "      <td>(aad, aad, JJ)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aain</td>\n",
       "      <td>aain</td>\n",
       "      <td>VBP</td>\n",
       "      <td>(aain, aain, VBP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aare</td>\n",
       "      <td>aare</td>\n",
       "      <td>IN</td>\n",
       "      <td>(aare, aare, IN)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  misspelling   lemma  POS           tok_lem_POS\n",
       "0          aa      aa   VB          (aa, aa, VB)\n",
       "1      aabout  aabout   IN  (aabout, aabout, IN)\n",
       "2         aad     aad   JJ        (aad, aad, JJ)\n",
       "3        aain    aain  VBP     (aain, aain, VBP)\n",
       "4        aare    aare   IN      (aare, aare, IN)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recreating tok_lem_POS column to match dictionary\n",
    "\n",
    "misspell_df['tok_lem_POS'] = list(zip(misspell_df.misspelling, misspell_df.lemma, misspell_df.POS))\n",
    "misspell_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping dictionary to DataFrame\n",
    "\n",
    "misspell_df['freq'] = misspell_df['tok_lem_POS'].map(misspell_freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting by frequency\n",
    "\n",
    "misspell_df = misspell_df.sort_values(by=['freq'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>misspelling</th>\n",
       "      <th>lemma</th>\n",
       "      <th>POS</th>\n",
       "      <th>tok_lem_POS</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alot</td>\n",
       "      <td>alot</td>\n",
       "      <td>NN</td>\n",
       "      <td>(alot, alot, NN)</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>studing</td>\n",
       "      <td>studing</td>\n",
       "      <td>VBG</td>\n",
       "      <td>(studing, studing, VBG)</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>goverment</td>\n",
       "      <td>goverment</td>\n",
       "      <td>NN</td>\n",
       "      <td>(goverment, goverment, NN)</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>grammer</td>\n",
       "      <td>grammer</td>\n",
       "      <td>NN</td>\n",
       "      <td>(grammer, grammer, NN)</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>becuase</td>\n",
       "      <td>becuase</td>\n",
       "      <td>NN</td>\n",
       "      <td>(becuase, becuase, NN)</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  misspelling      lemma  POS                 tok_lem_POS  freq\n",
       "0        alot       alot   NN            (alot, alot, NN)   127\n",
       "1     studing    studing  VBG     (studing, studing, VBG)    74\n",
       "2   goverment  goverment   NN  (goverment, goverment, NN)    47\n",
       "3     grammer    grammer   NN      (grammer, grammer, NN)    29\n",
       "4     becuase    becuase   NN      (becuase, becuase, NN)    28"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Resetting index\n",
    "misspell_df = misspell_df.reset_index(drop = True)\n",
    "misspell_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scowl_supp\n",
    "The following is the basis for the 'scowl_supp' list used earlier. Here, errors with a frequency of 10 or more were manually checked, and if determined to be a real word, were added to the scowl_supp list. There were originally 267 items which met this criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>misspelling</th>\n",
       "      <th>lemma</th>\n",
       "      <th>POS</th>\n",
       "      <th>tok_lem_POS</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alot</td>\n",
       "      <td>alot</td>\n",
       "      <td>NN</td>\n",
       "      <td>(alot, alot, NN)</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>studing</td>\n",
       "      <td>studing</td>\n",
       "      <td>VBG</td>\n",
       "      <td>(studing, studing, VBG)</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>goverment</td>\n",
       "      <td>goverment</td>\n",
       "      <td>NN</td>\n",
       "      <td>(goverment, goverment, NN)</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>grammer</td>\n",
       "      <td>grammer</td>\n",
       "      <td>NN</td>\n",
       "      <td>(grammer, grammer, NN)</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>becuase</td>\n",
       "      <td>becuase</td>\n",
       "      <td>NN</td>\n",
       "      <td>(becuase, becuase, NN)</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>contry</td>\n",
       "      <td>contry</td>\n",
       "      <td>NN</td>\n",
       "      <td>(contry, contry, NN)</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>childrens</td>\n",
       "      <td>child</td>\n",
       "      <td>NNS</td>\n",
       "      <td>(childrens, child, NNS)</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>apartement</td>\n",
       "      <td>apartement</td>\n",
       "      <td>NN</td>\n",
       "      <td>(apartement, apartement, NN)</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>fastfood</td>\n",
       "      <td>fastfood</td>\n",
       "      <td>NN</td>\n",
       "      <td>(fastfood, fastfood, NN)</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>desicion</td>\n",
       "      <td>desicion</td>\n",
       "      <td>NN</td>\n",
       "      <td>(desicion, desicion, NN)</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>77 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   misspelling       lemma  POS                   tok_lem_POS  freq\n",
       "0         alot        alot   NN              (alot, alot, NN)   127\n",
       "1      studing     studing  VBG       (studing, studing, VBG)    74\n",
       "2    goverment   goverment   NN    (goverment, goverment, NN)    47\n",
       "3      grammer     grammer   NN        (grammer, grammer, NN)    29\n",
       "4      becuase     becuase   NN        (becuase, becuase, NN)    28\n",
       "..         ...         ...  ...                           ...   ...\n",
       "72      contry      contry   NN          (contry, contry, NN)    10\n",
       "73   childrens       child  NNS       (childrens, child, NNS)    10\n",
       "74  apartement  apartement   NN  (apartement, apartement, NN)    10\n",
       "75    fastfood    fastfood   NN      (fastfood, fastfood, NN)    10\n",
       "76    desicion    desicion   NN      (desicion, desicion, NN)    10\n",
       "\n",
       "[77 rows x 5 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(misspell_df.loc[misspell_df.freq >= 10]))\n",
    "misspell_df.loc[misspell_df.freq >= 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible segmentation\n",
    "\n",
    "Selected segmenter and spellchecker: SymSpell https://github.com/mammothb/symspellpy\n",
    "\n",
    "There is a dictionary file which which needs to be installed (saved to repo):\n",
    "[frequency_dictionary_en_82_765.txt](https://symspellpy.readthedocs.io/en/latest/users/installing.html)\n",
    "\n",
    "To install symspellpy the first time, use pip in command line: `pip install -U symspellpy`\n",
    "\n",
    "Prior to spelling correct, we first consider using the segmenter. This is a potentially useful first step as misspellings like 'alot' or 'dogmeat' will be separated into 'a lot' and 'dog meat' rather than corrected to a single word like 'lot'.  \n",
    "\n",
    "However, when segementing misspellings, the segmenter over performs, segmenting non-words into real words where it was clearly not intended, e.g. _improtant_ into _imp rot ant_ or _befor_ into _be for_. As such, the segmenting will not be automated. \n",
    "\n",
    "Instead, one manual segmentation will be carried out: _alot_ -> _a lot_ since _alot_ is the most common misspelling remaining in our dataframe (127 occurrences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 23135851162),\n",
       " ('of', 13151942776),\n",
       " ('and', 12997637966),\n",
       " ('to', 12136980858),\n",
       " ('a', 9081174698)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting up symspell\n",
    "\n",
    "from itertools import islice\n",
    "import pkg_resources\n",
    "from symspellpy import SymSpell\n",
    "from symspellpy import Verbosity\n",
    "sym_spell = SymSpell()\n",
    "dictionary_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "sym_spell.load_dictionary(dictionary_path, 0, 1)\n",
    "\n",
    "# Print out first 5 elements to demonstrate that dictionary is successfully loaded\n",
    "list(islice(sym_spell.words.items(), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing segmenter with 'alot' and 'dogmeat'\n",
    "\n",
    "# Set max_dictionary_edit_distance to avoid spelling correction\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=0, prefix_length=7)\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "\n",
    "# It is also possible to display frequency with result.distance_sum and edit distance with .log_prob_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating function for applying the above code\n",
    "\n",
    "def get_segments(word):\n",
    "    segments = sym_spell.word_segmentation(word)\n",
    "    if len(segments.corrected_string.split(' ')) > 1 \\\n",
    "    and segments.corrected_string.split(' ')[0] in scowl and segments.corrected_string.split(' ')[1] in scowl:\n",
    "        return segments.corrected_string\n",
    "    else:\n",
    "        return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog meat\n",
      "fireplace\n",
      "becuase\n"
     ]
    }
   ],
   "source": [
    "# Testing function\n",
    "\n",
    "print(get_segments('dogmeat')) # Should be segmented\n",
    "print(get_segments('fireplace')) # Should not be segmented\n",
    "print(get_segments('becuase')) # Should not be segmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>misspelling</th>\n",
       "      <th>lemma</th>\n",
       "      <th>POS</th>\n",
       "      <th>tok_lem_POS</th>\n",
       "      <th>freq</th>\n",
       "      <th>segments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alot</td>\n",
       "      <td>alot</td>\n",
       "      <td>NN</td>\n",
       "      <td>(alot, alot, NN)</td>\n",
       "      <td>127</td>\n",
       "      <td>a lot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>studing</td>\n",
       "      <td>studing</td>\n",
       "      <td>VBG</td>\n",
       "      <td>(studing, studing, VBG)</td>\n",
       "      <td>74</td>\n",
       "      <td>stu ding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>goverment</td>\n",
       "      <td>goverment</td>\n",
       "      <td>NN</td>\n",
       "      <td>(goverment, goverment, NN)</td>\n",
       "      <td>47</td>\n",
       "      <td>g over men t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>grammer</td>\n",
       "      <td>grammer</td>\n",
       "      <td>NN</td>\n",
       "      <td>(grammer, grammer, NN)</td>\n",
       "      <td>29</td>\n",
       "      <td>gramme r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>becuase</td>\n",
       "      <td>becuase</td>\n",
       "      <td>NN</td>\n",
       "      <td>(becuase, becuase, NN)</td>\n",
       "      <td>28</td>\n",
       "      <td>becuase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>beatiful</td>\n",
       "      <td>beatiful</td>\n",
       "      <td>JJ</td>\n",
       "      <td>(beatiful, beatiful, JJ)</td>\n",
       "      <td>28</td>\n",
       "      <td>beat if ul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>differents</td>\n",
       "      <td>differents</td>\n",
       "      <td>NNS</td>\n",
       "      <td>(differents, differents, NNS)</td>\n",
       "      <td>23</td>\n",
       "      <td>different s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>resturant</td>\n",
       "      <td>resturant</td>\n",
       "      <td>NN</td>\n",
       "      <td>(resturant, resturant, NN)</td>\n",
       "      <td>23</td>\n",
       "      <td>re stu rant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>becouse</td>\n",
       "      <td>becouse</td>\n",
       "      <td>IN</td>\n",
       "      <td>(becouse, becouse, IN)</td>\n",
       "      <td>23</td>\n",
       "      <td>becouse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>lifes</td>\n",
       "      <td>life</td>\n",
       "      <td>NN</td>\n",
       "      <td>(lifes, life, NN)</td>\n",
       "      <td>22</td>\n",
       "      <td>life s</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  misspelling       lemma  POS                    tok_lem_POS  freq  \\\n",
       "0        alot        alot   NN               (alot, alot, NN)   127   \n",
       "1     studing     studing  VBG        (studing, studing, VBG)    74   \n",
       "2   goverment   goverment   NN     (goverment, goverment, NN)    47   \n",
       "3     grammer     grammer   NN         (grammer, grammer, NN)    29   \n",
       "4     becuase     becuase   NN         (becuase, becuase, NN)    28   \n",
       "5    beatiful    beatiful   JJ       (beatiful, beatiful, JJ)    28   \n",
       "6  differents  differents  NNS  (differents, differents, NNS)    23   \n",
       "7   resturant   resturant   NN     (resturant, resturant, NN)    23   \n",
       "8     becouse     becouse   IN         (becouse, becouse, IN)    23   \n",
       "9       lifes        life   NN              (lifes, life, NN)    22   \n",
       "\n",
       "       segments  \n",
       "0         a lot  \n",
       "1      stu ding  \n",
       "2  g over men t  \n",
       "3      gramme r  \n",
       "4       becuase  \n",
       "5    beat if ul  \n",
       "6   different s  \n",
       "7   re stu rant  \n",
       "8       becouse  \n",
       "9        life s  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying the function to create a new column\n",
    "\n",
    "misspell_df['segments'] =  misspell_df['misspelling'].apply(get_segments)\n",
    "misspell_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting this new column as segmentation creates false segments of misspelled words\n",
    "\n",
    "del misspell_df['segments']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying spelling correction\n",
    "\n",
    "In some ways SymSpell is not ideal as sentence context is not considered, only general frequencies. However, other well-known spellcheckers (hunspell, pyspell, etc.) use the same strategy - frequency based criteria for suggestions, without considering immediate cotext. As such, we have followed this common practice, but it is important to remember that accuracy of corrected tokens will not be 100% and must be taken into consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "because, 1, 271323986\n"
     ]
    }
   ],
   "source": [
    "# Testing spelling suggestions with 'becuase'\n",
    "\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "dictionary_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "# term_index is the column of the term and count_index is the column of the term frequency\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "\n",
    "input_term = \"becuase\"\n",
    "suggestions = sym_spell.lookup(input_term, Verbosity.CLOSEST, max_edit_distance=2, #Edit distance can be adjusted\n",
    "                               transfer_casing=True, #Optional argument set to ignore case\n",
    "                              include_unknown=True) #Return same word if unknown\n",
    "for suggestion in suggestions:\n",
    "    print(suggestion)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating function for applying the above code\n",
    "\n",
    "def get_suggestions(word):\n",
    "    if len(word) >= 4:\n",
    "        suggestions = sym_spell.lookup(word, Verbosity.CLOSEST,max_edit_distance=2, transfer_casing=True)\n",
    "    else:\n",
    "        suggestions = sym_spell.lookup(word, Verbosity.CLOSEST,max_edit_distance=1, transfer_casing=True)\n",
    "    return [str(x).split(',') for x in suggestions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: The function has a variable edit distance: words of length 4 or more get edit distance of 2, shorter words get edit distance of 1. These preferences can be adjusted in the function if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>misspelling</th>\n",
       "      <th>lemma</th>\n",
       "      <th>POS</th>\n",
       "      <th>tok_lem_POS</th>\n",
       "      <th>freq</th>\n",
       "      <th>suggestions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alot</td>\n",
       "      <td>alot</td>\n",
       "      <td>NN</td>\n",
       "      <td>(alot, alot, NN)</td>\n",
       "      <td>127</td>\n",
       "      <td>[[lot,  1,  106405208], [slot,  1,  21602762],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>studing</td>\n",
       "      <td>studing</td>\n",
       "      <td>VBG</td>\n",
       "      <td>(studing, studing, VBG)</td>\n",
       "      <td>74</td>\n",
       "      <td>[[studying,  1,  9763653], [studding,  1,  345...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>goverment</td>\n",
       "      <td>goverment</td>\n",
       "      <td>NN</td>\n",
       "      <td>(goverment, goverment, NN)</td>\n",
       "      <td>47</td>\n",
       "      <td>[[government,  1,  206582673]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>grammer</td>\n",
       "      <td>grammer</td>\n",
       "      <td>NN</td>\n",
       "      <td>(grammer, grammer, NN)</td>\n",
       "      <td>29</td>\n",
       "      <td>[[grammar,  1,  8019137], [grammes,  1,  13549...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>becuase</td>\n",
       "      <td>becuase</td>\n",
       "      <td>NN</td>\n",
       "      <td>(becuase, becuase, NN)</td>\n",
       "      <td>28</td>\n",
       "      <td>[[because,  1,  271323986]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  misspelling      lemma  POS                 tok_lem_POS  freq  \\\n",
       "0        alot       alot   NN            (alot, alot, NN)   127   \n",
       "1     studing    studing  VBG     (studing, studing, VBG)    74   \n",
       "2   goverment  goverment   NN  (goverment, goverment, NN)    47   \n",
       "3     grammer    grammer   NN      (grammer, grammer, NN)    29   \n",
       "4     becuase    becuase   NN      (becuase, becuase, NN)    28   \n",
       "\n",
       "                                         suggestions  \n",
       "0  [[lot,  1,  106405208], [slot,  1,  21602762],...  \n",
       "1  [[studying,  1,  9763653], [studding,  1,  345...  \n",
       "2                     [[government,  1,  206582673]]  \n",
       "3  [[grammar,  1,  8019137], [grammes,  1,  13549...  \n",
       "4                        [[because,  1,  271323986]]  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying function to create new column\n",
    "\n",
    "misspell_df['suggestions'] =  misspell_df['misspelling'].apply(get_suggestions)\n",
    "misspell_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "736"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking how many items without suggestions\n",
    "\n",
    "len(misspell_df.loc[(misspell_df.suggestions.str.len() == 0),:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Items with no suggestions - these will be left in their original form though manual corrections could be applied if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new column with just the most likely correction (based on frequency)\n",
    "\n",
    "misspell_df['correction'] = [x[0][0] if len(x) != 0 else np.NaN for x in misspell_df['suggestions']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If no correction, use original word\n",
    "\n",
    "misspell_df.correction.fillna(misspell_df.misspelling, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>misspelling</th>\n",
       "      <th>lemma</th>\n",
       "      <th>POS</th>\n",
       "      <th>tok_lem_POS</th>\n",
       "      <th>freq</th>\n",
       "      <th>suggestions</th>\n",
       "      <th>correction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alot</td>\n",
       "      <td>alot</td>\n",
       "      <td>NN</td>\n",
       "      <td>(alot, alot, NN)</td>\n",
       "      <td>127</td>\n",
       "      <td>[[lot,  1,  106405208], [slot,  1,  21602762],...</td>\n",
       "      <td>lot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  misspelling lemma POS       tok_lem_POS  freq  \\\n",
       "0        alot  alot  NN  (alot, alot, NN)   127   \n",
       "\n",
       "                                         suggestions correction  \n",
       "0  [[lot,  1,  106405208], [slot,  1,  21602762],...        lot  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misspell_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>misspelling</th>\n",
       "      <th>lemma</th>\n",
       "      <th>POS</th>\n",
       "      <th>tok_lem_POS</th>\n",
       "      <th>freq</th>\n",
       "      <th>suggestions</th>\n",
       "      <th>correction</th>\n",
       "      <th>correction_POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alot</td>\n",
       "      <td>alot</td>\n",
       "      <td>NN</td>\n",
       "      <td>(alot, alot, NN)</td>\n",
       "      <td>127</td>\n",
       "      <td>[[lot,  1,  106405208], [slot,  1,  21602762],...</td>\n",
       "      <td>lot</td>\n",
       "      <td>(lot, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>studing</td>\n",
       "      <td>studing</td>\n",
       "      <td>VBG</td>\n",
       "      <td>(studing, studing, VBG)</td>\n",
       "      <td>74</td>\n",
       "      <td>[[studying,  1,  9763653], [studding,  1,  345...</td>\n",
       "      <td>studying</td>\n",
       "      <td>(studying, VBG)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>goverment</td>\n",
       "      <td>goverment</td>\n",
       "      <td>NN</td>\n",
       "      <td>(goverment, goverment, NN)</td>\n",
       "      <td>47</td>\n",
       "      <td>[[government,  1,  206582673]]</td>\n",
       "      <td>government</td>\n",
       "      <td>(government, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>grammer</td>\n",
       "      <td>grammer</td>\n",
       "      <td>NN</td>\n",
       "      <td>(grammer, grammer, NN)</td>\n",
       "      <td>29</td>\n",
       "      <td>[[grammar,  1,  8019137], [grammes,  1,  13549...</td>\n",
       "      <td>grammar</td>\n",
       "      <td>(grammar, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>becuase</td>\n",
       "      <td>becuase</td>\n",
       "      <td>NN</td>\n",
       "      <td>(becuase, becuase, NN)</td>\n",
       "      <td>28</td>\n",
       "      <td>[[because,  1,  271323986]]</td>\n",
       "      <td>because</td>\n",
       "      <td>(because, NN)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  misspelling      lemma  POS                 tok_lem_POS  freq  \\\n",
       "0        alot       alot   NN            (alot, alot, NN)   127   \n",
       "1     studing    studing  VBG     (studing, studing, VBG)    74   \n",
       "2   goverment  goverment   NN  (goverment, goverment, NN)    47   \n",
       "3     grammer    grammer   NN      (grammer, grammer, NN)    29   \n",
       "4     becuase    becuase   NN      (becuase, becuase, NN)    28   \n",
       "\n",
       "                                         suggestions  correction  \\\n",
       "0  [[lot,  1,  106405208], [slot,  1,  21602762],...         lot   \n",
       "1  [[studying,  1,  9763653], [studding,  1,  345...    studying   \n",
       "2                     [[government,  1,  206582673]]  government   \n",
       "3  [[grammar,  1,  8019137], [grammes,  1,  13549...     grammar   \n",
       "4                        [[because,  1,  271323986]]     because   \n",
       "\n",
       "     correction_POS  \n",
       "0         (lot, NN)  \n",
       "1   (studying, VBG)  \n",
       "2  (government, NN)  \n",
       "3     (grammar, NN)  \n",
       "4     (because, NN)  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create correction_POS column\n",
    "\n",
    "misspell_df['correction_POS'] = list(zip(misspell_df.correction, misspell_df.POS))\n",
    "misspell_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As described earlier - one manual correction for 'alot' will be added\n",
    "\n",
    "misspell_df.iloc[0,6] = 'a lot'\n",
    "misspell_df.at[0, 'correction_POS'] = ('a','DT'),('lot','NN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting unnecessary columns with duplicate information already contained in the tuple\n",
    "\n",
    "del misspell_df['lemma']\n",
    "del misspell_df['POS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>misspelling</th>\n",
       "      <th>tok_lem_POS</th>\n",
       "      <th>freq</th>\n",
       "      <th>suggestions</th>\n",
       "      <th>correction</th>\n",
       "      <th>correction_POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alot</td>\n",
       "      <td>(alot, alot, NN)</td>\n",
       "      <td>127</td>\n",
       "      <td>[[lot,  1,  106405208], [slot,  1,  21602762],...</td>\n",
       "      <td>a lot</td>\n",
       "      <td>((a, DT), (lot, NN))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>studing</td>\n",
       "      <td>(studing, studing, VBG)</td>\n",
       "      <td>74</td>\n",
       "      <td>[[studying,  1,  9763653], [studding,  1,  345...</td>\n",
       "      <td>studying</td>\n",
       "      <td>(studying, VBG)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>goverment</td>\n",
       "      <td>(goverment, goverment, NN)</td>\n",
       "      <td>47</td>\n",
       "      <td>[[government,  1,  206582673]]</td>\n",
       "      <td>government</td>\n",
       "      <td>(government, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>grammer</td>\n",
       "      <td>(grammer, grammer, NN)</td>\n",
       "      <td>29</td>\n",
       "      <td>[[grammar,  1,  8019137], [grammes,  1,  13549...</td>\n",
       "      <td>grammar</td>\n",
       "      <td>(grammar, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>becuase</td>\n",
       "      <td>(becuase, becuase, NN)</td>\n",
       "      <td>28</td>\n",
       "      <td>[[because,  1,  271323986]]</td>\n",
       "      <td>because</td>\n",
       "      <td>(because, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>beatiful</td>\n",
       "      <td>(beatiful, beatiful, JJ)</td>\n",
       "      <td>28</td>\n",
       "      <td>[[beautiful,  1,  58503804]]</td>\n",
       "      <td>beautiful</td>\n",
       "      <td>(beautiful, JJ)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>differents</td>\n",
       "      <td>(differents, differents, NNS)</td>\n",
       "      <td>23</td>\n",
       "      <td>[[different,  1,  179794224]]</td>\n",
       "      <td>different</td>\n",
       "      <td>(different, NNS)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>resturant</td>\n",
       "      <td>(resturant, resturant, NN)</td>\n",
       "      <td>23</td>\n",
       "      <td>[[restaurant,  1,  48255033]]</td>\n",
       "      <td>restaurant</td>\n",
       "      <td>(restaurant, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>becouse</td>\n",
       "      <td>(becouse, becouse, IN)</td>\n",
       "      <td>23</td>\n",
       "      <td>[[because,  1,  271323986]]</td>\n",
       "      <td>because</td>\n",
       "      <td>(because, IN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>lifes</td>\n",
       "      <td>(lifes, life, NN)</td>\n",
       "      <td>22</td>\n",
       "      <td>[[life,  1,  306559205], [lines,  1,  76806341...</td>\n",
       "      <td>life</td>\n",
       "      <td>(life, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>shoud</td>\n",
       "      <td>(shoud, shoud, VBP)</td>\n",
       "      <td>22</td>\n",
       "      <td>[[should,  1,  402028056], [shout,  1,  396586...</td>\n",
       "      <td>should</td>\n",
       "      <td>(should, VBP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>sould</td>\n",
       "      <td>(sould, sould, VBP)</td>\n",
       "      <td>22</td>\n",
       "      <td>[[would,  1,  572644147], [should,  1,  402028...</td>\n",
       "      <td>would</td>\n",
       "      <td>(would, VBP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>befor</td>\n",
       "      <td>(befor, befor, NN)</td>\n",
       "      <td>21</td>\n",
       "      <td>[[before,  1,  277546019]]</td>\n",
       "      <td>before</td>\n",
       "      <td>(before, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>somthing</td>\n",
       "      <td>(somthing, somthing, VBG)</td>\n",
       "      <td>21</td>\n",
       "      <td>[[something,  1,  131836210], [soothing,  1,  ...</td>\n",
       "      <td>something</td>\n",
       "      <td>(something, VBG)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>everytime</td>\n",
       "      <td>(everytime, everytime, NN)</td>\n",
       "      <td>18</td>\n",
       "      <td>[[overtime,  2,  4095339]]</td>\n",
       "      <td>overtime</td>\n",
       "      <td>(overtime, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>referes</td>\n",
       "      <td>(referes, referes, VBZ)</td>\n",
       "      <td>18</td>\n",
       "      <td>[[refers,  1,  11869181], [referee,  1,  29419...</td>\n",
       "      <td>refers</td>\n",
       "      <td>(refers, VBZ)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>tution</td>\n",
       "      <td>(tution, tution, NN)</td>\n",
       "      <td>18</td>\n",
       "      <td>[[tuition,  1,  9860033]]</td>\n",
       "      <td>tuition</td>\n",
       "      <td>(tuition, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>apartament</td>\n",
       "      <td>(apartament, apartament, NN)</td>\n",
       "      <td>18</td>\n",
       "      <td>[[apartment,  1,  30771172]]</td>\n",
       "      <td>apartment</td>\n",
       "      <td>(apartment, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>becouse</td>\n",
       "      <td>(becouse, becouse, NN)</td>\n",
       "      <td>18</td>\n",
       "      <td>[[because,  1,  271323986]]</td>\n",
       "      <td>because</td>\n",
       "      <td>(because, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>jop</td>\n",
       "      <td>(jop, jop, NN)</td>\n",
       "      <td>18</td>\n",
       "      <td>[[top,  1,  484213771], [job,  1,  177706929],...</td>\n",
       "      <td>top</td>\n",
       "      <td>(top, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>appartment</td>\n",
       "      <td>(appartment, appartment, NN)</td>\n",
       "      <td>17</td>\n",
       "      <td>[[apartment,  1,  30771172]]</td>\n",
       "      <td>apartment</td>\n",
       "      <td>(apartment, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>freind</td>\n",
       "      <td>(freind, freind, NN)</td>\n",
       "      <td>17</td>\n",
       "      <td>[[friend,  1,  154527125]]</td>\n",
       "      <td>friend</td>\n",
       "      <td>(friend, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>begining</td>\n",
       "      <td>(begining, begining, NN)</td>\n",
       "      <td>17</td>\n",
       "      <td>[[beginning,  1,  49148844]]</td>\n",
       "      <td>beginning</td>\n",
       "      <td>(beginning, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>confortable</td>\n",
       "      <td>(confortable, confortable, JJ)</td>\n",
       "      <td>17</td>\n",
       "      <td>[[comfortable,  1,  19520487], [conformable,  ...</td>\n",
       "      <td>comfortable</td>\n",
       "      <td>(comfortable, JJ)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>eventhough</td>\n",
       "      <td>(eventhough, eventhough, IN)</td>\n",
       "      <td>17</td>\n",
       "      <td>[]</td>\n",
       "      <td>eventhough</td>\n",
       "      <td>(eventhough, IN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>defition</td>\n",
       "      <td>(defition, defition, NN)</td>\n",
       "      <td>16</td>\n",
       "      <td>[[edition,  2,  110051463], [decision,  2,  71...</td>\n",
       "      <td>edition</td>\n",
       "      <td>(edition, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>becasue</td>\n",
       "      <td>(becasue, becasue, NN)</td>\n",
       "      <td>16</td>\n",
       "      <td>[[because,  1,  271323986]]</td>\n",
       "      <td>because</td>\n",
       "      <td>(because, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>healty</td>\n",
       "      <td>(healty, healty, NN)</td>\n",
       "      <td>15</td>\n",
       "      <td>[[health,  1,  440416431], [healthy,  1,  3036...</td>\n",
       "      <td>health</td>\n",
       "      <td>(health, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>coffe</td>\n",
       "      <td>(coffe, coffe, NN)</td>\n",
       "      <td>15</td>\n",
       "      <td>[[coffee,  1,  42086828], [coffey,  1,  740006...</td>\n",
       "      <td>coffee</td>\n",
       "      <td>(coffee, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>freinds</td>\n",
       "      <td>(freinds, freinds, NNS)</td>\n",
       "      <td>15</td>\n",
       "      <td>[[friends,  1,  110732827]]</td>\n",
       "      <td>friends</td>\n",
       "      <td>(friends, NNS)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>roomate</td>\n",
       "      <td>(roomate, roomate, NN)</td>\n",
       "      <td>14</td>\n",
       "      <td>[[roommate,  1,  6484327]]</td>\n",
       "      <td>roommate</td>\n",
       "      <td>(roommate, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>sentance</td>\n",
       "      <td>(sentance, sentance, NN)</td>\n",
       "      <td>14</td>\n",
       "      <td>[[sentence,  1,  14359872]]</td>\n",
       "      <td>sentence</td>\n",
       "      <td>(sentence, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>frisby</td>\n",
       "      <td>(frisby, frisby, NN)</td>\n",
       "      <td>14</td>\n",
       "      <td>[[frisky,  1,  231840]]</td>\n",
       "      <td>frisky</td>\n",
       "      <td>(frisky, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>poeple</td>\n",
       "      <td>(poeple, poeple, NN)</td>\n",
       "      <td>14</td>\n",
       "      <td>[[people,  1,  480303376], [popple,  1,  34253]]</td>\n",
       "      <td>people</td>\n",
       "      <td>(people, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>etat</td>\n",
       "      <td>(etat, etat, NN)</td>\n",
       "      <td>14</td>\n",
       "      <td>[[eat,  1,  29237400], [stat,  1,  8628104], [...</td>\n",
       "      <td>eat</td>\n",
       "      <td>(eat, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>frind</td>\n",
       "      <td>(frind, frind, NN)</td>\n",
       "      <td>14</td>\n",
       "      <td>[[find,  1,  502043038], [friend,  1,  1545271...</td>\n",
       "      <td>find</td>\n",
       "      <td>(find, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>addtion</td>\n",
       "      <td>(addtion, addtion, NN)</td>\n",
       "      <td>14</td>\n",
       "      <td>[[addition,  1,  73434482]]</td>\n",
       "      <td>addition</td>\n",
       "      <td>(addition, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>monthes</td>\n",
       "      <td>(monthes, monthes, NNS)</td>\n",
       "      <td>13</td>\n",
       "      <td>[[months,  1,  111192257], [montes,  1,  303659]]</td>\n",
       "      <td>months</td>\n",
       "      <td>(months, NNS)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>nowdays</td>\n",
       "      <td>(nowdays, nowdays, NNS)</td>\n",
       "      <td>13</td>\n",
       "      <td>[[nowadays,  1,  2547379]]</td>\n",
       "      <td>nowadays</td>\n",
       "      <td>(nowadays, NNS)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>improtant</td>\n",
       "      <td>(improtant, improtant, JJ)</td>\n",
       "      <td>13</td>\n",
       "      <td>[[important,  1,  136103455]]</td>\n",
       "      <td>important</td>\n",
       "      <td>(important, JJ)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>experince</td>\n",
       "      <td>(experince, experince, NN)</td>\n",
       "      <td>12</td>\n",
       "      <td>[[experience,  1,  137134662]]</td>\n",
       "      <td>experience</td>\n",
       "      <td>(experience, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>languge</td>\n",
       "      <td>(languge, languge, NN)</td>\n",
       "      <td>12</td>\n",
       "      <td>[[language,  1,  138517992], [langue,  1,  512...</td>\n",
       "      <td>language</td>\n",
       "      <td>(language, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>frinds</td>\n",
       "      <td>(frinds, frinds, NNS)</td>\n",
       "      <td>12</td>\n",
       "      <td>[[friends,  1,  110732827], [finds,  1,  16280...</td>\n",
       "      <td>friends</td>\n",
       "      <td>(friends, NNS)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>occured</td>\n",
       "      <td>(occured, occur, VBN)</td>\n",
       "      <td>12</td>\n",
       "      <td>[[occurred,  1,  19073806]]</td>\n",
       "      <td>occurred</td>\n",
       "      <td>(occurred, VBN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>befor</td>\n",
       "      <td>(befor, befor, IN)</td>\n",
       "      <td>12</td>\n",
       "      <td>[[before,  1,  277546019]]</td>\n",
       "      <td>before</td>\n",
       "      <td>(before, IN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>everythings</td>\n",
       "      <td>(everythings, everythings, NNS)</td>\n",
       "      <td>12</td>\n",
       "      <td>[[everything,  1,  91388083]]</td>\n",
       "      <td>everything</td>\n",
       "      <td>(everything, NNS)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>auther</td>\n",
       "      <td>(auther, auther, NN)</td>\n",
       "      <td>12</td>\n",
       "      <td>[[author,  1,  179813446], [luther,  1,  63288...</td>\n",
       "      <td>author</td>\n",
       "      <td>(author, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>suger</td>\n",
       "      <td>(suger, suger, NN)</td>\n",
       "      <td>12</td>\n",
       "      <td>[[super,  1,  66703287], [sugar,  1,  22450333...</td>\n",
       "      <td>super</td>\n",
       "      <td>(super, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>resons</td>\n",
       "      <td>(resons, resons, NNS)</td>\n",
       "      <td>12</td>\n",
       "      <td>[[reasons,  1,  36685859], [resins,  1,  14381...</td>\n",
       "      <td>reasons</td>\n",
       "      <td>(reasons, NNS)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>yut</td>\n",
       "      <td>(yut, yut, NN)</td>\n",
       "      <td>12</td>\n",
       "      <td>[[but,  1,  999899654], [out,  1,  741601852],...</td>\n",
       "      <td>but</td>\n",
       "      <td>(but, NN)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    misspelling                      tok_lem_POS  freq  \\\n",
       "0          alot                 (alot, alot, NN)   127   \n",
       "1       studing          (studing, studing, VBG)    74   \n",
       "2     goverment       (goverment, goverment, NN)    47   \n",
       "3       grammer           (grammer, grammer, NN)    29   \n",
       "4       becuase           (becuase, becuase, NN)    28   \n",
       "5      beatiful         (beatiful, beatiful, JJ)    28   \n",
       "6    differents    (differents, differents, NNS)    23   \n",
       "7     resturant       (resturant, resturant, NN)    23   \n",
       "8       becouse           (becouse, becouse, IN)    23   \n",
       "9         lifes                (lifes, life, NN)    22   \n",
       "10        shoud              (shoud, shoud, VBP)    22   \n",
       "11        sould              (sould, sould, VBP)    22   \n",
       "12        befor               (befor, befor, NN)    21   \n",
       "13     somthing        (somthing, somthing, VBG)    21   \n",
       "17    everytime       (everytime, everytime, NN)    18   \n",
       "19      referes          (referes, referes, VBZ)    18   \n",
       "18       tution             (tution, tution, NN)    18   \n",
       "15   apartament     (apartament, apartament, NN)    18   \n",
       "16      becouse           (becouse, becouse, NN)    18   \n",
       "14          jop                   (jop, jop, NN)    18   \n",
       "20   appartment     (appartment, appartment, NN)    17   \n",
       "21       freind             (freind, freind, NN)    17   \n",
       "22     begining         (begining, begining, NN)    17   \n",
       "23  confortable   (confortable, confortable, JJ)    17   \n",
       "24   eventhough     (eventhough, eventhough, IN)    17   \n",
       "25     defition         (defition, defition, NN)    16   \n",
       "26      becasue           (becasue, becasue, NN)    16   \n",
       "29       healty             (healty, healty, NN)    15   \n",
       "28        coffe               (coffe, coffe, NN)    15   \n",
       "27      freinds          (freinds, freinds, NNS)    15   \n",
       "30      roomate           (roomate, roomate, NN)    14   \n",
       "31     sentance         (sentance, sentance, NN)    14   \n",
       "32       frisby             (frisby, frisby, NN)    14   \n",
       "33       poeple             (poeple, poeple, NN)    14   \n",
       "34         etat                 (etat, etat, NN)    14   \n",
       "35        frind               (frind, frind, NN)    14   \n",
       "36      addtion           (addtion, addtion, NN)    14   \n",
       "37      monthes          (monthes, monthes, NNS)    13   \n",
       "38      nowdays          (nowdays, nowdays, NNS)    13   \n",
       "39    improtant       (improtant, improtant, JJ)    13   \n",
       "48    experince       (experince, experince, NN)    12   \n",
       "49      languge           (languge, languge, NN)    12   \n",
       "42       frinds            (frinds, frinds, NNS)    12   \n",
       "50      occured            (occured, occur, VBN)    12   \n",
       "46        befor               (befor, befor, IN)    12   \n",
       "47  everythings  (everythings, everythings, NNS)    12   \n",
       "41       auther             (auther, auther, NN)    12   \n",
       "45        suger               (suger, suger, NN)    12   \n",
       "44       resons            (resons, resons, NNS)    12   \n",
       "43          yut                   (yut, yut, NN)    12   \n",
       "\n",
       "                                          suggestions   correction  \\\n",
       "0   [[lot,  1,  106405208], [slot,  1,  21602762],...        a lot   \n",
       "1   [[studying,  1,  9763653], [studding,  1,  345...     studying   \n",
       "2                      [[government,  1,  206582673]]   government   \n",
       "3   [[grammar,  1,  8019137], [grammes,  1,  13549...      grammar   \n",
       "4                         [[because,  1,  271323986]]      because   \n",
       "5                        [[beautiful,  1,  58503804]]    beautiful   \n",
       "6                       [[different,  1,  179794224]]    different   \n",
       "7                       [[restaurant,  1,  48255033]]   restaurant   \n",
       "8                         [[because,  1,  271323986]]      because   \n",
       "9   [[life,  1,  306559205], [lines,  1,  76806341...         life   \n",
       "10  [[should,  1,  402028056], [shout,  1,  396586...       should   \n",
       "11  [[would,  1,  572644147], [should,  1,  402028...        would   \n",
       "12                         [[before,  1,  277546019]]       before   \n",
       "13  [[something,  1,  131836210], [soothing,  1,  ...    something   \n",
       "17                         [[overtime,  2,  4095339]]     overtime   \n",
       "19  [[refers,  1,  11869181], [referee,  1,  29419...       refers   \n",
       "18                          [[tuition,  1,  9860033]]      tuition   \n",
       "15                       [[apartment,  1,  30771172]]    apartment   \n",
       "16                        [[because,  1,  271323986]]      because   \n",
       "14  [[top,  1,  484213771], [job,  1,  177706929],...          top   \n",
       "20                       [[apartment,  1,  30771172]]    apartment   \n",
       "21                         [[friend,  1,  154527125]]       friend   \n",
       "22                       [[beginning,  1,  49148844]]    beginning   \n",
       "23  [[comfortable,  1,  19520487], [conformable,  ...  comfortable   \n",
       "24                                                 []   eventhough   \n",
       "25  [[edition,  2,  110051463], [decision,  2,  71...      edition   \n",
       "26                        [[because,  1,  271323986]]      because   \n",
       "29  [[health,  1,  440416431], [healthy,  1,  3036...       health   \n",
       "28  [[coffee,  1,  42086828], [coffey,  1,  740006...       coffee   \n",
       "27                        [[friends,  1,  110732827]]      friends   \n",
       "30                         [[roommate,  1,  6484327]]     roommate   \n",
       "31                        [[sentence,  1,  14359872]]     sentence   \n",
       "32                            [[frisky,  1,  231840]]       frisky   \n",
       "33   [[people,  1,  480303376], [popple,  1,  34253]]       people   \n",
       "34  [[eat,  1,  29237400], [stat,  1,  8628104], [...          eat   \n",
       "35  [[find,  1,  502043038], [friend,  1,  1545271...         find   \n",
       "36                        [[addition,  1,  73434482]]     addition   \n",
       "37  [[months,  1,  111192257], [montes,  1,  303659]]       months   \n",
       "38                         [[nowadays,  1,  2547379]]     nowadays   \n",
       "39                      [[important,  1,  136103455]]    important   \n",
       "48                     [[experience,  1,  137134662]]   experience   \n",
       "49  [[language,  1,  138517992], [langue,  1,  512...     language   \n",
       "42  [[friends,  1,  110732827], [finds,  1,  16280...      friends   \n",
       "50                        [[occurred,  1,  19073806]]     occurred   \n",
       "46                         [[before,  1,  277546019]]       before   \n",
       "47                      [[everything,  1,  91388083]]   everything   \n",
       "41  [[author,  1,  179813446], [luther,  1,  63288...       author   \n",
       "45  [[super,  1,  66703287], [sugar,  1,  22450333...        super   \n",
       "44  [[reasons,  1,  36685859], [resins,  1,  14381...      reasons   \n",
       "43  [[but,  1,  999899654], [out,  1,  741601852],...          but   \n",
       "\n",
       "          correction_POS  \n",
       "0   ((a, DT), (lot, NN))  \n",
       "1        (studying, VBG)  \n",
       "2       (government, NN)  \n",
       "3          (grammar, NN)  \n",
       "4          (because, NN)  \n",
       "5        (beautiful, JJ)  \n",
       "6       (different, NNS)  \n",
       "7       (restaurant, NN)  \n",
       "8          (because, IN)  \n",
       "9             (life, NN)  \n",
       "10         (should, VBP)  \n",
       "11          (would, VBP)  \n",
       "12          (before, NN)  \n",
       "13      (something, VBG)  \n",
       "17        (overtime, NN)  \n",
       "19         (refers, VBZ)  \n",
       "18         (tuition, NN)  \n",
       "15       (apartment, NN)  \n",
       "16         (because, NN)  \n",
       "14             (top, NN)  \n",
       "20       (apartment, NN)  \n",
       "21          (friend, NN)  \n",
       "22       (beginning, NN)  \n",
       "23     (comfortable, JJ)  \n",
       "24      (eventhough, IN)  \n",
       "25         (edition, NN)  \n",
       "26         (because, NN)  \n",
       "29          (health, NN)  \n",
       "28          (coffee, NN)  \n",
       "27        (friends, NNS)  \n",
       "30        (roommate, NN)  \n",
       "31        (sentence, NN)  \n",
       "32          (frisky, NN)  \n",
       "33          (people, NN)  \n",
       "34             (eat, NN)  \n",
       "35            (find, NN)  \n",
       "36        (addition, NN)  \n",
       "37         (months, NNS)  \n",
       "38       (nowadays, NNS)  \n",
       "39       (important, JJ)  \n",
       "48      (experience, NN)  \n",
       "49        (language, NN)  \n",
       "42        (friends, NNS)  \n",
       "50       (occurred, VBN)  \n",
       "46          (before, IN)  \n",
       "47     (everything, NNS)  \n",
       "41          (author, NN)  \n",
       "45           (super, NN)  \n",
       "44        (reasons, NNS)  \n",
       "43             (but, NN)  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misspell_df.sort_values(by=['freq'], ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incorporating corrections into `pelic_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11090\n",
      "10245\n"
     ]
    }
   ],
   "source": [
    "# First removing items from misspell_df where no correction will take place\n",
    "\n",
    "print(len(misspell_df))\n",
    "misspell_df = misspell_df.loc[misspell_df.misspelling != misspell_df.correction]\n",
    "print(len(misspell_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>misspelling</th>\n",
       "      <th>tok_lem_POS</th>\n",
       "      <th>freq</th>\n",
       "      <th>suggestions</th>\n",
       "      <th>correction</th>\n",
       "      <th>correction_POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alot</td>\n",
       "      <td>(alot, alot, NN)</td>\n",
       "      <td>127</td>\n",
       "      <td>[[lot,  1,  106405208], [slot,  1,  21602762],...</td>\n",
       "      <td>a lot</td>\n",
       "      <td>((a, DT), (lot, NN))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>studing</td>\n",
       "      <td>(studing, studing, VBG)</td>\n",
       "      <td>74</td>\n",
       "      <td>[[studying,  1,  9763653], [studding,  1,  345...</td>\n",
       "      <td>studying</td>\n",
       "      <td>(studying, VBG)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>goverment</td>\n",
       "      <td>(goverment, goverment, NN)</td>\n",
       "      <td>47</td>\n",
       "      <td>[[government,  1,  206582673]]</td>\n",
       "      <td>government</td>\n",
       "      <td>(government, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>grammer</td>\n",
       "      <td>(grammer, grammer, NN)</td>\n",
       "      <td>29</td>\n",
       "      <td>[[grammar,  1,  8019137], [grammes,  1,  13549...</td>\n",
       "      <td>grammar</td>\n",
       "      <td>(grammar, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>becuase</td>\n",
       "      <td>(becuase, becuase, NN)</td>\n",
       "      <td>28</td>\n",
       "      <td>[[because,  1,  271323986]]</td>\n",
       "      <td>because</td>\n",
       "      <td>(because, NN)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  misspelling                 tok_lem_POS  freq  \\\n",
       "0        alot            (alot, alot, NN)   127   \n",
       "1     studing     (studing, studing, VBG)    74   \n",
       "2   goverment  (goverment, goverment, NN)    47   \n",
       "3     grammer      (grammer, grammer, NN)    29   \n",
       "4     becuase      (becuase, becuase, NN)    28   \n",
       "\n",
       "                                         suggestions  correction  \\\n",
       "0  [[lot,  1,  106405208], [slot,  1,  21602762],...       a lot   \n",
       "1  [[studying,  1,  9763653], [studding,  1,  345...    studying   \n",
       "2                     [[government,  1,  206582673]]  government   \n",
       "3  [[grammar,  1,  8019137], [grammes,  1,  13549...     grammar   \n",
       "4                        [[because,  1,  271323986]]     because   \n",
       "\n",
       "         correction_POS  \n",
       "0  ((a, DT), (lot, NN))  \n",
       "1       (studying, VBG)  \n",
       "2      (government, NN)  \n",
       "3         (grammar, NN)  \n",
       "4         (because, NN)  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misspell_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dictionary for mappying - key = incorrect spelling, value = correct spelling\n",
    "\n",
    "misspell_dict = pd.Series(misspell_df.correction_POS.values,misspell_df.tok_lem_POS).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('alot', 'alot', 'NN'): (('a', 'DT'), ('lot', 'NN')),\n",
       " ('studing', 'studing', 'VBG'): ('studying', 'VBG'),\n",
       " ('goverment', 'goverment', 'NN'): ('government', 'NN'),\n",
       " ('grammer', 'grammer', 'NN'): ('grammar', 'NN'),\n",
       " ('becuase', 'becuase', 'NN'): ('because', 'NN'),\n",
       " ('beatiful', 'beatiful', 'JJ'): ('beautiful', 'JJ'),\n",
       " ('differents', 'differents', 'NNS'): ('different', 'NNS'),\n",
       " ('resturant', 'resturant', 'NN'): ('restaurant', 'NN'),\n",
       " ('becouse', 'becouse', 'IN'): ('because', 'IN'),\n",
       " ('lifes', 'life', 'NN'): ('life', 'NN'),\n",
       " ('shoud', 'shoud', 'VBP'): ('should', 'VBP'),\n",
       " ('sould', 'sould', 'VBP'): ('would', 'VBP'),\n",
       " ('befor', 'befor', 'NN'): ('before', 'NN'),\n",
       " ('somthing', 'somthing', 'VBG'): ('something', 'VBG'),\n",
       " ('jop', 'jop', 'NN'): ('top', 'NN'),\n",
       " ('apartament', 'apartament', 'NN'): ('apartment', 'NN'),\n",
       " ('becouse', 'becouse', 'NN'): ('because', 'NN'),\n",
       " ('everytime', 'everytime', 'NN'): ('overtime', 'NN'),\n",
       " ('tution', 'tution', 'NN'): ('tuition', 'NN'),\n",
       " ('referes', 'referes', 'VBZ'): ('refers', 'VBZ'),\n",
       " ('appartment', 'appartment', 'NN'): ('apartment', 'NN'),\n",
       " ('freind', 'freind', 'NN'): ('friend', 'NN'),\n",
       " ('begining', 'begining', 'NN'): ('beginning', 'NN'),\n",
       " ('confortable', 'confortable', 'JJ'): ('comfortable', 'JJ'),\n",
       " ('defition', 'defition', 'NN'): ('edition', 'NN'),\n",
       " ('becasue', 'becasue', 'NN'): ('because', 'NN'),\n",
       " ('freinds', 'freinds', 'NNS'): ('friends', 'NNS'),\n",
       " ('coffe', 'coffe', 'NN'): ('coffee', 'NN'),\n",
       " ('healty', 'healty', 'NN'): ('health', 'NN'),\n",
       " ('roomate', 'roomate', 'NN'): ('roommate', 'NN'),\n",
       " ('sentance', 'sentance', 'NN'): ('sentence', 'NN'),\n",
       " ('frisby', 'frisby', 'NN'): ('frisky', 'NN'),\n",
       " ('poeple', 'poeple', 'NN'): ('people', 'NN'),\n",
       " ('etat', 'etat', 'NN'): ('eat', 'NN'),\n",
       " ('frind', 'frind', 'NN'): ('find', 'NN'),\n",
       " ('addtion', 'addtion', 'NN'): ('addition', 'NN'),\n",
       " ('monthes', 'monthes', 'NNS'): ('months', 'NNS'),\n",
       " ('nowdays', 'nowdays', 'NNS'): ('nowadays', 'NNS'),\n",
       " ('improtant', 'improtant', 'JJ'): ('important', 'JJ'),\n",
       " ('beacuse', 'beacuse', 'IN'): ('because', 'IN'),\n",
       " ('auther', 'auther', 'NN'): ('author', 'NN'),\n",
       " ('frinds', 'frinds', 'NNS'): ('friends', 'NNS'),\n",
       " ('yut', 'yut', 'NN'): ('but', 'NN'),\n",
       " ('resons', 'resons', 'NNS'): ('reasons', 'NNS'),\n",
       " ('suger', 'suger', 'NN'): ('super', 'NN'),\n",
       " ('befor', 'befor', 'IN'): ('before', 'IN'),\n",
       " ('everythings', 'everythings', 'NNS'): ('everything', 'NNS'),\n",
       " ('experince', 'experince', 'NN'): ('experience', 'NN'),\n",
       " ('languge', 'languge', 'NN'): ('language', 'NN'),\n",
       " ('occured', 'occur', 'VBN'): ('occurred', 'VBN'),\n",
       " ('citys', 'citys', 'NN'): ('city', 'NN'),\n",
       " ('favorit', 'favorit', 'NN'): ('favourite', 'NN'),\n",
       " ('alfter', 'alfter', 'NN'): ('after', 'NN'),\n",
       " ('marrige', 'marrige', 'NN'): ('marriage', 'NN'),\n",
       " ('differnt', 'differnt', 'JJ'): ('different', 'JJ'),\n",
       " ('eatting', 'eatting', 'VBG'): ('eating', 'VBG'),\n",
       " ('familly', 'familly', 'RB'): ('family', 'RB'),\n",
       " ('wonderfull', 'wonderfull', 'JJ'): ('wonderful', 'JJ'),\n",
       " ('unversity', 'unversity', 'NN'): ('university', 'NN'),\n",
       " ('scool', 'scool', 'NN'): ('school', 'NN'),\n",
       " ('belived', 'belived', 'VBN'): ('believed', 'VBN'),\n",
       " ('postive', 'postive', 'JJ'): ('positive', 'JJ'),\n",
       " ('shose', 'shose', 'NN'): ('those', 'NN'),\n",
       " ('peaple', 'peaple', 'NN'): ('people', 'NN'),\n",
       " ('resturants', 'resturants', 'NNS'): ('restaurants', 'NNS'),\n",
       " ('impotant', 'impotant', 'JJ'): ('important', 'JJ'),\n",
       " ('favorit', 'favorit', 'JJ'): ('favourite', 'JJ'),\n",
       " ('sociaty', 'sociaty', 'NN'): ('society', 'NN'),\n",
       " ('childern', 'childern', 'NN'): ('children', 'NN'),\n",
       " ('diffrent', 'diffrent', 'JJ'): ('different', 'JJ'),\n",
       " ('chiken', 'chiken', 'NN'): ('chicken', 'NN'),\n",
       " ('contry', 'contry', 'NN'): ('country', 'NN'),\n",
       " ('childrens', 'child', 'NNS'): ('children', 'NNS'),\n",
       " ('apartement', 'apartement', 'NN'): ('apartment', 'NN'),\n",
       " ('fastfood', 'fastfood', 'NN'): ('eastwood', 'NN'),\n",
       " ('desicion', 'desicion', 'NN'): ('decision', 'NN'),\n",
       " ('becase', 'becase', 'NN'): ('because', 'NN'),\n",
       " ('realy', 'realy', 'VBP'): ('real', 'VBP'),\n",
       " ('goverments', 'goverments', 'NNS'): ('governments', 'NNS'),\n",
       " ('happend', 'happend', 'VBP'): ('happen', 'VBP'),\n",
       " ('nepotizm', 'nepotizm', 'NN'): ('nepotism', 'NN'),\n",
       " ('realy', 'realy', 'JJ'): ('real', 'JJ'),\n",
       " ('mounth', 'mounth', 'NN'): ('month', 'NN'),\n",
       " ('polution', 'polution', 'NN'): ('solution', 'NN'),\n",
       " ('youself', 'youself', 'PRP'): ('yourself', 'PRP'),\n",
       " ('htm', 'htm', 'NN'): ('him', 'NN'),\n",
       " ('favorate', 'favorate', 'NN'): ('favourite', 'NN'),\n",
       " ('pronounciation', 'pronounciation', 'NN'): ('pronunciation', 'NN'),\n",
       " ('becuse', 'becuse', 'IN'): ('because', 'IN'),\n",
       " ('evry', 'evry', 'JJ'): ('very', 'JJ'),\n",
       " ('ther', 'ther', 'NN'): ('the', 'NN'),\n",
       " ('advantege', 'advantege', 'NN'): ('advantage', 'NN'),\n",
       " ('usualy', 'usualy', 'JJ'): ('usually', 'JJ'),\n",
       " ('becuse', 'becuse', 'NN'): ('because', 'NN'),\n",
       " ('recieve', 'recieve', 'VB'): ('receive', 'VB'),\n",
       " ('occured', 'occur', 'VBD'): ('occurred', 'VBD'),\n",
       " ('contries', 'contries', 'NNS'): ('countries', 'NNS'),\n",
       " ('choosed', 'choosed', 'VBD'): ('choose', 'VBD'),\n",
       " ('aupair', 'aupair', 'NN'): ('repair', 'NN'),\n",
       " ('neccessary', 'neccessary', 'JJ'): ('necessary', 'JJ'),\n",
       " ('heared', 'heared', 'VBN'): ('heard', 'VBN'),\n",
       " ('thes', 'thes', 'NNS'): ('the', 'NNS'),\n",
       " ('iam', 'iam', 'NN'): ('am', 'NN'),\n",
       " ('advertisment', 'advertisment', 'NN'): ('advertisement', 'NN'),\n",
       " ('beleive', 'beleive', 'VBP'): ('believe', 'VBP'),\n",
       " ('benifits', 'benifits', 'NNS'): ('benefits', 'NNS'),\n",
       " ('unforgetable', 'unforgetable', 'JJ'): ('unforgettable', 'JJ'),\n",
       " ('gi', 'gi', 'NN'): ('i', 'NN'),\n",
       " ('easly', 'easly', 'RB'): ('easy', 'RB'),\n",
       " ('intersting', 'intersting', 'VBG'): ('interesting', 'VBG'),\n",
       " ('airplan', 'airplan', 'NN'): ('airplay', 'NN'),\n",
       " ('experiance', 'experiance', 'NN'): ('experience', 'NN'),\n",
       " ('conclution', 'conclution', 'NN'): ('conclusion', 'NN'),\n",
       " ('begining', 'begining', 'VBG'): ('beginning', 'VBG'),\n",
       " ('espacially', 'espacially', 'RB'): ('especially', 'RB'),\n",
       " ('ludwing', 'ludwing', 'VBG'): ('ludwig', 'VBG'),\n",
       " ('friens', 'friens', 'NNS'): ('friend', 'NNS'),\n",
       " ('acording', 'acording', 'VBG'): ('according', 'VBG'),\n",
       " ('activites', 'activites', 'NNS'): ('activities', 'NNS'),\n",
       " ('usally', 'usally', 'RB'): ('usually', 'RB'),\n",
       " ('heared', 'heared', 'VBD'): ('heard', 'VBD'),\n",
       " ('alot', 'alot', 'VBN'): ('lot', 'VBN'),\n",
       " ('helth', 'helth', 'NN'): ('health', 'NN'),\n",
       " ('preson', 'preson', 'NN'): ('person', 'NN'),\n",
       " ('centr', 'centr', 'NN'): ('centre', 'NN'),\n",
       " ('benefites', 'benefites', 'NNS'): ('benefits', 'NNS'),\n",
       " ('morden', 'morden', 'JJ'): ('borden', 'JJ'),\n",
       " ('countery', 'countery', 'NN'): ('country', 'NN'),\n",
       " ('ddui', 'ddui', 'NN'): ('due', 'NN'),\n",
       " ('baket', 'baket', 'NN'): ('basket', 'NN'),\n",
       " ('convserve', 'convserve', 'VB'): ('conserve', 'VB'),\n",
       " ('securaty', 'securaty', 'NN'): ('security', 'NN'),\n",
       " ('thr', 'thr', 'JJ'): ('the', 'JJ'),\n",
       " ('aprtment', 'aprtment', 'NN'): ('apartment', 'NN'),\n",
       " ('caral', 'caral', 'JJ'): ('carl', 'JJ'),\n",
       " ('bacause', 'bacause', 'NN'): ('because', 'NN'),\n",
       " ('reson', 'reson', 'NN'): ('reason', 'NN'),\n",
       " ('enlish', 'enlish', 'JJ'): ('english', 'JJ'),\n",
       " ('agressive', 'agressive', 'JJ'): ('aggressive', 'JJ'),\n",
       " ('writting', 'writting', 'VBG'): ('writing', 'VBG'),\n",
       " ('intersted', 'intersted', 'VBN'): ('interested', 'VBN'),\n",
       " ('thoub', 'thoub', 'NN'): ('thou', 'NN'),\n",
       " ('acadimic', 'acadimic', 'JJ'): ('academic', 'JJ'),\n",
       " ('everthing', 'everthing', 'VBG'): ('everything', 'VBG'),\n",
       " ('milion', 'milion', 'NN'): ('million', 'NN'),\n",
       " ('amircan', 'amircan', 'JJ'): ('american', 'JJ'),\n",
       " ('pepole', 'pepole', 'NN'): ('people', 'NN'),\n",
       " ('goverment', 'goverment', 'JJ'): ('government', 'JJ'),\n",
       " ('habilis', 'habilis', 'NN'): ('habits', 'NN'),\n",
       " ('bulding', 'bulding', 'NN'): ('building', 'NN'),\n",
       " ('ture', 'ture', 'NN'): ('sure', 'NN'),\n",
       " ('togather', 'togather', 'NN'): ('together', 'NN'),\n",
       " ('openion', 'openion', 'NN'): ('opinion', 'NN'),\n",
       " ('similiar', 'similiar', 'JJ'): ('similar', 'JJ'),\n",
       " ('ax', 'ax', 'JJ'): ('axe', 'JJ'),\n",
       " ('diffirent', 'diffirent', 'JJ'): ('different', 'JJ'),\n",
       " ('especialy', 'especialy', 'NN'): ('especially', 'NN'),\n",
       " ('tring', 'tring', 'VBG'): ('thing', 'VBG'),\n",
       " ('evrything', 'evrything', 'VBG'): ('everything', 'VBG'),\n",
       " ('familys', 'family', 'NN'): ('family', 'NN'),\n",
       " ('falled', 'falled', 'VBD'): ('called', 'VBD'),\n",
       " ('womens', 'womens', 'NNS'): ('women', 'NNS'),\n",
       " ('longman', 'longman', 'JJ'): ('longan', 'JJ'),\n",
       " ('diferent', 'diferent', 'JJ'): ('different', 'JJ'),\n",
       " ('diffrent', 'diffrent', 'NN'): ('different', 'NN'),\n",
       " ('millitary', 'millitary', 'NN'): ('military', 'NN'),\n",
       " ('importent', 'importent', 'JJ'): ('important', 'JJ'),\n",
       " ('rac', 'rac', 'NN'): ('mac', 'NN'),\n",
       " ('differnt', 'differnt', 'NN'): ('different', 'NN'),\n",
       " ('imformation', 'imformation', 'NN'): ('information', 'NN'),\n",
       " ('beutifull', 'beutifull', 'JJ'): ('beautiful', 'JJ'),\n",
       " ('knowlege', 'knowlege', 'NN'): ('knowledge', 'NN'),\n",
       " ('customes', 'customes', 'NNS'): ('customer', 'NNS'),\n",
       " ('studyed', 'studyed', 'VBN'): ('studied', 'VBN'),\n",
       " ('finshed', 'finshed', 'VBD'): ('finished', 'VBD'),\n",
       " ('interst', 'interst', 'NN'): ('interest', 'NN'),\n",
       " ('worring', 'worring', 'VBG'): ('working', 'VBG'),\n",
       " ('firends', 'firends', 'NNS'): ('friends', 'NNS'),\n",
       " ('recomand', 'recomand', 'VBP'): ('recommend', 'VBP'),\n",
       " ('ordor', 'ordor', 'SYM'): ('order', 'SYM'),\n",
       " ('humen', 'humen', 'NNS'): ('human', 'NNS'),\n",
       " ('yueh', 'yueh', 'SYM'): ('such', 'SYM'),\n",
       " ('performence', 'performence', 'NN'): ('performance', 'NN'),\n",
       " ('sucessful', 'sucessful', 'JJ'): ('successful', 'JJ'),\n",
       " ('compus', 'compus', 'NN'): ('campus', 'NN'),\n",
       " ('completly', 'completly', 'RB'): ('completely', 'RB'),\n",
       " ('whome', 'whome', 'NN'): ('home', 'NN'),\n",
       " ('recommond', 'recommond', 'VBP'): ('recommend', 'VBP'),\n",
       " ('deprission', 'deprission', 'NN'): ('depression', 'NN'),\n",
       " ('synethesia', 'synethesia', 'NN'): ('synthesis', 'NN'),\n",
       " ('programe', 'programe', 'NN'): ('program', 'NN'),\n",
       " ('infront', 'infront', 'NN'): ('front', 'NN'),\n",
       " ('charactors', 'charactors', 'NNS'): ('characters', 'NNS'),\n",
       " ('homwork', 'homwork', 'NN'): ('homework', 'NN'),\n",
       " ('seng', 'seng', 'NN'): ('send', 'NN'),\n",
       " ('gliese', 'gliese', 'JJ'): ('lies', 'JJ'),\n",
       " ('usualy', 'usualy', 'VBP'): ('usually', 'VBP'),\n",
       " ('popluar', 'popluar', 'JJ'): ('popular', 'JJ'),\n",
       " ('culteres', 'culteres', 'NNS'): ('cultures', 'NNS'),\n",
       " ('chuan', 'chuan', 'NN'): ('chan', 'NN'),\n",
       " ('helpfull', 'helpfull', 'JJ'): ('helpful', 'JJ'),\n",
       " ('diaphragmic', 'diaphragmic', 'JJ'): ('diaphragm', 'JJ'),\n",
       " ('stadying', 'stadying', 'VBG'): ('studying', 'VBG'),\n",
       " ('exessive', 'exessive', 'JJ'): ('excessive', 'JJ'),\n",
       " ('coutry', 'coutry', 'NN'): ('country', 'NN'),\n",
       " ('heum', 'heum', 'NN'): ('hum', 'NN'),\n",
       " ('milions', 'milions', 'NNS'): ('millions', 'NNS'),\n",
       " ('brillient', 'brillient', 'NN'): ('brilliant', 'NN'),\n",
       " ('dgree', 'dgree', 'NN'): ('degree', 'NN'),\n",
       " ('ipods', 'ipod', 'NNS'): ('ipod', 'NNS'),\n",
       " ('bridege', 'bridege', 'NN'): ('bridge', 'NN'),\n",
       " ('sevral', 'sevral', 'JJ'): ('several', 'JJ'),\n",
       " ('brillient', 'brillient', 'VBN'): ('brilliant', 'VBN'),\n",
       " ('lb', 'lb', 'NN'): ('la', 'NN'),\n",
       " ('devorce', 'devorce', 'NN'): ('divorce', 'NN'),\n",
       " ('bacause', 'bacause', 'IN'): ('because', 'IN'),\n",
       " ('sause', 'sause', 'NN'): ('cause', 'NN'),\n",
       " ('rainning', 'rainning', 'VBG'): ('raining', 'VBG'),\n",
       " ('dicision', 'dicision', 'NN'): ('decision', 'NN'),\n",
       " ('desease', 'desease', 'NN'): ('disease', 'NN'),\n",
       " ('unfortunatly', 'unfortunatly', 'RB'): ('unfortunately', 'RB'),\n",
       " ('bady', 'bady', 'NN'): ('body', 'NN'),\n",
       " ('badroom', 'badroom', 'NN'): ('bedroom', 'NN'),\n",
       " ('preperation', 'preperation', 'NN'): ('preparation', 'NN'),\n",
       " ('didnot', 'didnot', 'VBP'): ('idiot', 'VBP'),\n",
       " ('alfeter', 'alfeter', 'NN'): ('after', 'NN'),\n",
       " ('sentece', 'sentece', 'NN'): ('sentence', 'NN'),\n",
       " ('wav', 'wav', 'NN'): ('was', 'NN'),\n",
       " ('msn', 'msn', 'NN'): ('man', 'NN'),\n",
       " ('safa', 'safa', 'NN'): ('safe', 'NN'),\n",
       " ('rosling', 'rosling', 'VBG'): ('rolling', 'VBG'),\n",
       " ('enought', 'enought', 'JJ'): ('enough', 'JJ'),\n",
       " ('vist', 'vist', 'VB'): ('list', 'VB'),\n",
       " ('doughter', 'doughter', 'NN'): ('daughter', 'NN'),\n",
       " ('whos', 'whos', 'NN'): ('who', 'NN'),\n",
       " ('enjoied', 'enjoied', 'VBD'): ('enjoyed', 'VBD'),\n",
       " ('drived', 'drived', 'VBD'): ('drive', 'VBD'),\n",
       " ('puplic', 'puplic', 'JJ'): ('public', 'JJ'),\n",
       " ('eatten', 'eatten', 'VBN'): ('eaten', 'VBN'),\n",
       " ('drowing', 'drowing', 'VBG'): ('growing', 'VBG'),\n",
       " ('dvds', 'dvds', 'NN'): ('dds', 'NN'),\n",
       " ('forign', 'forign', 'JJ'): ('foreign', 'JJ'),\n",
       " ('forigners', 'forigners', 'NNS'): ('foreigners', 'NNS'),\n",
       " ('vegtables', 'vegtables', 'NNS'): ('vegetables', 'NNS'),\n",
       " ('vedio', 'vedio', 'NN'): ('vedic', 'NN'),\n",
       " ('uniqe', 'uniqe', 'JJ'): ('unique', 'JJ'),\n",
       " ('geting', 'geting', 'VBG'): ('getting', 'VBG'),\n",
       " ('analytes', 'analyte', 'NNS'): ('analyses', 'NNS'),\n",
       " ('avoide', 'avoide', 'VB'): ('avoid', 'VB'),\n",
       " ('kyung', 'kyung', 'NN'): ('young', 'NN'),\n",
       " ('governement', 'governement', 'NN'): ('government', 'NN'),\n",
       " ('saleries', 'saleries', 'NNS'): ('salaries', 'NNS'),\n",
       " ('wnat', 'wnat', 'VBP'): ('what', 'VBP'),\n",
       " ('unkown', 'unkown', 'JJ'): ('unknown', 'JJ'),\n",
       " ('diffierent', 'diffierent', 'JJ'): ('different', 'JJ'),\n",
       " ('unsucced', 'unsucced', 'VBD'): ('unsuited', 'VBD'),\n",
       " ('esl', 'esl', 'JJ'): ('est', 'JJ'),\n",
       " ('dificult', 'dificult', 'NN'): ('difficult', 'NN'),\n",
       " ('scholl', 'scholl', 'NN'): ('school', 'NN'),\n",
       " ('nigative', 'nigative', 'JJ'): ('negative', 'JJ'),\n",
       " ('larg', 'larg', 'JJ'): ('large', 'JJ'),\n",
       " ('usefull', 'usefull', 'JJ'): ('useful', 'JJ'),\n",
       " ('sheeps', 'sheep', 'NNS'): ('sheets', 'NNS'),\n",
       " ('rissotto', 'rissotto', 'NN'): ('risotto', 'NN'),\n",
       " ('discribe', 'discribe', 'VB'): ('describe', 'VB'),\n",
       " ('diffcult', 'diffcult', 'NN'): ('difficult', 'NN'),\n",
       " ('postion', 'postion', 'NN'): ('position', 'NN'),\n",
       " ('theather', 'theather', 'NN'): ('heather', 'NN'),\n",
       " ('cuold', 'cuold', 'NN'): ('could', 'NN'),\n",
       " ('beutiful', 'beutiful', 'JJ'): ('beautiful', 'JJ'),\n",
       " ('pleace', 'pleace', 'NN'): ('please', 'NN'),\n",
       " ('benifit', 'benifit', 'NN'): ('benefit', 'NN'),\n",
       " ('changings', 'changings', 'NNS'): ('changing', 'NNS'),\n",
       " ('jik', 'jik', 'NN'): ('jim', 'NN'),\n",
       " ('troble', 'troble', 'JJ'): ('trouble', 'JJ'),\n",
       " ('trodition', 'trodition', 'NN'): ('tradition', 'NN'),\n",
       " ('beleive', 'beleive', 'VB'): ('believe', 'VB'),\n",
       " ('reserch', 'reserch', 'NN'): ('research', 'NN'),\n",
       " ('regulary', 'regulary', 'JJ'): ('regular', 'JJ'),\n",
       " ('decieded', 'decieded', 'VBD'): ('decided', 'VBD'),\n",
       " ('poeple', 'poeple', 'NNS'): ('people', 'NNS'),\n",
       " ('beginnig', 'beginnig', 'NN'): ('beginning', 'NN'),\n",
       " ('transpotation', 'transpotation', 'NN'): ('transportation', 'NN'),\n",
       " ('thoes', 'thoes', 'JJ'): ('those', 'JJ'),\n",
       " ('transportaion', 'transportaion', 'NN'): ('transportation', 'NN'),\n",
       " ('persone', 'persone', 'NN'): ('person', 'NN'),\n",
       " ('feild', 'feild', 'NN'): ('field', 'NN'),\n",
       " ('contruction', 'contruction', 'NN'): ('construction', 'NN'),\n",
       " ('expecially', 'expecially', 'RB'): ('especially', 'RB'),\n",
       " ('recieved', 'recieved', 'VBD'): ('received', 'VBD'),\n",
       " ('intuitives', 'intuitives', 'NNS'): ('intuitive', 'NNS'),\n",
       " ('achive', 'achive', 'VB'): ('archive', 'VB'),\n",
       " ('remeber', 'remeber', 'VB'): ('remember', 'VB'),\n",
       " ('transfomation', 'transfomation', 'NN'): ('transformation', 'NN'),\n",
       " ('sterotypes', 'sterotypes', 'NNS'): ('stereotypes', 'NNS'),\n",
       " ('studiying', 'studiying', 'VBG'): ('studying', 'VBG'),\n",
       " ('tradional', 'tradional', 'JJ'): ('traditional', 'JJ'),\n",
       " ('coutries', 'coutries', 'NNS'): ('countries', 'NNS'),\n",
       " ('collegues', 'collegues', 'NNS'): ('colleges', 'NNS'),\n",
       " ('belived', 'belived', 'VBD'): ('believed', 'VBD'),\n",
       " ('feets', 'feets', 'NNS'): ('feet', 'NNS'),\n",
       " ('cm', 'cm', 'NN'): ('pm', 'NN'),\n",
       " ('chilhood', 'chilhood', 'NN'): ('childhood', 'NN'),\n",
       " ('poors', 'poors', 'NNS'): ('poor', 'NNS'),\n",
       " ('temparature', 'temparature', 'NN'): ('temperature', 'NN'),\n",
       " ('ubicate', 'ubicate', 'JJ'): ('urinate', 'JJ'),\n",
       " ('responsabilities', 'responsabilities', 'NNS'): ('responsibilities', 'NNS'),\n",
       " ('becuase', 'becuase', 'VB'): ('because', 'VB'),\n",
       " ('childern', 'childern', 'JJ'): ('children', 'JJ'),\n",
       " ('chiken', 'chiken', 'JJ'): ('chicken', 'JJ'),\n",
       " ('responsability', 'responsability', 'NN'): ('responsibility', 'NN'),\n",
       " ('beacuse', 'beacuse', 'NN'): ('because', 'NN'),\n",
       " ('paragragh', 'paragragh', 'NN'): ('paragraph', 'NN'),\n",
       " ('uesd', 'uesd', 'VBP'): ('used', 'VBP'),\n",
       " ('inthe', 'inthe', 'JJ'): ('the', 'JJ'),\n",
       " ('oppotunity', 'oppotunity', 'NN'): ('opportunity', 'NN'),\n",
       " ('thoes', 'thoes', 'NNS'): ('those', 'NNS'),\n",
       " ('techology', 'techology', 'NN'): ('technology', 'NN'),\n",
       " ('frienship', 'frienship', 'NN'): ('friendship', 'NN'),\n",
       " ('proffesional', 'proffesional', 'JJ'): ('professional', 'JJ'),\n",
       " ('comunication', 'comunication', 'NN'): ('communication', 'NN'),\n",
       " ('campony', 'campony', 'NN'): ('company', 'NN'),\n",
       " ('senteces', 'senteces', 'NNS'): ('sentences', 'NNS'),\n",
       " ('lke', 'lke', 'VBP'): ('like', 'VBP'),\n",
       " ('calss', 'calss', 'NN'): ('class', 'NN'),\n",
       " ('littel', 'littel', 'NN'): ('little', 'NN'),\n",
       " ('hourse', 'hourse', 'NN'): ('house', 'NN'),\n",
       " ('caffiene', 'caffiene', 'NN'): ('caffeine', 'NN'),\n",
       " ('continously', 'continously', 'RB'): ('continuously', 'RB'),\n",
       " ('tobacoo', 'tobacoo', 'NN'): ('tobacco', 'NN'),\n",
       " ('elementry', 'elementry', 'NN'): ('elementary', 'NN'),\n",
       " ('intersting', 'intersting', 'JJ'): ('interesting', 'JJ'),\n",
       " ('choosen', 'choosen', 'VBN'): ('choose', 'VBN'),\n",
       " ('archaelogists', 'archaelogists', 'NNS'): ('archaeologists', 'NNS'),\n",
       " ('donot', 'donot', 'NN'): ('donor', 'NN'),\n",
       " ('voiling', 'voiling', 'VBG'): ('boiling', 'VBG'),\n",
       " ('eldery', 'eldery', 'NN'): ('elderly', 'NN'),\n",
       " ('studints', 'studints', 'NNS'): ('students', 'NNS'),\n",
       " ('improvment', 'improvment', 'NN'): ('improvement', 'NN'),\n",
       " ('aquire', 'aquire', 'VB'): ('acquire', 'VB'),\n",
       " ('tenis', 'tenis', 'NN'): ('tennis', 'NN'),\n",
       " ('mypyramid', 'mypyramid', 'NN'): ('pyramid', 'NN'),\n",
       " ('indivisuals', 'indivisuals', 'NNS'): ('individuals', 'NNS'),\n",
       " ('aouther', 'aouther', 'NN'): ('souther', 'NN'),\n",
       " ('weaknes', 'weaknes', 'NNS'): ('weakness', 'NNS'),\n",
       " ('scound', 'scound', 'NN'): ('sound', 'NN'),\n",
       " ('chidren', 'chidren', 'NN'): ('children', 'NN'),\n",
       " ('fter', 'fter', 'NN'): ('after', 'NN'),\n",
       " ('comercials', 'comercials', 'NNS'): ('commercials', 'NNS'),\n",
       " ('proplems', 'proplems', 'NNS'): ('problems', 'NNS'),\n",
       " ('swiming', 'swiming', 'VBG'): ('swimming', 'VBG'),\n",
       " ('anoise', 'anoise', 'JJ'): ('noise', 'JJ'),\n",
       " ('inforamtion', 'inforamtion', 'NN'): ('information', 'NN'),\n",
       " ('cohee', 'cohee', 'NN'): ('cohen', 'NN'),\n",
       " ('immuno', 'immuno', 'NN'): ('immune', 'NN'),\n",
       " ('problame', 'problame', 'NN'): ('problem', 'NN'),\n",
       " ('ther', 'ther', 'CC'): ('the', 'CC'),\n",
       " ('competion', 'competion', 'NN'): ('completion', 'NN'),\n",
       " ('taht', 'taht', 'NN'): ('that', 'NN'),\n",
       " ('tfgp', 'tfgp', 'IN'): ('top', 'IN'),\n",
       " ('companys', 'company', 'NN'): ('company', 'NN'),\n",
       " ('secound', 'secound', 'NN'): ('second', 'NN'),\n",
       " ('supose', 'supose', 'VBP'): ('suppose', 'VBP'),\n",
       " ('droped', 'droped', 'VBD'): ('dropped', 'VBD'),\n",
       " ('thesedays', 'thesedays', 'NNS'): ('tuesdays', 'NNS'),\n",
       " ('ther', 'ther', 'JJR'): ('the', 'JJR'),\n",
       " ('peole', 'peole', 'NN'): ('people', 'NN'),\n",
       " ('identfy', 'identfy', 'VB'): ('identify', 'VB'),\n",
       " ('taht', 'taht', 'IN'): ('that', 'IN'),\n",
       " ('comming', 'comming', 'VBG'): ('coming', 'VBG'),\n",
       " ('ch', 'ch', 'NN'): ('cd', 'NN'),\n",
       " ('schedual', 'schedual', 'JJ'): ('schedule', 'JJ'),\n",
       " ('vaction', 'vaction', 'NN'): ('action', 'NN'),\n",
       " ('dokalah', 'dokalah', 'NN'): ('douala', 'NN'),\n",
       " ('prblems', 'prblems', 'NNS'): ('problems', 'NNS'),\n",
       " ('somthing', 'somthing', 'NN'): ('something', 'NN'),\n",
       " ('defferent', 'defferent', 'JJ'): ('different', 'JJ'),\n",
       " ('judu', 'judu', 'NN'): ('judy', 'NN'),\n",
       " ('befor', 'befor', 'JJ'): ('before', 'JJ'),\n",
       " ('befor', 'befor', 'VB'): ('before', 'VB'),\n",
       " ('diferents', 'diferents', 'NNS'): ('different', 'NNS'),\n",
       " ('universites', 'universites', 'NNS'): ('universities', 'NNS'),\n",
       " ('universitey', 'universitey', 'NN'): ('university', 'NN'),\n",
       " ('befor', 'befor', 'VBP'): ('before', 'VBP'),\n",
       " ('aviod', 'aviod', 'VB'): ('avoid', 'VB'),\n",
       " ('hapiness', 'hapiness', 'NN'): ('happiness', 'NN'),\n",
       " ('beginging', 'beginging', 'NN'): ('beginning', 'NN'),\n",
       " ('happend', 'happend', 'VBN'): ('happen', 'VBN'),\n",
       " ('tryed', 'tryed', 'VBD'): ('tried', 'VBD'),\n",
       " ('precussionist', 'precussionist', 'NN'): ('percussionist', 'NN'),\n",
       " ('becuase', 'becuase', 'VBP'): ('because', 'VBP'),\n",
       " ('embarassed', 'embarassed', 'VBN'): ('embarrassed', 'VBN'),\n",
       " ('seperated', 'seperated', 'VBD'): ('separated', 'VBD'),\n",
       " ('diciplined', 'diciplined', 'VBN'): ('disciplined', 'VBN'),\n",
       " ('nto', 'nto', 'IN'): ('to', 'IN'),\n",
       " ('destory', 'destory', 'VB'): ('destroy', 'VB'),\n",
       " ('skrable', 'skrable', 'JJ'): ('skiable', 'JJ'),\n",
       " ('guk', 'guk', 'NN'): ('guy', 'NN'),\n",
       " ('habbits', 'habbits', 'NNS'): ('habits', 'NNS'),\n",
       " ('softwears', 'softwears', 'NNS'): ('software', 'NNS'),\n",
       " ('pratice', 'pratice', 'VB'): ('practice', 'VB'),\n",
       " ('porgrame', 'porgrame', 'NN'): ('program', 'NN'),\n",
       " ('porgram', 'porgram', 'NN'): ('program', 'NN'),\n",
       " ('typeing', 'typeing', 'VBG'): ('typing', 'VBG'),\n",
       " ('sisiter', 'sisiter', 'NN'): ('sister', 'NN'),\n",
       " ('becasue', 'becasue', 'IN'): ('because', 'IN'),\n",
       " ('konw', 'konw', 'VB'): ('know', 'VB'),\n",
       " ('someome', 'someome', 'NN'): ('someone', 'NN'),\n",
       " ('gramatical', 'gramatical', 'JJ'): ('grammatical', 'JJ'),\n",
       " ('somestr', 'somestr', 'NN'): ('semester', 'NN'),\n",
       " ('nyu', 'nyu', 'NN'): ('nyx', 'NN'),\n",
       " ('differnt', 'differnt', 'VBN'): ('different', 'VBN'),\n",
       " ('prefering', 'prefering', 'VBG'): ('preferring', 'VBG'),\n",
       " ('harmfull', 'harmfull', 'NN'): ('harmful', 'NN'),\n",
       " ('dirsor', 'dirsor', 'NN'): ('mirror', 'NN'),\n",
       " ('trademen', 'trademen', 'NNS'): ('tradesmen', 'NNS'),\n",
       " ('iss', 'iss', 'NN'): ('is', 'NN'),\n",
       " ('ususally', 'ususally', 'RB'): ('usually', 'RB'),\n",
       " ('counrty', 'counrty', 'NN'): ('county', 'NN'),\n",
       " ('girlfrind', 'girlfrind', 'NN'): ('girlfriend', 'NN'),\n",
       " ('basical', 'basical', 'JJ'): ('basic', 'JJ'),\n",
       " ('stil', 'stil', 'VBP'): ('still', 'VBP'),\n",
       " ('dishs', 'dishs', 'NN'): ('dish', 'NN'),\n",
       " ('disipline', 'disipline', 'NN'): ('discipline', 'NN'),\n",
       " ('stors', 'stors', 'NNPS'): ('store', 'NNPS'),\n",
       " ('servise', 'servise', 'NN'): ('service', 'NN'),\n",
       " ('togther', 'togther', 'NN'): ('together', 'NN'),\n",
       " ('vegeterian', 'vegeterian', 'JJ'): ('vegetarian', 'JJ'),\n",
       " ('tofel', 'tofel', 'JJ'): ('towel', 'JJ'),\n",
       " ('shold', 'shold', 'VBP'): ('should', 'VBP'),\n",
       " ('hemspheres', 'hemspheres', 'NNS'): ('hemispheres', 'NNS'),\n",
       " ('hemsphere', 'hemsphere', 'RB'): ('hemisphere', 'RB'),\n",
       " ('attintion', 'attintion', 'NN'): ('attention', 'NN'),\n",
       " ('lamen', 'lamen', 'NNS'): ('laden', 'NNS'),\n",
       " ('benfits', 'benfits', 'NNS'): ('benefits', 'NNS'),\n",
       " ('diffrences', 'diffrences', 'NNS'): ('differences', 'NNS'),\n",
       " ('speacial', 'speacial', 'JJ'): ('special', 'JJ'),\n",
       " ('treament', 'treament', 'NN'): ('treatment', 'NN'),\n",
       " ('shuld', 'shuld', 'MD'): ('should', 'MD'),\n",
       " ('biger', 'biger', 'JJR'): ('tiger', 'JJR'),\n",
       " ('helthy', 'helthy', 'JJ'): ('healthy', 'JJ'),\n",
       " ('attendence', 'attendence', 'NN'): ('attendance', 'NN'),\n",
       " ('plase', 'plase', 'NN'): ('please', 'NN'),\n",
       " ('bikinies', 'bikinies', 'NNS'): ('bikinis', 'NNS'),\n",
       " ('difrent', 'difrent', 'JJ'): ('different', 'JJ'),\n",
       " ('shoul', 'shoul', 'VBP'): ('should', 'VBP'),\n",
       " ('transporation', 'transporation', 'NN'): ('transportation', 'NN'),\n",
       " ('shoping', 'shoping', 'VBG'): ('shopping', 'VBG'),\n",
       " ('freetime', 'freetime', 'NN'): ('freebie', 'NN'),\n",
       " ('inconfidence', 'inconfidence', 'NN'): ('confidence', 'NN'),\n",
       " ('exmple', 'exmple', 'NN'): ('example', 'NN'),\n",
       " ('aggs', 'aggs', 'NN'): ('ages', 'NN'),\n",
       " ('finacial', 'finacial', 'JJ'): ('financial', 'JJ'),\n",
       " ('adition', 'adition', 'NN'): ('edition', 'NN'),\n",
       " ('reaserch', 'reaserch', 'NN'): ('research', 'NN'),\n",
       " ('meate', 'meate', 'NN'): ('meat', 'NN'),\n",
       " ('wheater', 'wheater', 'NN'): ('heater', 'NN'),\n",
       " ('famus', 'famus', 'JJ'): ('famous', 'JJ'),\n",
       " ('fellings', 'fellings', 'NNS'): ('feelings', 'NNS'),\n",
       " ('maked', 'maked', 'VBD'): ('make', 'VBD'),\n",
       " ('adivices', 'adivices', 'NNS'): ('advices', 'NNS'),\n",
       " ('firecrakers', 'firecrakers', 'NNS'): ('firecrackers', 'NNS'),\n",
       " ('experenice', 'experenice', 'NN'): ('experience', 'NN'),\n",
       " ('advertisments', 'advertisments', 'NNS'): ('advertisements', 'NNS'),\n",
       " ('adivces', 'adivces', 'NNS'): ('advices', 'NNS'),\n",
       " ('facinated', 'facinated', 'VBN'): ('fascinated', 'VBN'),\n",
       " ('adviced', 'adviced', 'VBD'): ('advice', 'VBD'),\n",
       " ('explaination', 'explaination', 'NN'): ('explanation', 'NN'),\n",
       " ('restuarant', 'restuarant', 'NN'): ('restaurant', 'NN'),\n",
       " ('expencive', 'expencive', 'JJ'): ('expensive', 'JJ'),\n",
       " ('favorate', 'favorate', 'JJ'): ('favourite', 'JJ'),\n",
       " ('restorant', 'restorant', 'NN'): ('restaurant', 'NN'),\n",
       " ('finaly', 'finaly', 'NN'): ('final', 'NN'),\n",
       " ('whoes', 'whoes', 'NNS'): ('shoes', 'NNS'),\n",
       " ('roomates', 'roomates', 'NNS'): ('roommates', 'NNS'),\n",
       " ('ruels', 'ruels', 'NNS'): ('rules', 'NNS'),\n",
       " ('acommodations', 'acommodations', 'NNS'): ('accommodations', 'NNS'),\n",
       " ('festifal', 'festifal', 'NN'): ('festival', 'NN'),\n",
       " ('runing', 'runing', 'VBG'): ('running', 'VBG'),\n",
       " ('mounths', 'mounths', 'NNS'): ('months', 'NNS'),\n",
       " ('restraunt', 'restraunt', 'NN'): ('restraint', 'NN'),\n",
       " ('fainally', 'fainally', 'RB'): ('finally', 'RB'),\n",
       " ('whith', 'whith', 'JJ'): ('with', 'JJ'),\n",
       " ('remenber', 'remenber', 'VBP'): ('remember', 'VBP'),\n",
       " ('encourge', 'encourge', 'VB'): ('encourage', 'VB'),\n",
       " ('moniter', 'moniter', 'NN'): ('monitor', 'NN'),\n",
       " ('relized', 'relized', 'VBD'): ('realized', 'VBD'),\n",
       " ('rac', 'rac', 'VB'): ('mac', 'VB'),\n",
       " ('everytime', 'everytime', 'JJ'): ('overtime', 'JJ'),\n",
       " ('accross', 'accross', 'IN'): ('across', 'IN'),\n",
       " ('espicially', 'espicially', 'RB'): ('especially', 'RB'),\n",
       " ('researchs', 'researchs', 'NNS'): ('research', 'NNS'),\n",
       " ('existance', 'existance', 'NN'): ('existence', 'NN'),\n",
       " ('famos', 'famos', 'JJ'): ('famous', 'JJ'),\n",
       " ('advisces', 'advisces', 'NNS'): ('advises', 'NNS'),\n",
       " ('adventages', 'adventages', 'NNS'): ('advantages', 'NNS'),\n",
       " ('sould', 'sould', 'JJ'): ('would', 'JJ'),\n",
       " ('langage', 'langage', 'NN'): ('language', 'NN'),\n",
       " ('convience', 'convience', 'NN'): ('convince', 'NN'),\n",
       " ('pinao', 'pinao', 'NN'): ('piano', 'NN'),\n",
       " ('attoney', 'attoney', 'NN'): ('attorney', 'NN'),\n",
       " ('shuld', 'shuld', 'VBP'): ('should', 'VBP'),\n",
       " ('attitute', 'attitute', 'NN'): ('attitude', 'NN'),\n",
       " ('thoap', 'thoap', 'NN'): ('thorp', 'NN'),\n",
       " ('jolary', 'jolary', 'NN'): ('salary', 'NN'),\n",
       " ('sould', 'sould', 'MD'): ('would', 'MD'),\n",
       " ('attentivety', 'attentivety', 'NN'): ('attentively', 'NN'),\n",
       " ('goodisms', 'goodisms', 'NN'): ('goodies', 'NN'),\n",
       " ('dificult', 'dificult', 'JJ'): ('difficult', 'JJ'),\n",
       " ('bucause', 'bucause', 'IN'): ('because', 'IN'),\n",
       " ('behinde', 'behinde', 'IN'): ('behind', 'IN'),\n",
       " ('laguages', 'laguages', 'NNS'): ('languages', 'NNS'),\n",
       " ('exausted', 'exausted', 'VBN'): ('exhausted', 'VBN'),\n",
       " ('tryed', 'tryed', 'VBN'): ('tried', 'VBN'),\n",
       " ('strenght', 'strenght', 'NN'): ('strength', 'NN'),\n",
       " ('exampe', 'exampe', 'NN'): ('example', 'NN'),\n",
       " ('maind', 'maind', 'VB'): ('main', 'VB'),\n",
       " ('woked', 'woked', 'VBD'): ('worked', 'VBD'),\n",
       " ('differant', 'differant', 'JJ'): ('different', 'JJ'),\n",
       " ('quistion', 'quistion', 'NN'): ('question', 'NN'),\n",
       " ('mokhtar', 'mokhtar', 'NN'): ('mortar', 'NN'),\n",
       " ('humman', 'humman', 'JJ'): ('human', 'JJ'),\n",
       " ('similarties', 'similarties', 'NNS'): ('similarities', 'NNS'),\n",
       " ('qutra', 'qutra', 'NN'): ('sutra', 'NN'),\n",
       " ('sucess', 'sucess', 'NN'): ('success', 'NN'),\n",
       " ('orginal', 'orginal', 'JJ'): ('original', 'JJ'),\n",
       " ('univresl', 'univresl', 'NN'): ('universal', 'NN'),\n",
       " ('accaunting', 'accaunting', 'VBG'): ('accounting', 'VBG'),\n",
       " ('mahy', 'mahy', 'NN'): ('may', 'NN'),\n",
       " ('nimono', 'nimono', 'FW'): ('kimono', 'FW'),\n",
       " ('gotton', 'gotton', 'VB'): ('cotton', 'VB'),\n",
       " ('comunity', 'comunity', 'NN'): ('community', 'NN'),\n",
       " ('mahram', 'mahram', 'NN'): ('madam', 'NN'),\n",
       " ('ocassion', 'ocassion', 'NN'): ('passion', 'NN'),\n",
       " ('opinon', 'opinon', 'NN'): ('opinion', 'NN'),\n",
       " ('aginst', 'aginst', 'IN'): ('against', 'IN'),\n",
       " ('exat', 'exat', 'VBN'): ('eat', 'VBN'),\n",
       " ('moto', 'moto', 'NN'): ('motor', 'NN'),\n",
       " ('behavoir', 'behavoir', 'NN'): ('behaviour', 'NN'),\n",
       " ('carlories', 'carlories', 'NNS'): ('calories', 'NNS'),\n",
       " ('sufferd', 'sufferd', 'VBN'): ('suffer', 'VBN'),\n",
       " ('shoud', 'shoud', 'JJ'): ('should', 'JJ'),\n",
       " ('castel', 'castel', 'NN'): ('castle', 'NN'),\n",
       " ('occurance', 'occurance', 'NN'): ('occurrence', 'NN'),\n",
       " ('merried', 'merried', 'VBN'): ('married', 'VBN'),\n",
       " ('aladha', 'aladha', 'NN'): ('alaska', 'NN'),\n",
       " ('assalam', 'assalam', 'NN'): ('assam', 'NN'),\n",
       " ('stly', 'stly', 'RB'): ('stay', 'RB'),\n",
       " ('shawal', 'shawal', 'NN'): ('shawl', 'NN'),\n",
       " ('shawer', 'shawer', 'NN'): ('shower', 'NN'),\n",
       " ('spaice', 'spaice', 'NN'): ('space', 'NN'),\n",
       " ('mathmetics', 'mathmetics', 'NNS'): ('mathematics', 'NNS'),\n",
       " ('shcool', 'shcool', 'VB'): ('school', 'VB'),\n",
       " ('rigth', 'rigth', 'NN'): ('right', 'NN'),\n",
       " ('othere', 'othere', 'JJ'): ('other', 'JJ'),\n",
       " ('peopl', 'peopl', 'NN'): ('people', 'NN'),\n",
       " ('fgure', 'fgure', 'VB'): ('figure', 'VB'),\n",
       " ('foor', 'foor', 'NN'): ('for', 'NN'),\n",
       " ('coporations', 'coporations', 'NNS'): ('corporations', 'NNS'),\n",
       " ('fnd', 'fnd', 'VB'): ('and', 'VB'),\n",
       " ('jennnifer', 'jennnifer', 'RB'): ('jennifer', 'RB'),\n",
       " ('suprised', 'suprised', 'JJ'): ('surprised', 'JJ'),\n",
       " ('spcial', 'spcial', 'JJ'): ('special', 'JJ'),\n",
       " ('datas', 'datas', 'NNS'): ('data', 'NNS'),\n",
       " ('ghoat', 'ghoat', 'NN'): ('ghost', 'NN'),\n",
       " ('suprise', 'suprise', 'NN'): ('surprise', 'NN'),\n",
       " ('risturants', 'risturants', 'NNS'): ('restaurants', 'NNS'),\n",
       " ('trevel', 'trevel', 'NN'): ('travel', 'NN'),\n",
       " ('tommorow', 'tommorow', 'NN'): ('tomorrow', 'NN'),\n",
       " ('delecious', 'delecious', 'JJ'): ('delicious', 'JJ'),\n",
       " ('treval', 'treval', 'NN'): ('reval', 'NN'),\n",
       " ('askes', 'askes', 'VBZ'): ('asked', 'VBZ'),\n",
       " ('injuires', 'injuires', 'NNS'): ('injuries', 'NNS'),\n",
       " ('thers', 'thers', 'NNS'): ('there', 'NNS'),\n",
       " ('newyork', 'newyork', 'JJ'): ('network', 'JJ'),\n",
       " ('suport', 'suport', 'VB'): ('support', 'VB'),\n",
       " ('yut', 'yut', 'VBN'): ('but', 'VBN'),\n",
       " ('jilbab', 'jilbab', 'NN'): ('bilbao', 'NN'),\n",
       " ('caushioned', 'caushioned', 'VBN'): ('cushioned', 'VBN'),\n",
       " ('belivers', 'belivers', 'NNS'): ('delivers', 'NNS'),\n",
       " ('wintter', 'wintter', 'NN'): ('winter', 'NN'),\n",
       " ('atmospher', 'atmospher', 'NN'): ('atmosphere', 'NN'),\n",
       " ('convinient', 'convinient', 'NN'): ('convenient', 'NN'),\n",
       " ('activties', 'activties', 'NNS'): ('activities', 'NNS'),\n",
       " ('pluged', 'pluged', 'VBN'): ('plugged', 'VBN'),\n",
       " ('iam', 'iam', 'JJ'): ('am', 'JJ'),\n",
       " ('researchs', 'researchs', 'NN'): ('research', 'NN'),\n",
       " ('nighbor', 'nighbor', 'NN'): ('neighbour', 'NN'),\n",
       " ('researsh', 'researsh', 'NN'): ('research', 'NN'),\n",
       " ('convinient', 'convinient', 'JJ'): ('convenient', 'JJ'),\n",
       " ('insted', 'insted', 'VBN'): ('instead', 'VBN'),\n",
       " ('poeple', 'poeple', 'JJ'): ('people', 'JJ'),\n",
       " ('usb', 'usb', 'NN'): ('us', 'NN'),\n",
       " ('insit', 'insit', 'VBP'): ('inst', 'VBP'),\n",
       " ('hyeon', 'hyeon', 'NN'): ('hyson', 'NN'),\n",
       " ('soving', 'soving', 'VBG'): ('moving', 'VBG'),\n",
       " ('peple', 'peple', 'NN'): ('people', 'NN'),\n",
       " ('wth', 'wth', 'NN'): ('with', 'NN'),\n",
       " ('useing', 'useing', 'VBG'): ('using', 'VBG'),\n",
       " ('comparsions', 'comparsions', 'NNS'): ('comparisons', 'NNS'),\n",
       " ('nicefood', 'nicefood', 'NN'): ('ninefold', 'NN'),\n",
       " ('comparision', 'comparision', 'NN'): ('comparison', 'NN'),\n",
       " ('sunligh', 'sunligh', 'JJ'): ('sunlight', 'JJ'),\n",
       " ('disapeared', 'disapeared', 'VBN'): ('disappeared', 'VBN'),\n",
       " ('shek', 'shek', 'NN'): ('she', 'NN'),\n",
       " ('nga', 'nga', 'NN'): ('aga', 'NN'),\n",
       " ('benefitial', 'benefitial', 'JJ'): ('beneficial', 'JJ'),\n",
       " ('concidered', 'concidered', 'VBN'): ('considered', 'VBN'),\n",
       " ('kowen', 'kowen', 'VBZ'): ('owen', 'VBZ'),\n",
       " ('successed', 'successed', 'VBD'): ('successes', 'VBD'),\n",
       " ('intertainment', 'intertainment', 'JJ'): ('entertainment', 'JJ'),\n",
       " ('meetng', 'meetng', 'NN'): ('meeting', 'NN'),\n",
       " ('youg', 'youg', 'NN'): ('you', 'NN'),\n",
       " ('beacause', 'beacause', 'NN'): ('because', 'NN'),\n",
       " ('beacause', 'beacause', 'IN'): ('because', 'IN'),\n",
       " ('restarant', 'restarant', 'NN'): ('restaurant', 'NN'),\n",
       " ('somone', 'somone', 'NN'): ('someone', 'NN'),\n",
       " ('guiet', 'guiet', 'JJ'): ('quiet', 'JJ'),\n",
       " ('bcak', 'bcak', 'RB'): ('back', 'RB'),\n",
       " ('adventrous', 'adventrous', 'JJ'): ('adventurous', 'JJ'),\n",
       " ('finshed', 'finshed', 'VBN'): ('finished', 'VBN'),\n",
       " ('experiances', 'experiances', 'NNS'): ('experiences', 'NNS'),\n",
       " ('intersted', 'intersted', 'VBD'): ('interested', 'VBD'),\n",
       " ('hopital', 'hopital', 'NN'): ('hospital', 'NN'),\n",
       " ('recieve', 'recieve', 'VBP'): ('receive', 'VBP'),\n",
       " ('desappeared', 'desappeared', 'JJ'): ('disappeared', 'JJ'),\n",
       " ('desappeared', 'desappeared', 'VBN'): ('disappeared', 'VBN'),\n",
       " ('growsth', 'growsth', 'NN'): ('growth', 'NN'),\n",
       " ('tipical', 'tipical', 'JJ'): ('typical', 'JJ'),\n",
       " ('possitive', 'possitive', 'JJ'): ('positive', 'JJ'),\n",
       " ('becuase', 'becuase', 'IN'): ('because', 'IN'),\n",
       " ('npr', 'npr', 'NN'): ('apr', 'NN'),\n",
       " ('basid', 'basid', 'VBZ'): ('based', 'VBZ'),\n",
       " ('considartion', 'considartion', 'NN'): ('consideration', 'NN'),\n",
       " ('oppotunities', 'oppotunities', 'NNS'): ('opportunities', 'NNS'),\n",
       " ('restaruants', 'restaruants', 'NNS'): ('restaurants', 'NNS'),\n",
       " ('consedred', 'consedred', 'VBN'): ('considered', 'VBN'),\n",
       " ('persent', 'persent', 'NN'): ('present', 'NN'),\n",
       " ('eveyone', 'eveyone', 'RB'): ('everyone', 'RB'),\n",
       " ('socrifice', 'socrifice', 'NN'): ('sacrifice', 'NN'),\n",
       " ('flexiable', 'flexiable', 'JJ'): ('flexible', 'JJ'),\n",
       " ('dentis', 'dentis', 'NN'): ('dennis', 'NN'),\n",
       " ('becaue', 'becaue', 'NN'): ('because', 'NN'),\n",
       " ('becaus', 'becaus', 'NN'): ('because', 'NN'),\n",
       " ('twon', 'twon', 'NN'): ('two', 'NN'),\n",
       " ('adolescen', 'adolescen', 'NNS'): ('adolescent', 'NNS'),\n",
       " ('acction', 'acction', 'NN'): ('action', 'NN'),\n",
       " ('bussiness', 'bussiness', 'JJ'): ('business', 'JJ'),\n",
       " ('fineshed', 'fineshed', 'VBD'): ('finished', 'VBD'),\n",
       " ('everytime', 'everytime', 'RB'): ('overtime', 'RB'),\n",
       " ('kamote', 'kamote', 'JJ'): ('remote', 'JJ'),\n",
       " ('responsablity', 'responsablity', 'NN'): ('responsibility', 'NN'),\n",
       " ('studing', 'studing', 'NN'): ('studying', 'NN'),\n",
       " ('nutrious', 'nutrious', 'JJ'): ('curious', 'JJ'),\n",
       " ('intervew', 'intervew', 'NN'): ('interview', 'NN'),\n",
       " ('tzu', 'tzu', 'NN'): ('thu', 'NN'),\n",
       " ('porblem', 'porblem', 'NN'): ('problem', 'NN'),\n",
       " ('experence', 'experence', 'NN'): ('experience', 'NN'),\n",
       " ('someting', 'someting', 'VBG'): ('something', 'VBG'),\n",
       " ('evetually', 'evetually', 'RB'): ('eventually', 'RB'),\n",
       " ('beause', 'beause', 'NN'): ('because', 'NN'),\n",
       " ('responsibilty', 'responsibilty', 'NN'): ('responsibility', 'NN'),\n",
       " ('feathres', 'feathres', 'NNS'): ('features', 'NNS'),\n",
       " ('bycicle', 'bycicle', 'NN'): ('bicycle', 'NN'),\n",
       " ('ubiqitous', 'ubiqitous', 'JJ'): ('ubiquitous', 'JJ'),\n",
       " ('respsct', 'respsct', 'VB'): ('respect', 'VB'),\n",
       " ('maried', 'maried', 'JJ'): ('married', 'JJ'),\n",
       " ('everynight', 'everynight', 'NN'): ('overnight', 'NN'),\n",
       " ('habbit', 'habbit', 'NN'): ('rabbit', 'NN'),\n",
       " ('businese', 'businese', 'NN'): ('business', 'NN'),\n",
       " ('unculis', 'unculis', 'JJ'): ('uncles', 'JJ'),\n",
       " ('becuse', 'becuse', 'VB'): ('because', 'VB'),\n",
       " ('smal', 'smal', 'JJ'): ('small', 'JJ'),\n",
       " ('fimaliy', 'fimaliy', 'RB'): ('finally', 'RB'),\n",
       " ('befor', 'befor', 'VBZ'): ('before', 'VBZ'),\n",
       " ('succecs', 'succecs', 'VBN'): ('success', 'VBN'),\n",
       " ('dicease', 'dicease', 'NN'): ('disease', 'NN'),\n",
       " ('unfortunatelly', 'unfortunatelly', 'RB'): ('unfortunately', 'RB'),\n",
       " ('dicided', 'dicided', 'VBD'): ('decided', 'VBD'),\n",
       " ('grammer', 'grammer', 'JJ'): ('grammar', 'JJ'),\n",
       " ('sindy', 'sindy', 'NN'): ('sandy', 'NN'),\n",
       " ('throgh', 'throgh', 'RB'): ('through', 'RB'),\n",
       " ('wondring', 'wondring', 'VBG'): ('wondering', 'VBG'),\n",
       " ('acheive', 'acheive', 'VB'): ('achieve', 'VB'),\n",
       " ('rades', 'rades', 'NNS'): ('rates', 'NNS'),\n",
       " ('recomendation', 'recomendation', 'NN'): ('recommendation', 'NN'),\n",
       " ('pesonalities', 'pesonalities', 'NNS'): ('personalities', 'NNS'),\n",
       " ('organaized', 'organaized', 'VBD'): ('organized', 'VBD'),\n",
       " ('conclusin', 'conclusin', 'NN'): ('conclusion', 'NN'),\n",
       " ('interduce', 'interduce', 'VB'): ('interface', 'VB'),\n",
       " ('homeloving', 'homeloving', 'NN'): ('homecoming', 'NN'),\n",
       " ('conutry', 'conutry', 'NN'): ('country', 'NN'),\n",
       " ('happend', 'happend', 'NN'): ('happen', 'NN'),\n",
       " ('trys', 'trys', 'VBD'): ('try', 'VBD'),\n",
       " ('kuk', 'kuk', 'NN'): ('kun', 'NN'),\n",
       " ('acheivement', 'acheivement', 'NN'): ('achievement', 'NN'),\n",
       " ('grabed', 'grabed', 'VBD'): ('graded', 'VBD'),\n",
       " ('concl', 'concl', 'NN'): ('conc', 'NN'),\n",
       " ('mosqe', 'mosqe', 'NN'): ('mosque', 'NN'),\n",
       " ('majers', 'majers', 'NNS'): ('makers', 'NNS'),\n",
       " ('diffcult', 'diffcult', 'JJ'): ('difficult', 'JJ'),\n",
       " ('qutra', 'qutra', 'NNS'): ('sutra', 'NNS'),\n",
       " ('addittion', 'addittion', 'NN'): ('addition', 'NN'),\n",
       " ('mirre', 'mirre', 'NN'): ('mitre', 'NN'),\n",
       " ('ordor', 'ordor', 'NN'): ('order', 'NN'),\n",
       " ('toei', 'toei', 'JJ'): ('toe', 'JJ'),\n",
       " ('oppsite', 'oppsite', 'NN'): ('opposite', 'NN'),\n",
       " ('houres', 'houres', 'NNS'): ('hours', 'NNS'),\n",
       " ('defination', 'defination', 'NN'): ('definition', 'NN'),\n",
       " ('tiji', 'tiji', 'JJ'): ('fiji', 'JJ'),\n",
       " ('definately', 'definately', 'RB'): ('definitely', 'RB'),\n",
       " ('beduoins', 'beduoins', 'NNS'): ('bedouins', 'NNS'),\n",
       " ('turist', 'turist', 'NN'): ('tourist', 'NN'),\n",
       " ('internacional', 'internacional', 'JJ'): ('international', 'JJ'),\n",
       " ('politaical', 'politaical', 'JJ'): ('political', 'JJ'),\n",
       " ('howerer', 'howerer', 'NN'): ('however', 'NN'),\n",
       " ('firest', 'firest', 'JJS'): ('first', 'JJS'),\n",
       " ('detaily', 'detaily', 'RB'): ('details', 'RB'),\n",
       " ('skiny', 'skiny', 'JJ'): ('skin', 'JJ'),\n",
       " ('firwords', 'firwords', 'NNS'): ('fireworks', 'NNS'),\n",
       " ('defferent', 'defferent', 'NN'): ('different', 'NN'),\n",
       " ('writen', 'writen', 'VBN'): ('write', 'VBN'),\n",
       " ('examble', 'examble', 'JJ'): ('example', 'JJ'),\n",
       " ('peroid', 'peroid', 'NN'): ('period', 'NN'),\n",
       " ('adecision', 'adecision', 'NN'): ('decision', 'NN'),\n",
       " ('af', 'af', 'IN'): ('of', 'IN'),\n",
       " ('knowlege', 'knowlege', 'NNS'): ('knowledge', 'NNS'),\n",
       " ('confertable', 'confertable', 'JJ'): ('comfortable', 'JJ'),\n",
       " ('resones', 'resones', 'NNS'): ('response', 'NNS'),\n",
       " ('htm', 'htm', 'VB'): ('him', 'VB'),\n",
       " ('sities', 'sities', 'NNS'): ('sites', 'NNS'),\n",
       " ('deebly', 'deebly', 'RB'): ('deeply', 'RB'),\n",
       " ('campanie', 'campanie', 'NN'): ('campania', 'NN'),\n",
       " ('copsule', 'copsule', 'NN'): ('capsule', 'NN'),\n",
       " ('alawys', 'alawys', 'VBP'): ('always', 'VBP'),\n",
       " ('treeflights', 'treeflights', 'NNS'): ('streetlights', 'NNS'),\n",
       " ('easely', 'easely', 'RB'): ('easily', 'RB'),\n",
       " ('ft', 'ft', 'NN'): ('it', 'NN'),\n",
       " ('naitve', 'naitve', 'JJ'): ('native', 'JJ'),\n",
       " ('imortant', 'imortant', 'JJ'): ('important', 'JJ'),\n",
       " ('easilly', 'easilly', 'RB'): ('easily', 'RB'),\n",
       " ('reasones', 'reasones', 'NNS'): ('reasons', 'NNS'),\n",
       " ('healty', 'healty', 'JJ'): ('health', 'JJ'),\n",
       " ('factores', 'factores', 'NNS'): ('factors', 'NNS'),\n",
       " ('easyier', 'easyier', 'JJR'): ('easier', 'JJR'),\n",
       " ('iwork', 'iwork', 'NN'): ('work', 'NN'),\n",
       " ('registeration', 'registeration', 'NN'): ('registration', 'NN'),\n",
       " ('losts', 'losts', 'VBZ'): ('posts', 'VBZ'),\n",
       " ('exemple', 'exemple', 'NN'): ('example', 'NN'),\n",
       " ('enghlish', 'enghlish', 'JJ'): ('english', 'JJ'),\n",
       " ('phychic', 'phychic', 'JJ'): ('psychic', 'JJ'),\n",
       " ('eastes', 'eastes', 'NNS'): ('easter', 'NNS'),\n",
       " ('easyly', 'easyly', 'VBP'): ('easily', 'VBP'),\n",
       " ('applyed', 'applyed', 'VBN'): ('applied', 'VBN'),\n",
       " ('indivisual', 'indivisual', 'JJ'): ('individual', 'JJ'),\n",
       " ('representive', 'representive', 'JJ'): ('representative', 'JJ'),\n",
       " ('oneday', 'oneday', 'NN'): ('monday', 'NN'),\n",
       " ('famouse', 'famouse', 'JJ'): ('famous', 'JJ'),\n",
       " ('proplem', 'proplem', 'NN'): ('problem', 'NN'),\n",
       " ('taht', 'taht', 'JJ'): ('that', 'JJ'),\n",
       " ('wathcing', 'wathcing', 'VBG'): ('watching', 'VBG'),\n",
       " ('imporatnt', 'imporatnt', 'JJ'): ('important', 'JJ'),\n",
       " ('sponser', 'sponser', 'NN'): ('sponsor', 'NN'),\n",
       " ('bousporus', 'bousporus', 'NN'): ('bosporus', 'NN'),\n",
       " ('watche', 'watche', 'VBP'): ('watch', 'VBP'),\n",
       " ('anout', 'anout', 'IN'): ('about', 'IN'),\n",
       " ('paloteo', 'paloteo', 'NN'): ('palate', 'NN'),\n",
       " ('mla', 'mla', 'NN'): ('la', 'NN'),\n",
       " ('clothers', 'clothers', 'NNS'): ('clothes', 'NNS'),\n",
       " ('cmfortable', 'cmfortable', 'JJ'): ('comfortable', 'JJ'),\n",
       " ('spirite', 'spirite', 'NN'): ('spirit', 'NN'),\n",
       " ('totaly', 'totaly', 'NNS'): ('total', 'NNS'),\n",
       " ('nave', 'nave', 'JJ'): ('naive', 'JJ'),\n",
       " ('secients', 'secients', 'NNS'): ('secrets', 'NNS'),\n",
       " ('biggist', 'biggist', 'NN'): ('biggest', 'NN'),\n",
       " ('spicies', 'spicies', 'NNS'): ('species', 'NNS'),\n",
       " ('cheicken', 'cheicken', 'NN'): ('chicken', 'NN'),\n",
       " ('bigest', 'bigest', 'JJS'): ('biggest', 'JJS'),\n",
       " ('ineresting', 'ineresting', 'VBG'): ('interesting', 'VBG'),\n",
       " ('rumore', 'rumore', 'NN'): ('more', 'NN'),\n",
       " ('abju', 'abju', 'NN'): ('about', 'NN'),\n",
       " ('seconed', 'seconed', 'VBN'): ('second', 'VBN'),\n",
       " ('influnce', 'influnce', 'NN'): ('influence', 'NN'),\n",
       " ('iphones', 'iphones', 'NNS'): ('phones', 'NNS'),\n",
       " ('ofen', 'ofen', 'VBP'): ('open', 'VBP'),\n",
       " ('brasilian', 'brasilian', 'JJ'): ('brazilian', 'JJ'),\n",
       " ('trasic', 'trasic', 'JJ'): ('tragic', 'JJ'),\n",
       " ('faciliate', 'faciliate', 'VB'): ('facilitate', 'VB'),\n",
       " ('hijjah', 'hijjah', 'NN'): ('hijab', 'NN'),\n",
       " ('reduse', 'reduse', 'VB'): ('reduce', 'VB'),\n",
       " ('durty', 'durty', 'NN'): ('duty', 'NN'),\n",
       " ('duu', 'duu', 'NN'): ('due', 'NN'),\n",
       " ('waies', 'waies', 'NNS'): ('wales', 'NNS'),\n",
       " ('dvd', 'dvd', 'NN'): ('did', 'NN'),\n",
       " ('immuno', 'immuno', 'VB'): ('immune', 'VB'),\n",
       " ('paile', 'paile', 'JJ'): ('pale', 'JJ'),\n",
       " ('funy', 'funy', 'JJ'): ('fun', 'JJ'),\n",
       " ('waitting', 'waitting', 'VBG'): ('waiting', 'VBG'),\n",
       " ('cna', 'cna', 'VBP'): ('can', 'VBP'),\n",
       " ('prohibe', 'prohibe', 'VB'): ('profile', 'VB'),\n",
       " ('dcor', 'dcor', 'NN'): ('decor', 'NN'),\n",
       " ('systerm', 'systerm', 'NN'): ('system', 'NN'),\n",
       " ('plaing', 'plaing', 'VBG'): ('playing', 'VBG'),\n",
       " ('mythes', 'mythes', 'NNS'): ('myths', 'NNS'),\n",
       " ('imprtant', 'imprtant', 'JJ'): ('important', 'JJ'),\n",
       " ('mussaman', 'mussaman', 'JJ'): ('mussulman', 'JJ'),\n",
       " ('paragraphes', 'paragraphes', 'NNS'): ('paragraphs', 'NNS'),\n",
       " ('mussaman', 'mussaman', 'NN'): ('mussulman', 'NN'),\n",
       " ('elementry', 'elementry', 'JJ'): ('elementary', 'JJ'),\n",
       " ('paragrah', 'paragrah', 'NN'): ('paragraph', 'NN'),\n",
       " ('teachr', 'teachr', 'NN'): ('teacher', 'NN'),\n",
       " ('famers', 'famers', 'NNS'): ('farmers', 'NNS'),\n",
       " ('saulty', 'saulty', 'NN'): ('faulty', 'NN'),\n",
       " ('indentify', 'indentify', 'VB'): ('identify', 'VB'),\n",
       " ('proverty', 'proverty', 'NN'): ('property', 'NN'),\n",
       " ('saudia', 'saudia', 'JJ'): ('saudi', 'JJ'),\n",
       " ('amune', 'amune', 'NN'): ('amine', 'NN'),\n",
       " ('chres', 'chres', 'NNS'): ('chris', 'NNS'),\n",
       " ('musqe', 'musqe', 'NN'): ('muse', 'NN'),\n",
       " ('repesent', 'repesent', 'JJ'): ('represent', 'JJ'),\n",
       " ('bodeis', 'bodeis', 'NN'): ('bodies', 'NN'),\n",
       " ('proyect', 'proyect', 'NN'): ('project', 'NN'),\n",
       " ('freids', 'freids', 'NNS'): ('freida', 'NNS'),\n",
       " ('trafic', 'trafic', 'JJ'): ('traffic', 'JJ'),\n",
       " ('tecniques', 'tecniques', 'NNS'): ('techniques', 'NNS'),\n",
       " ('bofeah', 'bofeah', 'JJ'): ('boreal', 'JJ'),\n",
       " ('llth', 'llth', 'NN'): ('lith', 'NN'),\n",
       " ('staeke', 'staeke', 'NN'): ('stake', 'NN'),\n",
       " ('exhusted', 'exhusted', 'VBN'): ('exhausted', 'VBN'),\n",
       " ('couse', 'couse', 'NN'): ('house', 'NN'),\n",
       " ('boild', 'boild', 'JJ'): ('build', 'JJ'),\n",
       " ('samll', 'samll', 'JJ'): ('small', 'JJ'),\n",
       " ('sandwitch', 'sandwitch', 'NN'): ('sandwich', 'NN'),\n",
       " ('teath', 'teath', 'NN'): ('death', 'NN'),\n",
       " ('participat', 'participat', 'VB'): ('participate', 'VB'),\n",
       " ('famely', 'famely', 'RB'): ('family', 'RB'),\n",
       " ('regaring', 'regaring', 'VBG'): ('regarding', 'VBG'),\n",
       " ('salam', 'salam', 'NN'): ('salem', 'NN'),\n",
       " ('listning', 'listning', 'VBG'): ('listing', 'VBG'),\n",
       " ('eduation', 'eduation', 'NN'): ('education', 'NN'),\n",
       " ('choiced', 'choiced', 'VBN'): ('choice', 'VBN'),\n",
       " ('countreis', 'countreis', 'NN'): ('countries', 'NN'),\n",
       " ('emportant', 'emportant', 'JJ'): ('important', 'JJ'),\n",
       " ('angery', 'angery', 'RB'): ('angry', 'RB'),\n",
       " ('emotibob', 'emotibob', 'NN'): ('emotion', 'NN'),\n",
       " ('transfered', 'transfered', 'VBD'): ('transferred', 'VBD'),\n",
       " ('scholership', 'scholership', 'NN'): ('scholarship', 'NN'),\n",
       " ('psycologists', 'psycologists', 'NNS'): ('psychologists', 'NNS'),\n",
       " ('scholl', 'scholl', 'VB'): ('school', 'VB'),\n",
       " ('panera', 'panera', 'NN'): ('paper', 'NN'),\n",
       " ('chirpings', 'chirpings', 'NNS'): ('chirping', 'NNS'),\n",
       " ('psycological', 'psycological', 'JJ'): ('psychological', 'JJ'),\n",
       " ('scheduals', 'scheduals', 'NNS'): ('schedule', 'NNS'),\n",
       " ('countrys', 'countrys', 'JJ'): ('country', 'JJ'),\n",
       " ('helpfull', 'helpfull', 'NN'): ('helpful', 'NN'),\n",
       " ('importnat', 'importnat', 'JJ'): ('important', 'JJ'),\n",
       " ('weekand', 'weekand', 'NN'): ('weekend', 'NN'),\n",
       " ('blieve', 'blieve', 'VBP'): ('believe', 'VBP'),\n",
       " ('ancxious', 'ancxious', 'JJ'): ('anxious', 'JJ'),\n",
       " ('countrys', 'countrys', 'NN'): ('country', 'NN'),\n",
       " ('famliy', 'famliy', 'NN'): ('family', 'NN'),\n",
       " ('litte', 'litte', 'NN'): ('little', 'NN'),\n",
       " ('middel', 'middel', 'NN'): ('middle', 'NN'),\n",
       " ('particualr', 'particualr', 'JJ'): ('particular', 'JJ'),\n",
       " ('chopstics', 'chopstics', 'NNS'): ('chopsticks', 'NNS'),\n",
       " ('tatal', 'tatal', 'JJ'): ('total', 'JJ'),\n",
       " ('ssshh', 'ssshh', 'NN'): ('ssh', 'NN'),\n",
       " ('enjoing', 'enjoing', 'VBG'): ('enjoying', 'VBG'),\n",
       " ('profite', 'profite', 'NN'): ('profile', 'NN'),\n",
       " ('appoinment', 'appoinment', 'NN'): ('appointment', 'NN'),\n",
       " ('beteween', 'beteween', 'VB'): ('between', 'VB'),\n",
       " ('aroud', 'aroud', 'JJ'): ('around', 'JJ'),\n",
       " ('sepented', 'sepented', 'VBD'): ('repented', 'VBD'),\n",
       " ('dallah', 'dallah', 'NN'): ('dallas', 'NN'),\n",
       " ('videogames', 'videogame', 'NNS'): ('videotapes', 'NNS'),\n",
       " ('themo', 'themo', 'NN'): ('them', 'NN'),\n",
       " ('probabaly', 'probabaly', 'IN'): ('probably', 'IN'),\n",
       " ('themo', 'themo', 'EX'): ('them', 'EX'),\n",
       " ('bettery', 'bettery', 'NN'): ('better', 'NN'),\n",
       " ('zhong', 'zhong', 'NN'): ('hong', 'NN'),\n",
       " ('brigten', 'brigten', 'VB'): ('brighten', 'VB'),\n",
       " ('remebered', 'remebered', 'VBN'): ('remembered', 'VBN'),\n",
       " ('haveing', 'haveing', 'VBG'): ('having', 'VBG'),\n",
       " ('neighbohood', 'neighbohood', 'NN'): ('neighbourhood', 'NN'),\n",
       " ('neighber', 'neighber', 'NN'): ('neither', 'NN'),\n",
       " ('foreing', 'foreing', 'JJ'): ('foreign', 'JJ'),\n",
       " ('survice', 'survice', 'NN'): ('service', 'NN'),\n",
       " ('leran', 'leran', 'VB'): ('learn', 'VB'),\n",
       " ('pck', 'pck', 'VB'): ('pc', 'VB'),\n",
       " ('architechure', 'architechure', 'NN'): ('architecture', 'NN'),\n",
       " ('playng', 'playng', 'JJ'): ('playing', 'JJ'),\n",
       " ('lerning', 'lerning', 'VBG'): ('learning', 'VBG'),\n",
       " ('alittle', 'alittle', 'JJ'): ('little', 'JJ'),\n",
       " ('seng', 'seng', 'JJ'): ('send', 'JJ'),\n",
       " ('sutable', 'sutable', 'JJ'): ('suitable', 'JJ'),\n",
       " ('sences', 'sences', 'NNS'): ('senses', 'NNS'),\n",
       " ('bridg', 'bridg', 'NN'): ('bring', 'NN'),\n",
       " ('negetive', 'negetive', 'JJ'): ('negative', 'JJ'),\n",
       " ('comfotable', 'comfotable', 'JJ'): ('comfortable', 'JJ'),\n",
       " ('entertaiment', 'entertaiment', 'NN'): ('entertainment', 'NN'),\n",
       " ('exective', 'exective', 'JJ'): ('executive', 'JJ'),\n",
       " ('seperated', 'seperated', 'VBN'): ('separated', 'VBN'),\n",
       " ('peoblems', 'peoblems', 'NNS'): ('problems', 'NNS'),\n",
       " ('dissapointment', 'dissapointment', 'NN'): ('disappointment', 'NN'),\n",
       " ('peoblem', 'peoblem', 'NN'): ('problem', 'NN'),\n",
       " ('vatiety', 'vatiety', 'NN'): ('variety', 'NN'),\n",
       " ('prewiting', 'prewiting', 'NN'): ('predicting', 'NN'),\n",
       " ('ougth', 'ougth', 'VBP'): ('ought', 'VBP'),\n",
       " ('extereme', 'extereme', 'JJ'): ('extreme', 'JJ'),\n",
       " ('movment', 'movment', 'NN'): ('moment', 'NN'),\n",
       " ('vegatables', 'vegatables', 'NNS'): ('vegetables', 'NNS'),\n",
       " ('servies', 'servies', 'NNS'): ('services', 'NNS'),\n",
       " ('ther', 'ther', 'RB'): ('the', 'RB'),\n",
       " ('forcus', 'forcus', 'VB'): ('focus', 'VB'),\n",
       " ('stil', 'stil', 'JJ'): ('still', 'JJ'),\n",
       " ('prieses', 'prieses', 'NNS'): ('priests', 'NNS'),\n",
       " ('chaeacter', 'chaeacter', 'NN'): ('character', 'NN'),\n",
       " ('alet', 'alet', 'VB'): ('let', 'VB'),\n",
       " ('artical', 'artical', 'JJ'): ('article', 'JJ'),\n",
       " ('ther', 'ther', 'EX'): ('the', 'EX'),\n",
       " ('jeddah', 'jeddah', 'NN'): ('judah', 'NN'),\n",
       " ('verious', 'verious', 'JJ'): ('various', 'JJ'),\n",
       " ('excuting', 'excuting', 'VBG'): ('exciting', 'VBG'),\n",
       " ('genero', 'genero', 'NN'): ('genera', 'NN'),\n",
       " ('speake', 'speake', 'VB'): ('speak', 'VB'),\n",
       " ('victms', 'victms', 'JJ'): ('victims', 'JJ'),\n",
       " ('speakng', 'speakng', 'NN'): ('speaking', 'NN'),\n",
       " ('divorse', 'divorse', 'JJ'): ('divorce', 'JJ'),\n",
       " ('aquired', 'aquired', 'JJ'): ('acquired', 'JJ'),\n",
       " ('abou', 'abou', 'IN'): ('about', 'IN'),\n",
       " ('approximetly', 'approximetly', 'RB'): ('approximately', 'RB'),\n",
       " ('costom', 'costom', 'NN'): ('custom', 'NN'),\n",
       " ('charcteristics', 'charcteristics', 'NNS'): ('characteristics', 'NNS'),\n",
       " ('vitual', 'vitual', 'JJ'): ('virtual', 'JJ'),\n",
       " ('plause', 'plause', 'NN'): ('clause', 'NN'),\n",
       " ('dopperganger', 'dopperganger', 'NN'): ('doppelganger', 'NN'),\n",
       " ('libery', 'libery', 'NN'): ('liberty', 'NN'),\n",
       " ('vo', 'vo', 'NN'): ('to', 'NN'),\n",
       " ('collegue', 'collegue', 'NN'): ('college', 'NN'),\n",
       " ('acknoleged', 'acknoleged', 'VBD'): ('acknowledged', 'VBD'),\n",
       " ('imediately', 'imediately', 'RB'): ('immediately', 'RB'),\n",
       " ('colleage', 'colleage', 'NN'): ('college', 'NN'),\n",
       " ('beutifull', 'beutifull', 'NN'): ('beautiful', 'NN'),\n",
       " ('approching', 'approching', 'VBG'): ('approaching', 'VBG'),\n",
       " ('speking', 'speking', 'VBG'): ('seeking', 'VBG'),\n",
       " ('bifida', 'bifida', 'NN'): ('bifid', 'NN'),\n",
       " ('breake', 'breake', 'NN'): ('break', 'NN'),\n",
       " ('drawed', 'draw', 'VBD'): ('drawer', 'VBD'),\n",
       " ('necesary', 'necesary', 'JJ'): ('necessary', 'JJ'),\n",
       " ('curcial', 'curcial', 'JJ'): ('crucial', 'JJ'),\n",
       " ('vocaton', 'vocaton', 'NN'): ('vocation', 'NN'),\n",
       " ('apportunity', 'apportunity', 'NN'): ('opportunity', 'NN'),\n",
       " ('pcked', 'pcked', 'VBD'): ('picked', 'VBD'),\n",
       " ('futuer', 'futuer', 'NN'): ('future', 'NN'),\n",
       " ('photograghs', 'photograghs', 'NNS'): ('photographs', 'NNS'),\n",
       " ('adn', 'adn', 'JJ'): ('and', 'JJ'),\n",
       " ('outsied', 'outsied', 'VBD'): ('outside', 'VBD'),\n",
       " ('aboout', 'aboout', 'IN'): ('about', 'IN'),\n",
       " ('whtch', 'whtch', 'VBP'): ('which', 'VBP'),\n",
       " ('mnutes', 'mnutes', 'NNS'): ('minutes', 'NNS'),\n",
       " ('visted', 'visted', 'VBD'): ('listed', 'VBD'),\n",
       " ('hc', 'hc', 'NN'): ('he', 'NN'),\n",
       " ('receve', 'receve', 'VBP'): ('receive', 'VBP'),\n",
       " ('stange', 'stange', 'JJ'): ('stage', 'JJ'),\n",
       " ('reccomend', 'reccomend', 'VBP'): ('recommend', 'VBP'),\n",
       " ('coputer', 'coputer', 'NN'): ('computer', 'NN'),\n",
       " ('corecctly', 'corecctly', 'RB'): ('correctly', 'RB'),\n",
       " ('strees', 'strees', 'NNS'): ('street', 'NNS'),\n",
       " ('steroetypes', 'steroetypes', 'NNS'): ('stereotypes', 'NNS'),\n",
       " ('phisicaly', 'phisicaly', 'IN'): ('physical', 'IN'),\n",
       " ('convinent', 'convinent', 'JJ'): ('continent', 'JJ'),\n",
       " ('corect', 'corect', 'JJ'): ('correct', 'JJ'),\n",
       " ('strutures', 'strutures', 'NNS'): ('structures', 'NNS'),\n",
       " ('expenes', 'expenes', 'NNS'): ('expenses', 'NNS'),\n",
       " ('convient', 'convient', 'NN'): ('convent', 'NN'),\n",
       " ('hirement', 'hirement', 'NN'): ('firemen', 'NN'),\n",
       " ('studens', 'studens', 'NNS'): ('students', 'NNS'),\n",
       " ('contradiccion', 'contradiccion', 'NN'): ('contradiction', 'NN'),\n",
       " ('continous', 'continous', 'JJ'): ('continuous', 'JJ'),\n",
       " ('steretypes', 'steretypes', 'NNS'): ('stereotypes', 'NNS'),\n",
       " ('highres', 'highres', 'NNS'): ('highers', 'NNS'),\n",
       " ('courtains', 'courtains', 'NNS'): ('curtains', 'NNS'),\n",
       " ('convenint', 'convenint', 'NN'): ('convenient', 'NN'),\n",
       " ('convinience', 'convinience', 'NN'): ('convenience', 'NN'),\n",
       " ('converient', 'converient', 'JJ'): ('convenient', 'JJ'),\n",
       " ('petersburgh', 'petersburgh', 'NN'): ('petersburg', 'NN'),\n",
       " ('steet', 'steet', 'NN'): ('street', 'NN'),\n",
       " ('reaveling', 'reaveling', 'VBG'): ('revealing', 'VBG'),\n",
       " ('statmenet', 'statmenet', 'NN'): ('statement', 'NN'),\n",
       " ('costomers', 'costomers', 'NNS'): ('customers', 'NNS'),\n",
       " ('homewrk', 'homewrk', 'NN'): ('homework', 'NN'),\n",
       " ('costomer', 'costomer', 'NN'): ('customer', 'NN'),\n",
       " ('converstation', 'converstation', 'NN'): ('conversation', 'NN'),\n",
       " ('recieved', 'recieved', 'VBN'): ('received', 'VBN'),\n",
       " ('pesonality', 'pesonality', 'NN'): ('personality', 'NN'),\n",
       " ('statisfy', 'statisfy', 'VB'): ('satisfy', 'VB'),\n",
       " ('exhusting', 'exhusting', 'VBG'): ('exhausting', 'VBG'),\n",
       " ('strengh', 'strengh', 'NN'): ('strength', 'NN'),\n",
       " ('courtries', 'courtries', 'NNS'): ('countries', 'NNS'),\n",
       " ('religon', 'religon', 'NN'): ('religion', 'NN'),\n",
       " ...}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misspell_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incorporating back into pelic_df\n",
    "\n",
    "pelic_df['tok_POS_corrected'] = pelic_df['tok_lem_POS'].apply\\\n",
    "(lambda row: [misspell_dict[(x[0].lower(),x[1],x[2])] if (x[0].lower(),x[1],x[2]) in misspell_dict else (x[0],x[2]) for x in row])\n",
    "\n",
    "# One minor issue is that this will make misspelled items lower case when originally upper case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('My', 'my', 'PRP$'), ('friend', 'friend', 'NN'), ('is', 'be', 'VBZ'), ('realy', 'realy', 'JJ'), ('nise', 'nise', 'RB'), ('guy', 'guy', 'NN'), ('.', '.', '.'), ('I', 'i', 'PRP'), ('like', 'like', 'VBP'), ('hem', 'hem', 'JJ'), ('becuase', 'becuase', 'NN'), ('he', 'he', 'PRP'), ('is', 'be', 'VBZ'), ('friendlly', 'friendlly', 'RB'), ('and', 'and', 'CC'), ('lovliy', 'lovliy', 'NN'), ('.', '.', '.'))\n",
      "[('My', 'PRP$'), ('friend', 'NN'), ('is', 'VBZ'), ('real', 'JJ'), ('nice', 'RB'), ('guy', 'NN'), ('.', '.'), ('I', 'PRP'), ('like', 'VBP'), ('hem', 'JJ'), ('because', 'NN'), ('he', 'PRP'), ('is', 'VBZ'), ('friendly', 'RB'), ('and', 'CC'), ('lovely', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Checking with 'becuase'\n",
    "\n",
    "print(pelic_df.loc[pelic_df.text.str.contains('becuase')].iloc[1,11]) #uncorrected\n",
    "print(pelic_df.loc[pelic_df.text.str.contains('becuase')].iloc[1,12]) #corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that many approrpriate corrections have been made, including _beccuase_ -> _because_ , _nise_ -> _nice_ , and _lovily_ -> _lovely_ .  \n",
    "Importantly, incorrect spellings that are actual words, e.g. _hem_ (should be _him_ in this case) are not corrected. In addition, as context is not considered, there will be some inaccuracies, e.g. _realy_ (marked as an adj) -> _real_ rather than _really_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anon_id</th>\n",
       "      <th>L1</th>\n",
       "      <th>gender</th>\n",
       "      <th>course_id</th>\n",
       "      <th>level_id</th>\n",
       "      <th>class_id</th>\n",
       "      <th>question_id</th>\n",
       "      <th>version</th>\n",
       "      <th>text_len</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tok_lem_POS</th>\n",
       "      <th>tok_POS_corrected</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eq0</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>Male</td>\n",
       "      <td>149</td>\n",
       "      <td>4</td>\n",
       "      <td>g</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>177</td>\n",
       "      <td>I met my friend Nife while I was studying in a...</td>\n",
       "      <td>[I, met, my, friend, Nife, while, I, was, stud...</td>\n",
       "      <td>((I, i, PRP), (met, meet, VBD), (my, my, PRP$)...</td>\n",
       "      <td>[(I, PRP), (met, VBD), (my, PRP$), (friend, NN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>am8</td>\n",
       "      <td>Thai</td>\n",
       "      <td>Female</td>\n",
       "      <td>149</td>\n",
       "      <td>4</td>\n",
       "      <td>g</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>137</td>\n",
       "      <td>Ten years ago, I met a women on the train betw...</td>\n",
       "      <td>[Ten, years, ago, ,, I, met, a, women, on, the...</td>\n",
       "      <td>((Ten, ten, CD), (years, year, NNS), (ago, ago...</td>\n",
       "      <td>[(Ten, CD), (years, NNS), (ago, RB), (,, ,), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dk5</td>\n",
       "      <td>Turkish</td>\n",
       "      <td>Female</td>\n",
       "      <td>115</td>\n",
       "      <td>4</td>\n",
       "      <td>w</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>In my country we usually don't use tea bags. F...</td>\n",
       "      <td>[In, my, country, we, usually, do, n't, use, t...</td>\n",
       "      <td>((In, in, IN), (my, my, PRP$), (country, count...</td>\n",
       "      <td>[(In, IN), (my, PRP$), (country, NN), (we, PRP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dk5</td>\n",
       "      <td>Turkish</td>\n",
       "      <td>Female</td>\n",
       "      <td>115</td>\n",
       "      <td>4</td>\n",
       "      <td>w</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>I organized the instructions by time.</td>\n",
       "      <td>[I, organized, the, instructions, by, time, .]</td>\n",
       "      <td>((I, i, PRP), (organized, organize, VBD), (the...</td>\n",
       "      <td>[(I, PRP), (organized, VBD), (the, DT), (instr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ad1</td>\n",
       "      <td>Korean</td>\n",
       "      <td>Female</td>\n",
       "      <td>115</td>\n",
       "      <td>4</td>\n",
       "      <td>w</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>59</td>\n",
       "      <td>First, prepare a port, loose tea, and cup.\\nSe...</td>\n",
       "      <td>[First, ,, prepare, a, port, ,, loose, tea, ,,...</td>\n",
       "      <td>((First, first, RB), (,, ,, ,), (prepare, prep...</td>\n",
       "      <td>[(First, RB), (,, ,), (prepare, VB), (a, DT), ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          anon_id       L1  gender course_id level_id class_id question_id  \\\n",
       "answer_id                                                                    \n",
       "1             eq0   Arabic    Male       149        4        g           5   \n",
       "2             am8     Thai  Female       149        4        g           5   \n",
       "3             dk5  Turkish  Female       115        4        w          12   \n",
       "4             dk5  Turkish  Female       115        4        w          13   \n",
       "5             ad1   Korean  Female       115        4        w          12   \n",
       "\n",
       "          version  text_len  \\\n",
       "answer_id                     \n",
       "1               1       177   \n",
       "2               1       137   \n",
       "3               1        64   \n",
       "4               1         6   \n",
       "5               1        59   \n",
       "\n",
       "                                                        text  \\\n",
       "answer_id                                                      \n",
       "1          I met my friend Nife while I was studying in a...   \n",
       "2          Ten years ago, I met a women on the train betw...   \n",
       "3          In my country we usually don't use tea bags. F...   \n",
       "4                      I organized the instructions by time.   \n",
       "5          First, prepare a port, loose tea, and cup.\\nSe...   \n",
       "\n",
       "                                                      tokens  \\\n",
       "answer_id                                                      \n",
       "1          [I, met, my, friend, Nife, while, I, was, stud...   \n",
       "2          [Ten, years, ago, ,, I, met, a, women, on, the...   \n",
       "3          [In, my, country, we, usually, do, n't, use, t...   \n",
       "4             [I, organized, the, instructions, by, time, .]   \n",
       "5          [First, ,, prepare, a, port, ,, loose, tea, ,,...   \n",
       "\n",
       "                                                 tok_lem_POS  \\\n",
       "answer_id                                                      \n",
       "1          ((I, i, PRP), (met, meet, VBD), (my, my, PRP$)...   \n",
       "2          ((Ten, ten, CD), (years, year, NNS), (ago, ago...   \n",
       "3          ((In, in, IN), (my, my, PRP$), (country, count...   \n",
       "4          ((I, i, PRP), (organized, organize, VBD), (the...   \n",
       "5          ((First, first, RB), (,, ,, ,), (prepare, prep...   \n",
       "\n",
       "                                           tok_POS_corrected  \n",
       "answer_id                                                     \n",
       "1          [(I, PRP), (met, VBD), (my, PRP$), (friend, NN...  \n",
       "2          [(Ten, CD), (years, NNS), (ago, RB), (,, ,), (...  \n",
       "3          [(In, IN), (my, PRP$), (country, NN), (we, PRP...  \n",
       "4          [(I, PRP), (organized, VBD), (the, DT), (instr...  \n",
       "5          [(First, RB), (,, ,), (prepare, VB), (a, DT), ...  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pelic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out new PELIC_compiled.csv\n",
    "\n",
    "pelic_df.to_csv('PELIC_compiled_spellcorrected.csv', encoding='utf-8', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle new pelic_df dataframe\n",
    "\n",
    "pelic_df.to_pickle('pelic_spellcorrected.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If preferred, this entire spelling correctin process can also be applied to [`answer.csv`]() instead of `PELIC_compiled`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Corrected-spelling)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
