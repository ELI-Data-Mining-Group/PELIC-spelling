{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PELIC spelling\n",
    "\n",
    "This notebook adds further processing to `PELIC_compiled.csv`  in the [`PELIC-dataset`](https://github.com/ELI-Data-Mining-Group/PELIC-dataset) repo by creating a column of tok_POS whose spelling has been automatically corrected.\n",
    "\n",
    "**Notebook contents:**\n",
    "- [Building `non_words_df`](#Building-non_words_df)\n",
    "- [Building `misspell_df`](#Building-misspell_df)\n",
    "- [Possible segmentation](#Applying-segmentation)\n",
    "- [Applying spelling correction](#Applying-spelling-correction)\n",
    "- [Incorporating corrections into `pelic_df`](#Incorporating-corrections-into-pelic_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building non_words_df\n",
    "In this section, we build a dataframe, `non_words_df`, which collects all of the non-words from the PELIC dataset (in `PELIC_compiled.csv`). The final dataframe has the following columns:\n",
    "- `non_word`: tuples with the non-words and their parts of speech\n",
    "- `sentence`: the complete sentence containing the non-word to provide context\n",
    "- `answer_id`: the id of the text they come from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "import nltk\n",
    "import random\n",
    "from pelitk import lex\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anon_id</th>\n",
       "      <th>L1</th>\n",
       "      <th>gender</th>\n",
       "      <th>course_id</th>\n",
       "      <th>level_id</th>\n",
       "      <th>class_id</th>\n",
       "      <th>question_id</th>\n",
       "      <th>version</th>\n",
       "      <th>text_len</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tok_lem_POS</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eq0</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>Male</td>\n",
       "      <td>149</td>\n",
       "      <td>4</td>\n",
       "      <td>g</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>177</td>\n",
       "      <td>I met my friend Nife while I was studying in a...</td>\n",
       "      <td>[I, met, my, friend, Nife, while, I, was, stud...</td>\n",
       "      <td>((I, i, PRP), (met, meet, VBD), (my, my, PRP$)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>am8</td>\n",
       "      <td>Thai</td>\n",
       "      <td>Female</td>\n",
       "      <td>149</td>\n",
       "      <td>4</td>\n",
       "      <td>g</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>137</td>\n",
       "      <td>Ten years ago, I met a women on the train betw...</td>\n",
       "      <td>[Ten, years, ago, ,, I, met, a, women, on, the...</td>\n",
       "      <td>((Ten, ten, CD), (years, year, NNS), (ago, ago...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dk5</td>\n",
       "      <td>Turkish</td>\n",
       "      <td>Female</td>\n",
       "      <td>115</td>\n",
       "      <td>4</td>\n",
       "      <td>w</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>In my country we usually don't use tea bags. F...</td>\n",
       "      <td>[In, my, country, we, usually, do, n't, use, t...</td>\n",
       "      <td>((In, in, IN), (my, my, PRP$), (country, count...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dk5</td>\n",
       "      <td>Turkish</td>\n",
       "      <td>Female</td>\n",
       "      <td>115</td>\n",
       "      <td>4</td>\n",
       "      <td>w</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>I organized the instructions by time.</td>\n",
       "      <td>[I, organized, the, instructions, by, time, .]</td>\n",
       "      <td>((I, i, PRP), (organized, organize, VBD), (the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ad1</td>\n",
       "      <td>Korean</td>\n",
       "      <td>Female</td>\n",
       "      <td>115</td>\n",
       "      <td>4</td>\n",
       "      <td>w</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>59</td>\n",
       "      <td>First, prepare a port, loose tea, and cup.\\nSe...</td>\n",
       "      <td>[First, ,, prepare, a, port, ,, loose, tea, ,,...</td>\n",
       "      <td>((First, first, RB), (,, ,, ,), (prepare, prep...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          anon_id       L1  gender course_id level_id class_id question_id  \\\n",
       "answer_id                                                                    \n",
       "1             eq0   Arabic    Male       149        4        g           5   \n",
       "2             am8     Thai  Female       149        4        g           5   \n",
       "3             dk5  Turkish  Female       115        4        w          12   \n",
       "4             dk5  Turkish  Female       115        4        w          13   \n",
       "5             ad1   Korean  Female       115        4        w          12   \n",
       "\n",
       "          version  text_len  \\\n",
       "answer_id                     \n",
       "1               1       177   \n",
       "2               1       137   \n",
       "3               1        64   \n",
       "4               1         6   \n",
       "5               1        59   \n",
       "\n",
       "                                                        text  \\\n",
       "answer_id                                                      \n",
       "1          I met my friend Nife while I was studying in a...   \n",
       "2          Ten years ago, I met a women on the train betw...   \n",
       "3          In my country we usually don't use tea bags. F...   \n",
       "4                      I organized the instructions by time.   \n",
       "5          First, prepare a port, loose tea, and cup.\\nSe...   \n",
       "\n",
       "                                                      tokens  \\\n",
       "answer_id                                                      \n",
       "1          [I, met, my, friend, Nife, while, I, was, stud...   \n",
       "2          [Ten, years, ago, ,, I, met, a, women, on, the...   \n",
       "3          [In, my, country, we, usually, do, n't, use, t...   \n",
       "4             [I, organized, the, instructions, by, time, .]   \n",
       "5          [First, ,, prepare, a, port, ,, loose, tea, ,,...   \n",
       "\n",
       "                                                 tok_lem_POS  \n",
       "answer_id                                                     \n",
       "1          ((I, i, PRP), (met, meet, VBD), (my, my, PRP$)...  \n",
       "2          ((Ten, ten, CD), (years, year, NNS), (ago, ago...  \n",
       "3          ((In, in, IN), (my, my, PRP$), (country, count...  \n",
       "4          ((I, i, PRP), (organized, organize, VBD), (the...  \n",
       "5          ((First, first, RB), (,, ,, ,), (prepare, prep...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in PELIC_compiled.csv\n",
    "\n",
    "pelic_df = pd.read_csv(\"../PELIC-dataset/PELIC_compiled.csv\", index_col = 'answer_id', # answer_id is unique\n",
    "                      dtype = {'level_id':'object','question_id':'object','version':'object','course_id':'object'}, # str not ints\n",
    "                               converters={'tokens':literal_eval,'tok_lem_POS':literal_eval}) # read in as lists\n",
    "pelic_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The focus here is the `tok_lem_POS` column, but all columns will be kept as the entire df will be written out at the end of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating small dataframe to be used for finding non-words\n",
    "\n",
    "non_words = pelic_df[['text','tok_lem_POS']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** For spelling correction, it is necessary to decide what list of words will be used for determining if a word is real or not.\n",
    "\n",
    "Here, we use the [`SCOWL_condensed.txt`](https://github.com/ELI-Data-Mining-Group/PELIC-spelling/blob/master/SCOWL_condensed.txt) file which is a combination of wordlists available for download at http://wordlist.aspell.net/. We include items from all the dictionaries _except_ the abbreviations dictionary. For a detailed look at the compilation of this dictionary, please see the [SCOWL_wordlist](https://github.com/ELI-Data-Mining-Group/PELIC-spelling/blob/master/SCOWL_wordlist.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cadelles', 'autonegation', 'rhinion', 'mikvahs', 'buttery']\n"
     ]
    }
   ],
   "source": [
    "#Reading in SCOWL_condensed as a set as a lookup list for spelling (500k words)\n",
    "\n",
    "scowl = set(open(\"SCOWL_condensed.txt\", \"r\").read().split('\\n'))\n",
    "print(random.sample(scowl,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "497552"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scowl = set([x.lower() for x in scowl])\n",
    "len(scowl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a list of words which should be considered words but which were previously being labelled as non-words. These items have been manually added to this list based on output later in this notebook. Most of these items are food items, names, or abbreviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238\n",
      "['adha', 'adj', 'ahamed', 'alaikum', 'anonurlpage', 'antiretroviral', 'arpa', 'atm', 'ave', 'beyonce', 'bibimbap', 'bio', 'biodiesel', 'bioethanol', 'bulgogi', 'bundang', 'cafe', 'carnaval', 'cds', 'cf', 'co', 'comscore', 'cyber', 'ddukboggi', 'def', 'dr', 'eg', 'eid', 'electrospray', 'entrees', 'erectus', 'etc', 'fiance', 'fiancee', 'fiter', 'fitir', 'fitr', 'fl', 'freediving', 'fukubukuro', 'geolinguist', 'hikikomori', 'hp', 'ibt', 'iq', 'iriver', 'jetta', 'jul', 'kabsa', 'kaled', 'kawader', 'kennywood', 'km', 'leisureville', 'll', 'maamool', 'mayumi', 'mcdonalds', 'min', 'mongongo', 'nc', 'neuro', 'nian', 'notting', 'okroshka', 'onsen', 'pajeon', 'pbt', 'pc', 'pcs', 'pp', 'pudim', 'puket', 'samear', 'shui', 'sq', 'st', 'staycation', 'sth', 'taoyuan', 'toefl', 'trans', 'transgene', 'tv', 'unsub', 'va', 'vol', 'vs', 'webaholic', 'webaholics', 'webaholism', 'wenjing', 'woong', 'yaoming', 'ying', 'yingdong', 'yugong', 'yuval', 'zi', 'abha', 'achuar', 'ae', 'afandi', 'aladha', 'alfater', 'alfeter', 'alfetr', 'alfter', 'alftr', 'ansan', 'apci', 'arial', 'ayumu', 'aziz', 'bartercard', 'bbc', 'bbq', 'beckham', 'bennigan', 'bmi', 'burkina', 'busan', 'camela', 'caral', 'ceos', 'cmu', 'cnn', 'cpu', 'cyworld', 'daegu', 'dammam', 'dc', 'dvd', 'dvds', 'eaid', 'esl', 'etat', 'ets', 'etsuko', 'eun', 'fargana', 'fda', 'federer', 'feng', 'frisbee', 'gaviotas', 'gmo', 'gpa', 'gunsan', 'hadza', 'hanshin', 'hd', 'hofstra', 'hsk', 'hyun', 'iaad', 'ielts', 'incheon', 'jang', 'japchae', 'jazan', 'jeddah', 'jeju', 'jiri', 'jong', 'jr', 'junine', 'kabsah', 'kakike', 'kanye', 'kimbab', 'kmt', 'koreas', 'krabi', 'ksa', 'kyung', 'lcd', 'lelan', 'lenovo', 'lg', 'lippman', 'longman', 'matsui', 'meltzer', 'mishori', 'mlb', 'morakot', 'mp', 'msn', 'mt', 'myung', 'nadal', 'najran', 'namwon', 'nano', 'nba', 'neimark', 'ngondo', 'noha', 'noor', 'nouwen', 'nyc', 'obon', 'occitane', 'oishi', 'paterno', 'pattak', 'pattee', 'pausini', 'pnc', 'qdoba', 'ritu', 'roh', 'ronaldo', 'rsa', 'sakarya', 'saletan', 'saudia', 'sawiris', 'sc', 'sep', 'sf', 'southside', 'sujin', 'tgif', 'toeic', 'tvs', 'upmc', 'usd', 'usda', 'vaio', 'wto', 'xbox', 'yokoso', 'yut', 'zawia', 'zhou', 'zong']\n"
     ]
    }
   ],
   "source": [
    "scowl_supp = open(\"SCOWL_supp.txt\", \"r\").read().split(',')\n",
    "scowl_supp = [x[2:-1] for x in scowl_supp]\n",
    "print(len(scowl_supp))\n",
    "print(scowl_supp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py:5303: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "# Lower case all toks\n",
    "\n",
    "non_words.tok_lem_POS = non_words.tok_lem_POS.apply(lambda row: [(x[0].lower(),x[1],x[2]) for x in row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find non-words\n",
    "\n",
    "def spell_check(tok_lem_POS_list):\n",
    "    word_list = scowl # Choose word_list here. Default is scowl described above.\n",
    "    not_in_word_list = []\n",
    "    for tok_lem_POS in tok_lem_POS_list:\n",
    "        if tok_lem_POS[0] not in word_list and tok_lem_POS[0] not in scowl_supp:\n",
    "            not_in_word_list.append(tok_lem_POS)\n",
    "    return not_in_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# Apply spell check function to find all misspelled-words. \n",
    "\n",
    "non_words['misspelled_words'] = non_words.tok_lem_POS.apply(spell_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tok_lem_POS</th>\n",
       "      <th>misspelled_words</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I met my friend Nife while I was studying in a...</td>\n",
       "      <td>[(i, i, PRP), (met, meet, VBD), (my, my, PRP$)...</td>\n",
       "      <td>[(., ., .), (., ., .), (., ., .), (;, ;, :), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ten years ago, I met a women on the train betw...</td>\n",
       "      <td>[(ten, ten, CD), (years, year, NNS), (ago, ago...</td>\n",
       "      <td>[(,, ,, ,), (,, ,, ,), (., ., .), (;, ;, :), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In my country we usually don't use tea bags. F...</td>\n",
       "      <td>[(in, in, IN), (my, my, PRP$), (country, count...</td>\n",
       "      <td>[(., ., .), (,, ,, ,), (., ., .), (., ., .), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I organized the instructions by time.</td>\n",
       "      <td>[(i, i, PRP), (organized, organize, VBD), (the...</td>\n",
       "      <td>[(., ., .)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>First, prepare a port, loose tea, and cup.\\nSe...</td>\n",
       "      <td>[(first, first, RB), (,, ,, ,), (prepare, prep...</td>\n",
       "      <td>[(,, ,, ,), (,, ,, ,), (,, ,, ,), (., ., .), (...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        text  \\\n",
       "answer_id                                                      \n",
       "1          I met my friend Nife while I was studying in a...   \n",
       "2          Ten years ago, I met a women on the train betw...   \n",
       "3          In my country we usually don't use tea bags. F...   \n",
       "4                      I organized the instructions by time.   \n",
       "5          First, prepare a port, loose tea, and cup.\\nSe...   \n",
       "\n",
       "                                                 tok_lem_POS  \\\n",
       "answer_id                                                      \n",
       "1          [(i, i, PRP), (met, meet, VBD), (my, my, PRP$)...   \n",
       "2          [(ten, ten, CD), (years, year, NNS), (ago, ago...   \n",
       "3          [(in, in, IN), (my, my, PRP$), (country, count...   \n",
       "4          [(i, i, PRP), (organized, organize, VBD), (the...   \n",
       "5          [(first, first, RB), (,, ,, ,), (prepare, prep...   \n",
       "\n",
       "                                            misspelled_words  \n",
       "answer_id                                                     \n",
       "1          [(., ., .), (., ., .), (., ., .), (;, ;, :), (...  \n",
       "2          [(,, ,, ,), (,, ,, ,), (., ., .), (;, ;, :), (...  \n",
       "3          [(., ., .), (,, ,, ,), (., ., .), (., ., .), (...  \n",
       "4                                                [(., ., .)]  \n",
       "5          [(,, ,, ,), (,, ,, ,), (,, ,, ,), (., ., .), (...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_words.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding context to the dataframe\n",
    "Seeing the mistakes in the context of a sentence will allow for better manual checking if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# Sent-tokenizing the text\n",
    "\n",
    "non_words['sentence'] = non_words['text'].apply(lambda x: nltk.sent_tokenize(x))\n",
    "\n",
    "# And delete text column which is no longer needed\n",
    "\n",
    "del non_words['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1182\n",
      "['city-state', 'well-rounded', 'standard-setting', 'trade-related', 'blood-curdling', 'hunter-gatherer', 'first-born', 'self-efficacy', 'cul-de-sac', 'self-expression']\n"
     ]
    }
   ],
   "source": [
    "# Checking for hyphenated words tagged as misspellings because SCOWL doesn't contain hypenated words\n",
    "\n",
    "hyphenated = set([x[0] for x in [x for y in non_words.misspelled_words.to_list() for x in y] if '-' in x[0]])\n",
    "print(len(hyphenated))\n",
    "print(list(hyphenated)[:10])\n",
    "\n",
    "# These need to be removed from the non-words dataframe if composed of valid words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['', \"'\"],\n",
       " ['', '***', '****'],\n",
       " ['', '+'],\n",
       " ['', '.'],\n",
       " ['',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  ''],\n",
       " [\"'\", ''],\n",
       " ['.', ''],\n",
       " ['/', ''],\n",
       " ['\\\\\\\\', ''],\n",
       " ['^', '^'],\n",
       " ['al', 'qaida'],\n",
       " ['austro', 'hungarian'],\n",
       " ['cd', 'rom'],\n",
       " ['co', 'authored'],\n",
       " ['co', 'ed'],\n",
       " ['co', 'educational'],\n",
       " ['co', 'exist'],\n",
       " ['co', 'existence'],\n",
       " ['co', 'founded'],\n",
       " ['co', 'founder'],\n",
       " ['co', 'founders'],\n",
       " ['co', 'host'],\n",
       " ['co', 'op'],\n",
       " ['co', 'operate'],\n",
       " ['co', 'operation'],\n",
       " ['co', 'pay'],\n",
       " ['co', 'pilot'],\n",
       " ['co', 'sleeping'],\n",
       " ['co', 'star'],\n",
       " ['co', 'worker'],\n",
       " ['co', 'workers'],\n",
       " ['co', 'written'],\n",
       " ['co', 'wrote'],\n",
       " ['mah', 'jong'],\n",
       " ['mid', '80s'],\n",
       " ['pay', 'tv'],\n",
       " ['roly', 'poly'],\n",
       " ['socio', 'cultural'],\n",
       " ['socio', 'economic'],\n",
       " ['trans', 'fat'],\n",
       " ['vis', 'a', 'vis'],\n",
       " ['wal', 'mart']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyphenated items whose components are not in scowl - possible misspellings or punctuation strings\n",
    "\n",
    "sorted([y for y in [x.split('-') for x in hyphenated] if y[0] not in scowl or y[1] not in scowl])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After manual checking, all the hypenated words are punctuation, real words (or true productive use of affixes) and can be removed from the non-words df.\n",
    "\n",
    "The following two cells \n",
    "1. remove all the hypenated words from the dataframe\n",
    "2. remove all words that don't contain a letter\n",
    "\n",
    "However, as all hyphenated word are fine, we will instead just eliminate all words that are not purely composed of letters. This will have the effect of removing the following categories from the dataframe:\n",
    "- punctuation\n",
    "- hyphenated words (e.g. well-known)\n",
    "- contractions (e.g. 'll, 've)\n",
    "- years (e.g. 1950s)\n",
    "- ordinals (e.g. 1st, 2nd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tangent\n",
    "The next three cells use the above information to create a more complete `PELIC-SCOWL.txt` wordlist for use with PELIC data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Creating a text file of hyphenated list for use elsewhere in creating an PELIC-SCOWL wordlist\n",
    "\n",
    "hyphenated = {word for word in hyphenated if any(x.isalpha() for x in word)}\n",
    "with open('hyphens.txt', 'w') as f:\n",
    "    for item in hyphenated:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Creating a text file of contractions for use elsewhere in creating an PELIC-SCOWL wordlist\n",
    "\n",
    "contractions = {\"'ll\",\"'ve\",\"n't\",\"'m\",\"'s\",\"'d\",\"'re\",\"'ve\"}\n",
    "with open('contractions.txt', 'w') as f:\n",
    "    for item in contractions:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Combining SCOWL_condensed, hyphenated, and contraction lists\n",
    "\n",
    "pelic_scowl = scowl|hyphenated|contractions\n",
    "pelic_scowl.remove('')\n",
    "pelic_scowl = sorted(list(pelic_scowl))\n",
    "\n",
    "with open('PELIC-SCOWL.txt', 'w') as f:\n",
    "    for item in pelic_scowl:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing hypenated words\n",
    "\n",
    "# non_words.misspelled_words = non_words.misspelled_words.apply(lambda row: [x for x in row if x[0] not in hyphenated])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing items that are only numbers or punctuation\n",
    "# .isalpha() cannot be used without 'any' as this also removes hyphenated words\n",
    "\n",
    "# non_words.misspelled_words = non_words.misspelled_words.apply(lambda row: [x for x in row if any(y.isalpha() for y in x[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680898\n",
      "20385\n"
     ]
    }
   ],
   "source": [
    "# Checking initial length of non_words list\n",
    "\n",
    "print(len([x for y in non_words.misspelled_words.to_list() for x in y]))\n",
    "print(len(set([x for y in non_words.misspelled_words.to_list() for x in y])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py:5303: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "# Removing items that are not purely alpha\n",
    "\n",
    "non_words.misspelled_words = non_words.misspelled_words.apply(lambda row: [x for x in row if x[0].isalpha()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26650\n",
      "15779\n"
     ]
    }
   ],
   "source": [
    "# Checking affect of removal\n",
    "\n",
    "print(len([x for y in non_words.misspelled_words.to_list() for x in y]))\n",
    "print(len(set([x for y in non_words.misspelled_words.to_list() for x in y])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing proper names - NNP, NNPS\n",
    "\n",
    "# non_words.misspelled_words = non_words.misspelled_words.apply(lambda row: [x for x in row if x[2] != 'NNP' and x[1] != 'NNPS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After manual checking, it was decided to keep in items tagged as NNP and NNPS as some items were in fact mistagged and were general capitalized nouns (NN) which were misspelled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26650\n",
      "15779\n"
     ]
    }
   ],
   "source": [
    "# Checking affect of removal\n",
    "\n",
    "print(len([x for y in non_words.misspelled_words.to_list() for x in y]))\n",
    "print(len(set([x for y in non_words.misspelled_words.to_list() for x in y])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing all words with length 1\n",
    "\n",
    "non_words.misspelled_words = non_words.misspelled_words.apply(lambda row: [x for x in row if len(x[0]) > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26638\n",
      "15770\n"
     ]
    }
   ],
   "source": [
    "# Checking affect of removal\n",
    "\n",
    "print(len([x for y in non_words.misspelled_words.to_list() for x in y]))\n",
    "print(len(set([x for y in non_words.misspelled_words.to_list() for x in y])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing all words with special characters (non_ascii)\n",
    "# after checking, these are all foreign words with accents and other non-latin characters\n",
    "\n",
    "# Creating function to check\n",
    "def is_ascii(s):\n",
    "    return all(ord(c) < 128 for c in s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('çark', 'çark', 'JJ'),\n",
       " ('currículm', 'currículm', 'NN'),\n",
       " ('beatricé', 'beatricé', 'NNP'),\n",
       " ('óæçá', 'óæçá', 'NNP'),\n",
       " ('çáãçäóé', 'çáãçäóé', 'NNP'),\n",
       " ('êíáýæäì', 'êíáýæäì', 'NNP'),\n",
       " ('çáíçá', 'çáíçá', 'NNP'),\n",
       " ('arára', 'arára', 'NNP'),\n",
       " ('çáôíä', 'çáôíä', 'NNP'),\n",
       " ('çááçòãé', 'çááçòãé', 'NNP'),\n",
       " ('ôûáåã', 'ôûáåã', 'NNP'),\n",
       " ('ãú', 'ãú', 'NNP'),\n",
       " ('ýçääì', 'ýçääì', 'NNP'),\n",
       " ('renée', 'renée', 'NNP'),\n",
       " ('bohème', 'bohème', 'NN'),\n",
       " ('bülent', 'bülent', 'NNP'),\n",
       " ('ßçä', 'ßçä', 'NNP'),\n",
       " ('opéra', 'opéra', 'NNP'),\n",
       " ('inácio', 'inácio', 'NNP'),\n",
       " ('béla', 'béla', 'NNP')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_word_list = set([x for y in non_words.misspelled_words.to_list() for x in y])\n",
    "foreign_words = [x for x in non_word_list if is_ascii(x[0]) == False ]\n",
    "foreign_words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing foreign words\n",
    "\n",
    "non_words.misspelled_words = non_words.misspelled_words.apply(lambda row: [x for x in row if x not in foreign_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26315\n",
      "15531\n"
     ]
    }
   ],
   "source": [
    "# Checking affect of removal\n",
    "\n",
    "print(len([x for y in non_words.misspelled_words.to_list() for x in y]))\n",
    "print(len(set([x for y in non_words.misspelled_words.to_list() for x in y])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tok_lem_POS</th>\n",
       "      <th>misspelled_words</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[(i, i, PRP), (met, meet, VBD), (my, my, PRP$)...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[I met my friend Nife while I was studying in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[(ten, ten, CD), (years, year, NNS), (ago, ago...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Ten years ago, I met a women on the train bet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[(in, in, IN), (my, my, PRP$), (country, count...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[In my country we usually don't use tea bags.,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[(i, i, PRP), (organized, organize, VBD), (the...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[I organized the instructions by time.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[(first, first, RB), (,, ,, ,), (prepare, prep...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[First, prepare a port, loose tea, and cup., S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tok_lem_POS misspelled_words  \\\n",
       "answer_id                                                                       \n",
       "1          [(i, i, PRP), (met, meet, VBD), (my, my, PRP$)...               []   \n",
       "2          [(ten, ten, CD), (years, year, NNS), (ago, ago...               []   \n",
       "3          [(in, in, IN), (my, my, PRP$), (country, count...               []   \n",
       "4          [(i, i, PRP), (organized, organize, VBD), (the...               []   \n",
       "5          [(first, first, RB), (,, ,, ,), (prepare, prep...               []   \n",
       "\n",
       "                                                    sentence  \n",
       "answer_id                                                     \n",
       "1          [I met my friend Nife while I was studying in ...  \n",
       "2          [Ten years ago, I met a women on the train bet...  \n",
       "3          [In my country we usually don't use tea bags.,...  \n",
       "4                    [I organized the instructions by time.]  \n",
       "5          [First, prepare a port, loose tea, and cup., S...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_words.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new dataframe so that each misspelling token is a separate row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing rows with no misspellings\n",
    "\n",
    "non_words2 = non_words.loc[non_words.misspelled_words.str.len() > 0,:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploding the lists in misspelled words so that each misspelling gets its own row\n",
    "\n",
    "non_words2 = non_words2.explode('misspelled_words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping only the sentence containing the error (the first occurence of the error if repeated)\n",
    "\n",
    "non_words2['sentence'] = list(zip([x[0] for x in non_words2.misspelled_words], non_words2.sentence))\n",
    "non_words2['sentence'] = non_words2['sentence'].apply(\n",
    "    lambda row: [i for i in row[1] if row[0] in lex.re_tokenize(i) or row[0]+\"n't\" in i.lower()])\n",
    "non_words2['sentence'] = [x[0] for x in non_words2['sentence']]\n",
    "non_words2 = non_words2.drop_duplicates(subset = ['misspelled_words','sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer_id</th>\n",
       "      <th>tok_lem_POS</th>\n",
       "      <th>misspelled_words</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>[(i, i, PRP), (organized, organize, VBD), (the...</td>\n",
       "      <td>(beacause, beacause, NN)</td>\n",
       "      <td>I organized the instructions by time, beacause...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>[(to, to, TO), (make, make, VB), (tea, tea, NN...</td>\n",
       "      <td>(wallmart, wallmart, NN)</td>\n",
       "      <td>next, you need to buy a box of tea in wallmart...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>[(first, first, RB), (,, ,, ,), (you, you, PRP...</td>\n",
       "      <td>(dovn, dovn, NN)</td>\n",
       "      <td>First, you should take some hot water, you can...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>[(first, first, RB), (,, ,, ,), (you, you, PRP...</td>\n",
       "      <td>(mircowave, mircowave, VBP)</td>\n",
       "      <td>First, you should take some hot water, you can...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>[(in, in, IN), (my, my, PRP$), (country, count...</td>\n",
       "      <td>(fitst, fitst, NNP)</td>\n",
       "      <td>Fitst, boil a water in a pot.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   answer_id                                        tok_lem_POS  \\\n",
       "0          8  [(i, i, PRP), (organized, organize, VBD), (the...   \n",
       "1         11  [(to, to, TO), (make, make, VB), (tea, tea, NN...   \n",
       "2         13  [(first, first, RB), (,, ,, ,), (you, you, PRP...   \n",
       "3         13  [(first, first, RB), (,, ,, ,), (you, you, PRP...   \n",
       "4         15  [(in, in, IN), (my, my, PRP$), (country, count...   \n",
       "\n",
       "              misspelled_words  \\\n",
       "0     (beacause, beacause, NN)   \n",
       "1     (wallmart, wallmart, NN)   \n",
       "2             (dovn, dovn, NN)   \n",
       "3  (mircowave, mircowave, VBP)   \n",
       "4          (fitst, fitst, NNP)   \n",
       "\n",
       "                                            sentence  \n",
       "0  I organized the instructions by time, beacause...  \n",
       "1  next, you need to buy a box of tea in wallmart...  \n",
       "2  First, you should take some hot water, you can...  \n",
       "3  First, you should take some hot water, you can...  \n",
       "4                      Fitst, boil a water in a pot.  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keeping the answer_id (which is no longer unique) as a separate column\n",
    "\n",
    "non_words2 = non_words2.reset_index(drop = False)\n",
    "non_words2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding a bigrams column, i.e. one token left and right of the misspelled word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer_id</th>\n",
       "      <th>tok_lem_POS</th>\n",
       "      <th>misspelled_words</th>\n",
       "      <th>sentence</th>\n",
       "      <th>enumerated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>[(i, i, PRP), (organized, organize, VBD), (the...</td>\n",
       "      <td>(beacause, beacause, NN)</td>\n",
       "      <td>I organized the instructions by time, beacause...</td>\n",
       "      <td>[(0, i), (1, organized), (2, the), (3, instruc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>[(to, to, TO), (make, make, VB), (tea, tea, NN...</td>\n",
       "      <td>(wallmart, wallmart, NN)</td>\n",
       "      <td>next, you need to buy a box of tea in wallmart...</td>\n",
       "      <td>[(0, next), (1, you), (2, need), (3, to), (4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>[(first, first, RB), (,, ,, ,), (you, you, PRP...</td>\n",
       "      <td>(dovn, dovn, NN)</td>\n",
       "      <td>First, you should take some hot water, you can...</td>\n",
       "      <td>[(0, first), (1, you), (2, should), (3, take),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>[(first, first, RB), (,, ,, ,), (you, you, PRP...</td>\n",
       "      <td>(mircowave, mircowave, VBP)</td>\n",
       "      <td>First, you should take some hot water, you can...</td>\n",
       "      <td>[(0, first), (1, you), (2, should), (3, take),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>[(in, in, IN), (my, my, PRP$), (country, count...</td>\n",
       "      <td>(fitst, fitst, NNP)</td>\n",
       "      <td>Fitst, boil a water in a pot.</td>\n",
       "      <td>[(0, fitst), (1, boil), (2, a), (3, water), (4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   answer_id                                        tok_lem_POS  \\\n",
       "0          8  [(i, i, PRP), (organized, organize, VBD), (the...   \n",
       "1         11  [(to, to, TO), (make, make, VB), (tea, tea, NN...   \n",
       "2         13  [(first, first, RB), (,, ,, ,), (you, you, PRP...   \n",
       "3         13  [(first, first, RB), (,, ,, ,), (you, you, PRP...   \n",
       "4         15  [(in, in, IN), (my, my, PRP$), (country, count...   \n",
       "\n",
       "              misspelled_words  \\\n",
       "0     (beacause, beacause, NN)   \n",
       "1     (wallmart, wallmart, NN)   \n",
       "2             (dovn, dovn, NN)   \n",
       "3  (mircowave, mircowave, VBP)   \n",
       "4          (fitst, fitst, NNP)   \n",
       "\n",
       "                                            sentence  \\\n",
       "0  I organized the instructions by time, beacause...   \n",
       "1  next, you need to buy a box of tea in wallmart...   \n",
       "2  First, you should take some hot water, you can...   \n",
       "3  First, you should take some hot water, you can...   \n",
       "4                      Fitst, boil a water in a pot.   \n",
       "\n",
       "                                          enumerated  \n",
       "0  [(0, i), (1, organized), (2, the), (3, instruc...  \n",
       "1  [(0, next), (1, you), (2, need), (3, to), (4, ...  \n",
       "2  [(0, first), (1, you), (2, should), (3, take),...  \n",
       "3  [(0, first), (1, you), (2, should), (3, take),...  \n",
       "4  [(0, fitst), (1, boil), (2, a), (3, water), (4...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a tokenized version of the sentence without punctuation and with the index for each token\n",
    "\n",
    "non_words2['enumerated'] = non_words2.sentence.apply(lambda x: lex.re_tokenize(x)).apply(enumerate).apply(list)\n",
    "non_words2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to extract the bigrams (1 word either side of misspelling)\n",
    "\n",
    "def get_bigrams(misspelled_word, enumerated_list):\n",
    "    if len(enumerated_list) <2:\n",
    "        return []\n",
    "    for tup in enumerated_list:\n",
    "        if tup[1] == misspelled_word[0]:\n",
    "            if tup[0] == 0:\n",
    "                bigram = ' '.join([x[1] for x in (enumerated_list[tup[0]],enumerated_list[tup[0]+1])])\n",
    "                return [bigram]\n",
    "            if tup[0] == len(enumerated_list)-1:\n",
    "                bigram = ' '.join([x[1] for x in (enumerated_list[tup[0]-1],enumerated_list[tup[0]])])\n",
    "                return [bigram]\n",
    "            else:\n",
    "                bigram1 = ' '.join([x[1] for x in (enumerated_list[tup[0]-1],enumerated_list[tup[0]])])\n",
    "                bigram2 = ' '.join([x[1] for x in (enumerated_list[tup[0]],enumerated_list[tup[0]+1])])\n",
    "                return [bigram1, bigram2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'every'), (1, 'paragragh'), (2, 's'), (3, 'instructions'), (4, 'depend'), (5, 'on'), (6, 'a'), (7, 'main'), (8, 'idea')]\n",
      "\n",
      " ('every', 'every', 'DT') ('depend', 'depend', 'VBP') ('idea', 'idea', 'NN')\n"
     ]
    }
   ],
   "source": [
    "# Testing the function\n",
    "\n",
    "test_list = non_words2.iloc[5,4]\n",
    "print(test_list)\n",
    "\n",
    "first_item = non_words2.iloc[5,1][0] # first item in list\n",
    "middle_item = non_words2.iloc[5,1][4] # item in in middle of list\n",
    "last_item = non_words2.iloc[5,1][8] # item at end of list\n",
    "print('\\n',first_item, middle_item, last_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['every paragragh']\n",
      "['instructions depend', 'depend on']\n",
      "['main idea']\n"
     ]
    }
   ],
   "source": [
    "print(get_bigrams(first_item,test_list)) # One possible bigram\n",
    "print(get_bigrams(middle_item,test_list)) # Two possible bigrams\n",
    "print(get_bigrams(last_item,test_list)) # One possible bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the above function\n",
    "\n",
    "non_words2['bigrams'] = non_words2[['misspelled_words','enumerated']].apply(lambda x: get_bigrams(x[0],x[1]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer_id</th>\n",
       "      <th>tok_lem_POS</th>\n",
       "      <th>misspelled_words</th>\n",
       "      <th>sentence</th>\n",
       "      <th>enumerated</th>\n",
       "      <th>bigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>909</th>\n",
       "      <td>1469</td>\n",
       "      <td>[(about, about, IN), (4, 4, CD), (days, day, N...</td>\n",
       "      <td>(shoudl, shoudl, VBP)</td>\n",
       "      <td>Anyway, we went to a hospital and the doctor s...</td>\n",
       "      <td>[(0, anyway), (1, we), (2, went), (3, to), (4,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>1797</td>\n",
       "      <td>[(i, i, PRP), (failed, fail, VBD), (the, the, ...</td>\n",
       "      <td>(idid, idid, NNP)</td>\n",
       "      <td>I failed the test because Ididn't study.</td>\n",
       "      <td>[(0, i), (1, failed), (2, the), (3, test), (4,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2446</th>\n",
       "      <td>4804</td>\n",
       "      <td>[(bill, bill, NNP), (shoul, shoul, VBP), (n't,...</td>\n",
       "      <td>(shoul, shoul, VBP)</td>\n",
       "      <td>Bill shouln't have lied about the price of the...</td>\n",
       "      <td>[(0, bill), (1, shouln), (2, t), (3, have), (4...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2449</th>\n",
       "      <td>4830</td>\n",
       "      <td>[(caroline, caroline, NNP), (must, must, MD), ...</td>\n",
       "      <td>(sould, sould, VBP)</td>\n",
       "      <td>Caroline souldn't have insulted Bill in front ...</td>\n",
       "      <td>[(0, caroline), (1, souldn), (2, t), (3, have)...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2725</th>\n",
       "      <td>6346</td>\n",
       "      <td>[(last, last, JJ), (year, year, NN), (,, ,, ,)...</td>\n",
       "      <td>(sould, sould, MD)</td>\n",
       "      <td>I told the officer that he caused the accident...</td>\n",
       "      <td>[(0, i), (1, told), (2, the), (3, officer), (4...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2728</th>\n",
       "      <td>6346</td>\n",
       "      <td>[(last, last, JJ), (year, year, NN), (,, ,, ,)...</td>\n",
       "      <td>(sould, sould, VBP)</td>\n",
       "      <td>I told the officer that he caused the accident...</td>\n",
       "      <td>[(0, i), (1, told), (2, the), (3, officer), (4...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3750</th>\n",
       "      <td>8640</td>\n",
       "      <td>[(before, before, IN), (i, i, PRP), (came, com...</td>\n",
       "      <td>(daoe, daoe, VBP)</td>\n",
       "      <td>I like to play soccer,but in Korea daoen't hav...</td>\n",
       "      <td>[(0, i), (1, like), (2, to), (3, play), (4, so...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4051</th>\n",
       "      <td>10001</td>\n",
       "      <td>[(you, you, PRP), (shoud, shoud, VBP), (n't, n...</td>\n",
       "      <td>(shoud, shoud, VBP)</td>\n",
       "      <td>You shoudn't</td>\n",
       "      <td>[(0, you), (1, shoudn), (2, t)]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10334</th>\n",
       "      <td>23939</td>\n",
       "      <td>[(i, i, PRP), (have, have, VBP), (an, a, DT), ...</td>\n",
       "      <td>(sould, sould, VBP)</td>\n",
       "      <td>I souldn't have gone to his wedding party.</td>\n",
       "      <td>[(0, i), (1, souldn), (2, t), (3, have), (4, g...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11393</th>\n",
       "      <td>26126</td>\n",
       "      <td>[(i, i, PRP), (like, like, VBP), (cooking, coo...</td>\n",
       "      <td>(doed, doed, VBP)</td>\n",
       "      <td>But In pittsburgh doedn't have good traditiona...</td>\n",
       "      <td>[(0, but), (1, in), (2, pittsburgh), (3, doedn...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11902</th>\n",
       "      <td>27409</td>\n",
       "      <td>[(on, on, IN), (saturday, saturday, NN), (,, ,...</td>\n",
       "      <td>(shoul, shoul, VBP)</td>\n",
       "      <td>I shouln't have gone Giant Eagle or I could ha...</td>\n",
       "      <td>[(0, i), (1, shouln), (2, t), (3, have), (4, g...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12068</th>\n",
       "      <td>27911</td>\n",
       "      <td>[(maui, maui, NNP), (is, be, VBZ), (the, the, ...</td>\n",
       "      <td>(counld, counld, VBP)</td>\n",
       "      <td>I counldn't forget the experience when I was i...</td>\n",
       "      <td>[(0, i), (1, counldn), (2, t), (3, forget), (4...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13442</th>\n",
       "      <td>30978</td>\n",
       "      <td>[(as, as, IN), (being, be, VBG), (a, a, DT), (...</td>\n",
       "      <td>(woould, woould, VBP)</td>\n",
       "      <td>All of them are native speakers, I was pretty ...</td>\n",
       "      <td>[(0, all), (1, of), (2, them), (3, are), (4, n...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16630</th>\n",
       "      <td>37339</td>\n",
       "      <td>[(you, you, PRP), (should, should, MD), (visit...</td>\n",
       "      <td>(shold, shold, VBP)</td>\n",
       "      <td>You sholdn't shake hands when you first meet s...</td>\n",
       "      <td>[(0, you), (1, sholdn), (2, t), (3, shake), (4...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       answer_id                                        tok_lem_POS  \\\n",
       "909         1469  [(about, about, IN), (4, 4, CD), (days, day, N...   \n",
       "1100        1797  [(i, i, PRP), (failed, fail, VBD), (the, the, ...   \n",
       "2446        4804  [(bill, bill, NNP), (shoul, shoul, VBP), (n't,...   \n",
       "2449        4830  [(caroline, caroline, NNP), (must, must, MD), ...   \n",
       "2725        6346  [(last, last, JJ), (year, year, NN), (,, ,, ,)...   \n",
       "2728        6346  [(last, last, JJ), (year, year, NN), (,, ,, ,)...   \n",
       "3750        8640  [(before, before, IN), (i, i, PRP), (came, com...   \n",
       "4051       10001  [(you, you, PRP), (shoud, shoud, VBP), (n't, n...   \n",
       "10334      23939  [(i, i, PRP), (have, have, VBP), (an, a, DT), ...   \n",
       "11393      26126  [(i, i, PRP), (like, like, VBP), (cooking, coo...   \n",
       "11902      27409  [(on, on, IN), (saturday, saturday, NN), (,, ,...   \n",
       "12068      27911  [(maui, maui, NNP), (is, be, VBZ), (the, the, ...   \n",
       "13442      30978  [(as, as, IN), (being, be, VBG), (a, a, DT), (...   \n",
       "16630      37339  [(you, you, PRP), (should, should, MD), (visit...   \n",
       "\n",
       "            misspelled_words  \\\n",
       "909    (shoudl, shoudl, VBP)   \n",
       "1100       (idid, idid, NNP)   \n",
       "2446     (shoul, shoul, VBP)   \n",
       "2449     (sould, sould, VBP)   \n",
       "2725      (sould, sould, MD)   \n",
       "2728     (sould, sould, VBP)   \n",
       "3750       (daoe, daoe, VBP)   \n",
       "4051     (shoud, shoud, VBP)   \n",
       "10334    (sould, sould, VBP)   \n",
       "11393      (doed, doed, VBP)   \n",
       "11902    (shoul, shoul, VBP)   \n",
       "12068  (counld, counld, VBP)   \n",
       "13442  (woould, woould, VBP)   \n",
       "16630    (shold, shold, VBP)   \n",
       "\n",
       "                                                sentence  \\\n",
       "909    Anyway, we went to a hospital and the doctor s...   \n",
       "1100            I failed the test because Ididn't study.   \n",
       "2446   Bill shouln't have lied about the price of the...   \n",
       "2449   Caroline souldn't have insulted Bill in front ...   \n",
       "2725   I told the officer that he caused the accident...   \n",
       "2728   I told the officer that he caused the accident...   \n",
       "3750   I like to play soccer,but in Korea daoen't hav...   \n",
       "4051                                        You shoudn't   \n",
       "10334         I souldn't have gone to his wedding party.   \n",
       "11393  But In pittsburgh doedn't have good traditiona...   \n",
       "11902  I shouln't have gone Giant Eagle or I could ha...   \n",
       "12068  I counldn't forget the experience when I was i...   \n",
       "13442  All of them are native speakers, I was pretty ...   \n",
       "16630  You sholdn't shake hands when you first meet s...   \n",
       "\n",
       "                                              enumerated bigrams  \n",
       "909    [(0, anyway), (1, we), (2, went), (3, to), (4,...    None  \n",
       "1100   [(0, i), (1, failed), (2, the), (3, test), (4,...    None  \n",
       "2446   [(0, bill), (1, shouln), (2, t), (3, have), (4...    None  \n",
       "2449   [(0, caroline), (1, souldn), (2, t), (3, have)...    None  \n",
       "2725   [(0, i), (1, told), (2, the), (3, officer), (4...    None  \n",
       "2728   [(0, i), (1, told), (2, the), (3, officer), (4...    None  \n",
       "3750   [(0, i), (1, like), (2, to), (3, play), (4, so...    None  \n",
       "4051                     [(0, you), (1, shoudn), (2, t)]    None  \n",
       "10334  [(0, i), (1, souldn), (2, t), (3, have), (4, g...    None  \n",
       "11393  [(0, but), (1, in), (2, pittsburgh), (3, doedn...    None  \n",
       "11902  [(0, i), (1, shouln), (2, t), (3, have), (4, g...    None  \n",
       "12068  [(0, i), (1, counldn), (2, t), (3, forget), (4...    None  \n",
       "13442  [(0, all), (1, of), (2, them), (3, are), (4, n...    None  \n",
       "16630  [(0, you), (1, sholdn), (2, t), (3, shake), (4...    None  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The 14 misspellings with contracted words did not return any suggestions as the n't is handled differently by \n",
    "# the re_tokenize function which strips punctuation and the nltk tokenizer used to create the tok_lem_POS column\n",
    "\n",
    "no_bigrams = non_words2.loc[(non_words2['bigrams'].isnull()) & (non_words2.enumerated.str.len() >1),:]\n",
    "print(len(no_bigrams))\n",
    "no_bigrams.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating function to fix these items\n",
    "\n",
    "misspelled_contractions = [x[0] for x in no_bigrams.misspelled_words]\n",
    "\n",
    "def fix_contractions(word):\n",
    "    if word == \"t\":\n",
    "        word = \"n't\"\n",
    "    if word[:-1] in misspelled_contractions:\n",
    "        word = word[:-1]\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the above function to the selected rows\n",
    "\n",
    "mask = non_words2.index.isin(no_bigrams.index)\n",
    "non_words2.loc[mask, 'enumerated'] = non_words2.loc[mask, 'enumerated'].apply(\n",
    "    lambda row: [(x[0],fix_contractions(x[1])) for x in row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reapplying the bigrams function\n",
    "\n",
    "non_words2['bigrams'] = non_words2[['misspelled_words','enumerated']].apply(lambda x: get_bigrams(x[0],x[1]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer_id</th>\n",
       "      <th>tok_lem_POS</th>\n",
       "      <th>misspelled_words</th>\n",
       "      <th>sentence</th>\n",
       "      <th>enumerated</th>\n",
       "      <th>bigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [answer_id, tok_lem_POS, misspelled_words, sentence, enumerated, bigrams]\n",
       "Index: []"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-checking that all rows have bigrams\n",
    "non_words2.loc[(non_words2['bigrams'].isnull()) & (non_words2.enumerated.str.len() >1),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting the enumerated column as no longer necessary\n",
    "\n",
    "del non_words2['enumerated']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the 'misspelled_words' column as there is only one word in each row\n",
    "\n",
    "non_words2 = non_words2.rename(columns={\"misspelled_words\": \"misspelling\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer_id</th>\n",
       "      <th>tok_lem_POS</th>\n",
       "      <th>misspelling</th>\n",
       "      <th>sentence</th>\n",
       "      <th>bigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>[(i, i, PRP), (organized, organize, VBD), (the...</td>\n",
       "      <td>(beacause, beacause, NN)</td>\n",
       "      <td>I organized the instructions by time, beacause...</td>\n",
       "      <td>[time beacause, beacause to]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>[(to, to, TO), (make, make, VB), (tea, tea, NN...</td>\n",
       "      <td>(wallmart, wallmart, NN)</td>\n",
       "      <td>next, you need to buy a box of tea in wallmart...</td>\n",
       "      <td>[in wallmart, wallmart or]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>[(first, first, RB), (,, ,, ,), (you, you, PRP...</td>\n",
       "      <td>(dovn, dovn, NN)</td>\n",
       "      <td>First, you should take some hot water, you can...</td>\n",
       "      <td>[use dovn, dovn mircowave]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>[(first, first, RB), (,, ,, ,), (you, you, PRP...</td>\n",
       "      <td>(mircowave, mircowave, VBP)</td>\n",
       "      <td>First, you should take some hot water, you can...</td>\n",
       "      <td>[dovn mircowave, mircowave or]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>[(in, in, IN), (my, my, PRP$), (country, count...</td>\n",
       "      <td>(fitst, fitst, NNP)</td>\n",
       "      <td>Fitst, boil a water in a pot.</td>\n",
       "      <td>[fitst boil]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   answer_id                                        tok_lem_POS  \\\n",
       "0          8  [(i, i, PRP), (organized, organize, VBD), (the...   \n",
       "1         11  [(to, to, TO), (make, make, VB), (tea, tea, NN...   \n",
       "2         13  [(first, first, RB), (,, ,, ,), (you, you, PRP...   \n",
       "3         13  [(first, first, RB), (,, ,, ,), (you, you, PRP...   \n",
       "4         15  [(in, in, IN), (my, my, PRP$), (country, count...   \n",
       "\n",
       "                   misspelling  \\\n",
       "0     (beacause, beacause, NN)   \n",
       "1     (wallmart, wallmart, NN)   \n",
       "2             (dovn, dovn, NN)   \n",
       "3  (mircowave, mircowave, VBP)   \n",
       "4          (fitst, fitst, NNP)   \n",
       "\n",
       "                                            sentence  \\\n",
       "0  I organized the instructions by time, beacause...   \n",
       "1  next, you need to buy a box of tea in wallmart...   \n",
       "2  First, you should take some hot water, you can...   \n",
       "3  First, you should take some hot water, you can...   \n",
       "4                      Fitst, boil a water in a pot.   \n",
       "\n",
       "                          bigrams  \n",
       "0    [time beacause, beacause to]  \n",
       "1      [in wallmart, wallmart or]  \n",
       "2      [use dovn, dovn mircowave]  \n",
       "3  [dovn mircowave, mircowave or]  \n",
       "4                    [fitst boil]  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking final non_words2 dataframe\n",
    "\n",
    "non_words2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21068\n",
      "15531\n"
     ]
    }
   ],
   "source": [
    "# Total number of non-words (tokens)\n",
    "print(len(non_words2))\n",
    "\n",
    "# Total number of non-words (types)\n",
    "print(non_words2.misspelling.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a dataframe of misspellings\n",
    "In the `non-words2` dataframe above, each row is an occurrence of a misspelling (i.e. _tokens_ ). We also want a dataframe where each row is a misspelling _type_ with frequency information attached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gathering the total misspellings and bigrams\n",
    "\n",
    "total_unigram_misspell = [x for x in non_words2['misspelling']]\n",
    "total_bigram_misspell = [x for y in non_words2['bigrams'] for x in y] #flattened list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('beacause', 'beacause', 'NN'), ('wallmart', 'wallmart', 'NN'), ('dovn', 'dovn', 'NN'), ('mircowave', 'mircowave', 'VBP'), ('fitst', 'fitst', 'NNP')]\n",
      "['time beacause', 'beacause to', 'in wallmart', 'wallmart or', 'use dovn']\n"
     ]
    }
   ],
   "source": [
    "print(total_unigram_misspell[:5])\n",
    "print(total_bigram_misspell[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating frequency dictionaries for unigrams and bigrams\n",
    "\n",
    "unigram_misspell_freq_dict = {}\n",
    "for word in total_unigram_misspell:\n",
    "    if word not in unigram_misspell_freq_dict:\n",
    "        unigram_misspell_freq_dict[word] = 1\n",
    "    else:\n",
    "        unigram_misspell_freq_dict[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_misspell_freq_dict = {}\n",
    "for bigram in total_bigram_misspell:\n",
    "    if bigram not in bigram_misspell_freq_dict:\n",
    "        bigram_misspell_freq_dict[bigram] = 1\n",
    "    else:\n",
    "        bigram_misspell_freq_dict[bigram] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('attitudt', 'attitudt', 'NN'), ('marrid', 'marrid', 'JJ'), ('littel', 'littel', 'NN'), ('hfcs', 'hfcs', 'NNP'), ('prefre', 'prefre', 'VBP')]\n",
      "['ramin mostaghim', 'cuase of', 't understnad', 'so fiday', 'engeler heart']\n"
     ]
    }
   ],
   "source": [
    "# Checking dictionaries\n",
    "\n",
    "print(random.sample(list(unigram_misspell_freq_dict),5))\n",
    "print(random.sample(list(bigram_misspell_freq_dict),5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15531\n",
      "31915\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates\n",
    "\n",
    "final_unigram_misspellings = sorted(list(set(total_unigram_misspell)))\n",
    "final_bigram_misspellings = sorted(list(set(total_bigram_misspell)))\n",
    "print(len(final_unigram_misspellings))\n",
    "print(len(final_bigram_misspellings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aa</td>\n",
       "      <td>aa</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aa</td>\n",
       "      <td>aa</td>\n",
       "      <td>VB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aabout</td>\n",
       "      <td>aabout</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aad</td>\n",
       "      <td>aad</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aain</td>\n",
       "      <td>aain</td>\n",
       "      <td>VBP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0       1    2\n",
       "0      aa      aa  NNP\n",
       "1      aa      aa   VB\n",
       "2  aabout  aabout   IN\n",
       "3     aad     aad   JJ\n",
       "4    aain    aain  VBP"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Constructing misspell_df\n",
    "\n",
    "misspell_df = pd.DataFrame(final_unigram_misspellings)\n",
    "misspell_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming columns to match other DataFrames in this notebook\n",
    "\n",
    "misspell_df.rename(columns = {0: 'misspelling',1:'lemma',2:'POS'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>misspelling</th>\n",
       "      <th>lemma</th>\n",
       "      <th>POS</th>\n",
       "      <th>tok_lem_POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aa</td>\n",
       "      <td>aa</td>\n",
       "      <td>NNP</td>\n",
       "      <td>(aa, aa, NNP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aa</td>\n",
       "      <td>aa</td>\n",
       "      <td>VB</td>\n",
       "      <td>(aa, aa, VB)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aabout</td>\n",
       "      <td>aabout</td>\n",
       "      <td>IN</td>\n",
       "      <td>(aabout, aabout, IN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aad</td>\n",
       "      <td>aad</td>\n",
       "      <td>JJ</td>\n",
       "      <td>(aad, aad, JJ)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aain</td>\n",
       "      <td>aain</td>\n",
       "      <td>VBP</td>\n",
       "      <td>(aain, aain, VBP)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  misspelling   lemma  POS           tok_lem_POS\n",
       "0          aa      aa  NNP         (aa, aa, NNP)\n",
       "1          aa      aa   VB          (aa, aa, VB)\n",
       "2      aabout  aabout   IN  (aabout, aabout, IN)\n",
       "3         aad     aad   JJ        (aad, aad, JJ)\n",
       "4        aain    aain  VBP     (aain, aain, VBP)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recreating tok_lem_POS column to match dictionary\n",
    "\n",
    "misspell_df['tok_lem_POS'] = list(zip(misspell_df.misspelling, misspell_df.lemma, misspell_df.POS))\n",
    "misspell_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping dictionary to DataFrame\n",
    "\n",
    "misspell_df['freq'] = misspell_df['tok_lem_POS'].map(unigram_misspell_freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting by frequency\n",
    "\n",
    "misspell_df = misspell_df.sort_values(by=['freq'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>misspelling</th>\n",
       "      <th>tok_lem_POS</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alot</td>\n",
       "      <td>(alot, alot, NN)</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>studing</td>\n",
       "      <td>(studing, studing, VBG)</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tofel</td>\n",
       "      <td>(tofel, tofel, NNP)</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>goverment</td>\n",
       "      <td>(goverment, goverment, NN)</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>iam</td>\n",
       "      <td>(iam, iam, NNP)</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>finaly</td>\n",
       "      <td>(finaly, finaly, NNP)</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>beatiful</td>\n",
       "      <td>(beatiful, beatiful, JJ)</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>becuase</td>\n",
       "      <td>(becuase, becuase, NN)</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>nickell</td>\n",
       "      <td>(nickell, nickell, NNP)</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>oss</td>\n",
       "      <td>(oss, oss, NNP)</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>becouse</td>\n",
       "      <td>(becouse, becouse, IN)</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>differents</td>\n",
       "      <td>(differents, differents, NNS)</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>shoud</td>\n",
       "      <td>(shoud, shoud, VBP)</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>okland</td>\n",
       "      <td>(okland, okland, NNP)</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>befor</td>\n",
       "      <td>(befor, befor, NN)</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>somthing</td>\n",
       "      <td>(somthing, somthing, VBG)</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>eilperin</td>\n",
       "      <td>(eilperin, eilperin, NNP)</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>resturant</td>\n",
       "      <td>(resturant, resturant, NN)</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>sould</td>\n",
       "      <td>(sould, sould, VBP)</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>grammer</td>\n",
       "      <td>(grammer, grammer, NN)</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   misspelling                    tok_lem_POS  freq\n",
       "0         alot               (alot, alot, NN)   103\n",
       "1      studing        (studing, studing, VBG)    62\n",
       "2        tofel            (tofel, tofel, NNP)    39\n",
       "3    goverment     (goverment, goverment, NN)    36\n",
       "4          iam                (iam, iam, NNP)    28\n",
       "5       finaly          (finaly, finaly, NNP)    26\n",
       "6     beatiful       (beatiful, beatiful, JJ)    24\n",
       "7      becuase         (becuase, becuase, NN)    24\n",
       "8      nickell        (nickell, nickell, NNP)    22\n",
       "9          oss                (oss, oss, NNP)    22\n",
       "10     becouse         (becouse, becouse, IN)    21\n",
       "11  differents  (differents, differents, NNS)    21\n",
       "12       shoud            (shoud, shoud, VBP)    21\n",
       "13      okland          (okland, okland, NNP)    20\n",
       "14       befor             (befor, befor, NN)    20\n",
       "15    somthing      (somthing, somthing, VBG)    20\n",
       "16    eilperin      (eilperin, eilperin, NNP)    20\n",
       "17   resturant     (resturant, resturant, NN)    18\n",
       "18       sould            (sould, sould, VBP)    18\n",
       "19     grammer         (grammer, grammer, NN)    18"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Resetting index and deleting unnecesary columns\n",
    "\n",
    "misspell_df = misspell_df.reset_index(drop = True)\n",
    "del misspell_df['lemma']\n",
    "del misspell_df['POS']\n",
    "\n",
    "misspell_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scowl_supp\n",
    "The following is the basis for the 'scowl_supp' list used earlier. Here, errors with a frequency of 10 or more were manually checked, and if determined to be a real word, were added to the scowl_supp list. There were originally 267 items which met this criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>misspelling</th>\n",
       "      <th>tok_lem_POS</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alot</td>\n",
       "      <td>(alot, alot, NN)</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>studing</td>\n",
       "      <td>(studing, studing, VBG)</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tofel</td>\n",
       "      <td>(tofel, tofel, NNP)</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>goverment</td>\n",
       "      <td>(goverment, goverment, NN)</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>iam</td>\n",
       "      <td>(iam, iam, NNP)</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>childrens</td>\n",
       "      <td>(childrens, child, NNS)</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>befor</td>\n",
       "      <td>(befor, befor, IN)</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>differnt</td>\n",
       "      <td>(differnt, differnt, JJ)</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>eatting</td>\n",
       "      <td>(eatting, eatting, VBG)</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>stangor</td>\n",
       "      <td>(stangor, stangor, NNP)</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   misspelling                 tok_lem_POS  freq\n",
       "0         alot            (alot, alot, NN)   103\n",
       "1      studing     (studing, studing, VBG)    62\n",
       "2        tofel         (tofel, tofel, NNP)    39\n",
       "3    goverment  (goverment, goverment, NN)    36\n",
       "4          iam             (iam, iam, NNP)    28\n",
       "..         ...                         ...   ...\n",
       "58   childrens     (childrens, child, NNS)    10\n",
       "59       befor          (befor, befor, IN)    10\n",
       "60    differnt    (differnt, differnt, JJ)    10\n",
       "61     eatting     (eatting, eatting, VBG)    10\n",
       "62     stangor     (stangor, stangor, NNP)    10\n",
       "\n",
       "[63 rows x 3 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(misspell_df.loc[misspell_df.freq >= 10]))\n",
    "misspell_df.loc[misspell_df.freq >= 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible segmentation\n",
    "\n",
    "Selected segmenter and spellchecker: SymSpell https://github.com/mammothb/symspellpy\n",
    "\n",
    "There is a dictionary file which which needs to be installed (saved to repo):\n",
    "[frequency_dictionary_en_82_765.txt](https://symspellpy.readthedocs.io/en/latest/users/installing.html)\n",
    "\n",
    "To install symspellpy the first time, use pip in command line: `pip install -U symspellpy`\n",
    "\n",
    "Prior to spelling correct, we first consider using the segmenter. This is a potentially useful first step as misspellings like 'alot' or 'dogmeat' will be separated into 'a lot' and 'dog meat' rather than corrected to a single word like 'lot'.  \n",
    "\n",
    "However, when segementing misspellings, the segmenter over performs, segmenting non-words into real words where it was clearly not intended, e.g. _improtant_ into _imp rot ant_ or _befor_ into _be for_. As such, the segmenting will not be automated. \n",
    "\n",
    "Instead, we rely on the bigram spelling correction which correctly segments items like _alot, iam, everytime,_ etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 23135851162),\n",
       " ('of', 13151942776),\n",
       " ('and', 12997637966),\n",
       " ('to', 12136980858),\n",
       " ('a', 9081174698)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting up symspell\n",
    "\n",
    "from itertools import islice\n",
    "import pkg_resources\n",
    "from symspellpy import SymSpell\n",
    "from symspellpy import Verbosity\n",
    "sym_spell = SymSpell()\n",
    "dictionary_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "sym_spell.load_dictionary(dictionary_path, 0, 1)\n",
    "\n",
    "# Print out first 5 elements to demonstrate that dictionary is successfully loaded\n",
    "list(islice(sym_spell.words.items(), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing segmenter with 'alot' and 'dogmeat'\n",
    "\n",
    "# Set max_dictionary_edit_distance to avoid spelling correction\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=0, prefix_length=7)\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "\n",
    "# It is also possible to display frequency with result.distance_sum and edit distance with .log_prob_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating function for applying the above code\n",
    "\n",
    "def get_segments(word):\n",
    "    segments = sym_spell.word_segmentation(word)\n",
    "    if len(segments.corrected_string.split(' ')) > 1 \\\n",
    "    and segments.corrected_string.split(' ')[0] in scowl and segments.corrected_string.split(' ')[1] in scowl:\n",
    "        return segments.corrected_string\n",
    "    else:\n",
    "        return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog meat\n",
      "fireplace\n",
      "becuase\n"
     ]
    }
   ],
   "source": [
    "# Testing function\n",
    "\n",
    "print(get_segments('dogmeat')) # Should be segmented\n",
    "print(get_segments('fireplace')) # Should not be segmented\n",
    "print(get_segments('becuase')) # Should not be segmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>misspelling</th>\n",
       "      <th>tok_lem_POS</th>\n",
       "      <th>freq</th>\n",
       "      <th>segments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alot</td>\n",
       "      <td>(alot, alot, NN)</td>\n",
       "      <td>103</td>\n",
       "      <td>a lot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>studing</td>\n",
       "      <td>(studing, studing, VBG)</td>\n",
       "      <td>62</td>\n",
       "      <td>stu ding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tofel</td>\n",
       "      <td>(tofel, tofel, NNP)</td>\n",
       "      <td>39</td>\n",
       "      <td>tofel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>goverment</td>\n",
       "      <td>(goverment, goverment, NN)</td>\n",
       "      <td>36</td>\n",
       "      <td>g over men t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>iam</td>\n",
       "      <td>(iam, iam, NNP)</td>\n",
       "      <td>28</td>\n",
       "      <td>i am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>finaly</td>\n",
       "      <td>(finaly, finaly, NNP)</td>\n",
       "      <td>26</td>\n",
       "      <td>final y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>beatiful</td>\n",
       "      <td>(beatiful, beatiful, JJ)</td>\n",
       "      <td>24</td>\n",
       "      <td>beat if ul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>becuase</td>\n",
       "      <td>(becuase, becuase, NN)</td>\n",
       "      <td>24</td>\n",
       "      <td>becuase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>nickell</td>\n",
       "      <td>(nickell, nickell, NNP)</td>\n",
       "      <td>22</td>\n",
       "      <td>nick ell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>oss</td>\n",
       "      <td>(oss, oss, NNP)</td>\n",
       "      <td>22</td>\n",
       "      <td>oss</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  misspelling                 tok_lem_POS  freq      segments\n",
       "0        alot            (alot, alot, NN)   103         a lot\n",
       "1     studing     (studing, studing, VBG)    62      stu ding\n",
       "2       tofel         (tofel, tofel, NNP)    39         tofel\n",
       "3   goverment  (goverment, goverment, NN)    36  g over men t\n",
       "4         iam             (iam, iam, NNP)    28          i am\n",
       "5      finaly       (finaly, finaly, NNP)    26       final y\n",
       "6    beatiful    (beatiful, beatiful, JJ)    24    beat if ul\n",
       "7     becuase      (becuase, becuase, NN)    24       becuase\n",
       "8     nickell     (nickell, nickell, NNP)    22      nick ell\n",
       "9         oss             (oss, oss, NNP)    22           oss"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying the function to create a new column\n",
    "\n",
    "misspell_df['segments'] =  misspell_df['misspelling'].apply(get_segments)\n",
    "misspell_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting this new column as segmentation creates false segments of misspelled words\n",
    "\n",
    "del misspell_df['segments']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying spelling correction\n",
    "\n",
    "In some ways SymSpell is not ideal as full sentence context is not considered, only general frequencies. However, other well-known spellcheckers (hunspell, pyspell, etc.) use the same strategy - frequency based criteria for suggestions, without considering immediate cotext. As such, we have followed this common practice, but it is important to remember that accuracy of corrected tokens will not be 100% and must be taken into consideration.\n",
    "\n",
    "As a compromise and to consider context, spelling corrections based on bigrams is first implemented. If no suggestions are available, spelling corrections based on unigrams are implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "because, 1, 271323986\n"
     ]
    }
   ],
   "source": [
    "# Testing spelling suggestions with 'becuase'\n",
    "\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "dictionary_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "# term_index is the column of the term and count_index is the column of the term frequency\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "\n",
    "input_term = \"becuase\"\n",
    "suggestions = sym_spell.lookup(input_term, Verbosity.CLOSEST, max_edit_distance=2, #Edit distance can be adjusted\n",
    "                               transfer_casing=True, #Optional argument set to ignore case\n",
    "                              include_unknown=True) #Return same word if unknown\n",
    "for suggestion in suggestions:\n",
    "    print(suggestion)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating function for finding unigram suggestions\n",
    "\n",
    "def get_unigram_suggestions(word):\n",
    "    if len(word) >= 4:\n",
    "        suggestions = sym_spell.lookup(word, Verbosity.CLOSEST,max_edit_distance=2, transfer_casing=True)\n",
    "    else:\n",
    "        suggestions = sym_spell.lookup(word, Verbosity.CLOSEST,max_edit_distance=1, transfer_casing=True)\n",
    "    return [str(x).split(',') for x in suggestions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['because', ' 1', ' 271323986']]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing function\n",
    "\n",
    "get_unigram_suggestions('becuase')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: The function has a variable edit distance: words of length 4 or more get edit distance of 2, shorter words get edit distance of 1. These preferences can be adjusted in the function if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "because of, 1, 3481714\n"
     ]
    }
   ],
   "source": [
    "# Testing spelling suggestions with 'becuase of'\n",
    "\n",
    "max_edit_distance_dictionary = 2\n",
    "prefix_length = 7\n",
    "sym_spell = SymSpell(max_edit_distance_dictionary, prefix_length)\n",
    "bigram_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\")\n",
    "if not sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1):\n",
    "    print(\"Dictionary file not found\")\n",
    "if not sym_spell.load_bigram_dictionary(bigram_path, term_index=0, count_index=2):\n",
    "    print(\"Bigram dictionary file not found\")\n",
    "input_term = 'becuase of'\n",
    "max_edit_distance_lookup = 2\n",
    "suggestions = sym_spell.lookup_compound(input_term, max_edit_distance_lookup)\n",
    "for suggestion in suggestions:\n",
    "    print(suggestion) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating function for finding bigram suggestions\n",
    "\n",
    "def get_bigram_suggestions(bigram):\n",
    "    suggestions = sym_spell.lookup_compound(bigram, max_edit_distance_lookup)\n",
    "    for suggestion in suggestions:\n",
    "        return [str(x).split(',') for x in suggestions] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['work hard', ' 2', ' 53229']]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing function\n",
    "get_bigram_suggestions('worq harg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returing to non_words2 dataframe and applying functions to create new column\n",
    "\n",
    "# Creating unigram suggestions column\n",
    "\n",
    "non_words2['unigram_suggestions'] =  non_words2['misspelling'].apply(\n",
    "    lambda x: get_unigram_suggestions(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turning into tuples for easier processing\n",
    "\n",
    "non_words2.unigram_suggestions = non_words2.unigram_suggestions.apply(\n",
    "    lambda row: [tuple(x) for x in row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating bigram suggestions column\n",
    "\n",
    "non_words2['bigram_suggestions'] =  non_words2['bigrams'].apply(\n",
    "    lambda row: [get_bigram_suggestions(x) for x in row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flattening and turning into tuples for easier processing\n",
    "\n",
    "non_words2.bigram_suggestions = non_words2.bigram_suggestions.apply(\n",
    "    lambda row: [tuple(x) for y in row for x in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer_id</th>\n",
       "      <th>tok_lem_POS</th>\n",
       "      <th>misspelling</th>\n",
       "      <th>sentence</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>unigram_suggestions</th>\n",
       "      <th>bigram_suggestions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>[(i, i, PRP), (organized, organize, VBD), (the...</td>\n",
       "      <td>(beacause, beacause, NN)</td>\n",
       "      <td>I organized the instructions by time, beacause...</td>\n",
       "      <td>[time beacause, beacause to]</td>\n",
       "      <td>[(because,  1,  271323986)]</td>\n",
       "      <td>[(time because,  1,  240561), (because to,  1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>[(to, to, TO), (make, make, VB), (tea, tea, NN...</td>\n",
       "      <td>(wallmart, wallmart, NN)</td>\n",
       "      <td>next, you need to buy a box of tea in wallmart...</td>\n",
       "      <td>[in wallmart, wallmart or]</td>\n",
       "      <td>[(walmart,  1,  2269839)]</td>\n",
       "      <td>[(in wall art,  1,  99805), (wall art or,  1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>[(first, first, RB), (,, ,, ,), (you, you, PRP...</td>\n",
       "      <td>(dovn, dovn, NN)</td>\n",
       "      <td>First, you should take some hot water, you can...</td>\n",
       "      <td>[use dovn, dovn mircowave]</td>\n",
       "      <td>[(down,  1,  224915894), (don,  1,  26003672),...</td>\n",
       "      <td>[(use down,  1,  157999), (down microwave,  2,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>[(first, first, RB), (,, ,, ,), (you, you, PRP...</td>\n",
       "      <td>(mircowave, mircowave, VBP)</td>\n",
       "      <td>First, you should take some hot water, you can...</td>\n",
       "      <td>[dovn mircowave, mircowave or]</td>\n",
       "      <td>[(microwave,  1,  8934594)]</td>\n",
       "      <td>[(down microwave,  2,  1960), (microwave or,  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>[(in, in, IN), (my, my, PRP$), (country, count...</td>\n",
       "      <td>(fitst, fitst, NNP)</td>\n",
       "      <td>Fitst, boil a water in a pot.</td>\n",
       "      <td>[fitst boil]</td>\n",
       "      <td>[(first,  1,  578161543), (fits,  1,  12942004...</td>\n",
       "      <td>[(first boil,  1,  1366)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   answer_id                                        tok_lem_POS  \\\n",
       "0          8  [(i, i, PRP), (organized, organize, VBD), (the...   \n",
       "1         11  [(to, to, TO), (make, make, VB), (tea, tea, NN...   \n",
       "2         13  [(first, first, RB), (,, ,, ,), (you, you, PRP...   \n",
       "3         13  [(first, first, RB), (,, ,, ,), (you, you, PRP...   \n",
       "4         15  [(in, in, IN), (my, my, PRP$), (country, count...   \n",
       "\n",
       "                   misspelling  \\\n",
       "0     (beacause, beacause, NN)   \n",
       "1     (wallmart, wallmart, NN)   \n",
       "2             (dovn, dovn, NN)   \n",
       "3  (mircowave, mircowave, VBP)   \n",
       "4          (fitst, fitst, NNP)   \n",
       "\n",
       "                                            sentence  \\\n",
       "0  I organized the instructions by time, beacause...   \n",
       "1  next, you need to buy a box of tea in wallmart...   \n",
       "2  First, you should take some hot water, you can...   \n",
       "3  First, you should take some hot water, you can...   \n",
       "4                      Fitst, boil a water in a pot.   \n",
       "\n",
       "                          bigrams  \\\n",
       "0    [time beacause, beacause to]   \n",
       "1      [in wallmart, wallmart or]   \n",
       "2      [use dovn, dovn mircowave]   \n",
       "3  [dovn mircowave, mircowave or]   \n",
       "4                    [fitst boil]   \n",
       "\n",
       "                                 unigram_suggestions  \\\n",
       "0                        [(because,  1,  271323986)]   \n",
       "1                          [(walmart,  1,  2269839)]   \n",
       "2  [(down,  1,  224915894), (don,  1,  26003672),...   \n",
       "3                        [(microwave,  1,  8934594)]   \n",
       "4  [(first,  1,  578161543), (fits,  1,  12942004...   \n",
       "\n",
       "                                  bigram_suggestions  \n",
       "0  [(time because,  1,  240561), (because to,  1,...  \n",
       "1  [(in wall art,  1,  99805), (wall art or,  1, ...  \n",
       "2  [(use down,  1,  157999), (down microwave,  2,...  \n",
       "3  [(down microwave,  2,  1960), (microwave or,  ...  \n",
       "4                          [(first boil,  1,  1366)]  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_words2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1966\n",
      "116\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "# Checking how many items without suggestions\n",
    "\n",
    "print(len(non_words2.loc[(non_words2.unigram_suggestions.str.len() == 0),:]))\n",
    "print(len(non_words2.loc[(non_words2.bigram_suggestions.str.len() == 0),:]))\n",
    "print(len(non_words2.loc[(non_words2.bigram_suggestions.str.len() == 0) & (non_words2.unigram_suggestions.str.len() == 0),:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer_id</th>\n",
       "      <th>tok_lem_POS</th>\n",
       "      <th>misspelling</th>\n",
       "      <th>sentence</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>unigram_suggestions</th>\n",
       "      <th>bigram_suggestions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>105</td>\n",
       "      <td>[(here, here, RB), (are, be, VBP), (a, a, DT),...</td>\n",
       "      <td>(asdfkjdlkfjadlfjalsdf, asdfkjdlkfjadlfjalsdf,...</td>\n",
       "      <td>asdfkjdlkfjadlfjalsdf</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4104</th>\n",
       "      <td>10490</td>\n",
       "      <td>[(getting, get, VBG), (good, good, NNP), (grad...</td>\n",
       "      <td>(quintcareers, quintcareers, NNS)</td>\n",
       "      <td>Quintcareers.</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7223</th>\n",
       "      <td>18380</td>\n",
       "      <td>[(hi, hi, NNP), (,, ,, ,), (adrian, adrian, NN...</td>\n",
       "      <td>(yeonjea, yeonjea, NN)</td>\n",
       "      <td>- Yeonjea.</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8328</th>\n",
       "      <td>20089</td>\n",
       "      <td>[(uuuuuuu, uuuuuuu, NN)]</td>\n",
       "      <td>(uuuuuuu, uuuuuuu, NN)</td>\n",
       "      <td>uuuuuuu</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8329</th>\n",
       "      <td>20096</td>\n",
       "      <td>[(yhryr, yhryr, NN)]</td>\n",
       "      <td>(yhryr, yhryr, NN)</td>\n",
       "      <td>yhryr</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8989</th>\n",
       "      <td>21585</td>\n",
       "      <td>[(1, 1, CD), (., ., .), (individual, individua...</td>\n",
       "      <td>(shadizubeidi, shadizubeidi, NNP)</td>\n",
       "      <td>Shadizubeidi.</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9161</th>\n",
       "      <td>21845</td>\n",
       "      <td>[(when, when, WRB), (i, i, PRP), ('m, be, VBP)...</td>\n",
       "      <td>(jaaaaaaaaaaaaaaaaaaajajjajajajaja, jaaaaaaaaa...</td>\n",
       "      <td>Jaaaaaaaaaaaaaaaaaaajajjajajajaja ;)</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10457</th>\n",
       "      <td>24136</td>\n",
       "      <td>[(hsthethghryjyjy, hsthethghryjyjy, NN)]</td>\n",
       "      <td>(hsthethghryjyjy, hsthethghryjyjy, NN)</td>\n",
       "      <td>hsthethghryjyjy</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11293</th>\n",
       "      <td>25988</td>\n",
       "      <td>[(fibromyalgia, fibromyalgia, NNP), (?, ?, .),...</td>\n",
       "      <td>(fibromyalgia, fibromyalgia, NNP)</td>\n",
       "      <td>Fibromyalgia ?</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13222</th>\n",
       "      <td>30421</td>\n",
       "      <td>[(stayhealthy, stayhealthy, NN)]</td>\n",
       "      <td>(stayhealthy, stayhealthy, NN)</td>\n",
       "      <td>StayHealthy</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13798</th>\n",
       "      <td>32028</td>\n",
       "      <td>[(hiiiiiii, hiiiiiii, NN)]</td>\n",
       "      <td>(hiiiiiii, hiiiiiii, NN)</td>\n",
       "      <td>hiiiiiii</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14551</th>\n",
       "      <td>33214</td>\n",
       "      <td>[(1, 1, CD), (), ), )), (it, it, PRP), (is, be...</td>\n",
       "      <td>(hahahaha, hahahaha, NN)</td>\n",
       "      <td>hahahaha.</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17909</th>\n",
       "      <td>39728</td>\n",
       "      <td>[(drylegbed, drylegbed, NNP)]</td>\n",
       "      <td>(drylegbed, drylegbed, NNP)</td>\n",
       "      <td>Drylegbed</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17952</th>\n",
       "      <td>39870</td>\n",
       "      <td>[(deepfried, deepfried, VBN)]</td>\n",
       "      <td>(deepfried, deepfried, VBN)</td>\n",
       "      <td>deepfried</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17953</th>\n",
       "      <td>39887</td>\n",
       "      <td>[(favlorless, favlorless, NN)]</td>\n",
       "      <td>(favlorless, favlorless, NN)</td>\n",
       "      <td>favlorless</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       answer_id                                        tok_lem_POS  \\\n",
       "71           105  [(here, here, RB), (are, be, VBP), (a, a, DT),...   \n",
       "4104       10490  [(getting, get, VBG), (good, good, NNP), (grad...   \n",
       "7223       18380  [(hi, hi, NNP), (,, ,, ,), (adrian, adrian, NN...   \n",
       "8328       20089                           [(uuuuuuu, uuuuuuu, NN)]   \n",
       "8329       20096                               [(yhryr, yhryr, NN)]   \n",
       "8989       21585  [(1, 1, CD), (., ., .), (individual, individua...   \n",
       "9161       21845  [(when, when, WRB), (i, i, PRP), ('m, be, VBP)...   \n",
       "10457      24136           [(hsthethghryjyjy, hsthethghryjyjy, NN)]   \n",
       "11293      25988  [(fibromyalgia, fibromyalgia, NNP), (?, ?, .),...   \n",
       "13222      30421                   [(stayhealthy, stayhealthy, NN)]   \n",
       "13798      32028                         [(hiiiiiii, hiiiiiii, NN)]   \n",
       "14551      33214  [(1, 1, CD), (), ), )), (it, it, PRP), (is, be...   \n",
       "17909      39728                      [(drylegbed, drylegbed, NNP)]   \n",
       "17952      39870                      [(deepfried, deepfried, VBN)]   \n",
       "17953      39887                     [(favlorless, favlorless, NN)]   \n",
       "\n",
       "                                             misspelling  \\\n",
       "71     (asdfkjdlkfjadlfjalsdf, asdfkjdlkfjadlfjalsdf,...   \n",
       "4104                   (quintcareers, quintcareers, NNS)   \n",
       "7223                              (yeonjea, yeonjea, NN)   \n",
       "8328                              (uuuuuuu, uuuuuuu, NN)   \n",
       "8329                                  (yhryr, yhryr, NN)   \n",
       "8989                   (shadizubeidi, shadizubeidi, NNP)   \n",
       "9161   (jaaaaaaaaaaaaaaaaaaajajjajajajaja, jaaaaaaaaa...   \n",
       "10457             (hsthethghryjyjy, hsthethghryjyjy, NN)   \n",
       "11293                  (fibromyalgia, fibromyalgia, NNP)   \n",
       "13222                     (stayhealthy, stayhealthy, NN)   \n",
       "13798                           (hiiiiiii, hiiiiiii, NN)   \n",
       "14551                           (hahahaha, hahahaha, NN)   \n",
       "17909                        (drylegbed, drylegbed, NNP)   \n",
       "17952                        (deepfried, deepfried, VBN)   \n",
       "17953                       (favlorless, favlorless, NN)   \n",
       "\n",
       "                                   sentence bigrams unigram_suggestions  \\\n",
       "71                    asdfkjdlkfjadlfjalsdf      []                  []   \n",
       "4104                          Quintcareers.      []                  []   \n",
       "7223                             - Yeonjea.      []                  []   \n",
       "8328                                uuuuuuu      []                  []   \n",
       "8329                                  yhryr      []                  []   \n",
       "8989                          Shadizubeidi.      []                  []   \n",
       "9161   Jaaaaaaaaaaaaaaaaaaajajjajajajaja ;)      []                  []   \n",
       "10457                       hsthethghryjyjy      []                  []   \n",
       "11293                        Fibromyalgia ?      []                  []   \n",
       "13222                           StayHealthy      []                  []   \n",
       "13798                              hiiiiiii      []                  []   \n",
       "14551                             hahahaha.      []                  []   \n",
       "17909                             Drylegbed      []                  []   \n",
       "17952                             deepfried      []                  []   \n",
       "17953                            favlorless      []                  []   \n",
       "\n",
       "      bigram_suggestions  \n",
       "71                    []  \n",
       "4104                  []  \n",
       "7223                  []  \n",
       "8328                  []  \n",
       "8329                  []  \n",
       "8989                  []  \n",
       "9161                  []  \n",
       "10457                 []  \n",
       "11293                 []  \n",
       "13222                 []  \n",
       "13798                 []  \n",
       "14551                 []  \n",
       "17909                 []  \n",
       "17952                 []  \n",
       "17953                 []  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_words2.loc[(non_words2.bigram_suggestions.str.len() == 0) & (non_words2.unigram_suggestions.str.len() == 0),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Items with no suggestions - these will be left in their original form though manual corrections could be applied if desired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a new column with just the most likely correction (based on frequency). Bigram suggestions are given preference before unigram suggestions. If there is no suggestion, the original word is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new column with just the most likely correction (based on frequency)\n",
    "\n",
    "def sort_tuple(tup):  \n",
    "    tup.sort(key = lambda x: x[2], reverse=True)  \n",
    "    return tup    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping the bigram correction with the highest frequency\n",
    "\n",
    "non_words2['bigram_correction'] = [sort_tuple(x)[0][0] if len(x) != 0 else np.NaN for x in non_words2['bigram_suggestions']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping the unigram correction with the highest frequency\n",
    "\n",
    "non_words2['unigram_correction'] = [sort_tuple(x)[0][0] if len(x) != 0 else np.NaN for x in non_words2['unigram_suggestions']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STUCK HERE\n",
    "Trying to keep only the word in the bigram correction that was originally misspelled, not the entire bigram. Below are failed experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating function to keep only the key word of the bigram correction\n",
    "\n",
    "def key_word(bigrams,bigram_correction):\n",
    "    key_words = []\n",
    "    for word in bigram_correction.split():\n",
    "        if word not in lex.re_tokenize(str(bigrams)):\n",
    "            key_words.append(word)\n",
    "    return key_words\n",
    "    #return \" \".join(key_words) -- experimenting with both version, either return a list or a single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer_id</th>\n",
       "      <th>tok_lem_POS</th>\n",
       "      <th>misspelling</th>\n",
       "      <th>sentence</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>unigram_suggestions</th>\n",
       "      <th>bigram_suggestions</th>\n",
       "      <th>bigram_correction</th>\n",
       "      <th>unigram_correction</th>\n",
       "      <th>final_correction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>[(i, i, PRP), (organized, organize, VBD), (the...</td>\n",
       "      <td>(beacause, beacause, NN)</td>\n",
       "      <td>I organized the instructions by time, beacause...</td>\n",
       "      <td>[time beacause, beacause to]</td>\n",
       "      <td>[(because,  1,  271323986)]</td>\n",
       "      <td>[(because to,  1,  3213023), (time because,  1...</td>\n",
       "      <td>because to</td>\n",
       "      <td>because</td>\n",
       "      <td>[because]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>[(to, to, TO), (make, make, VB), (tea, tea, NN...</td>\n",
       "      <td>(wallmart, wallmart, NN)</td>\n",
       "      <td>next, you need to buy a box of tea in wallmart...</td>\n",
       "      <td>[in wallmart, wallmart or]</td>\n",
       "      <td>[(walmart,  1,  2269839)]</td>\n",
       "      <td>[(in wall art,  1,  99805), (wall art or,  1, ...</td>\n",
       "      <td>in wall art</td>\n",
       "      <td>walmart</td>\n",
       "      <td>[wall, art]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>[(first, first, RB), (,, ,, ,), (you, you, PRP...</td>\n",
       "      <td>(dovn, dovn, NN)</td>\n",
       "      <td>First, you should take some hot water, you can...</td>\n",
       "      <td>[use dovn, dovn mircowave]</td>\n",
       "      <td>[(dove,  1,  3253560), (donn,  1,  299470), (d...</td>\n",
       "      <td>[(down microwave,  2,  1960), (use down,  1,  ...</td>\n",
       "      <td>down microwave</td>\n",
       "      <td>dove</td>\n",
       "      <td>[down, microwave]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>[(first, first, RB), (,, ,, ,), (you, you, PRP...</td>\n",
       "      <td>(mircowave, mircowave, VBP)</td>\n",
       "      <td>First, you should take some hot water, you can...</td>\n",
       "      <td>[dovn mircowave, mircowave or]</td>\n",
       "      <td>[(microwave,  1,  8934594)]</td>\n",
       "      <td>[(microwave or,  1,  22584), (down microwave, ...</td>\n",
       "      <td>microwave or</td>\n",
       "      <td>microwave</td>\n",
       "      <td>[microwave]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>[(in, in, IN), (my, my, PRP$), (country, count...</td>\n",
       "      <td>(fitst, fitst, NNP)</td>\n",
       "      <td>Fitst, boil a water in a pot.</td>\n",
       "      <td>[fitst boil]</td>\n",
       "      <td>[(fist,  1,  7319405), (first,  1,  578161543)...</td>\n",
       "      <td>[(first boil,  1,  1366)]</td>\n",
       "      <td>first boil</td>\n",
       "      <td>fist</td>\n",
       "      <td>[first]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   answer_id                                        tok_lem_POS  \\\n",
       "0          8  [(i, i, PRP), (organized, organize, VBD), (the...   \n",
       "1         11  [(to, to, TO), (make, make, VB), (tea, tea, NN...   \n",
       "2         13  [(first, first, RB), (,, ,, ,), (you, you, PRP...   \n",
       "3         13  [(first, first, RB), (,, ,, ,), (you, you, PRP...   \n",
       "4         15  [(in, in, IN), (my, my, PRP$), (country, count...   \n",
       "\n",
       "                   misspelling  \\\n",
       "0     (beacause, beacause, NN)   \n",
       "1     (wallmart, wallmart, NN)   \n",
       "2             (dovn, dovn, NN)   \n",
       "3  (mircowave, mircowave, VBP)   \n",
       "4          (fitst, fitst, NNP)   \n",
       "\n",
       "                                            sentence  \\\n",
       "0  I organized the instructions by time, beacause...   \n",
       "1  next, you need to buy a box of tea in wallmart...   \n",
       "2  First, you should take some hot water, you can...   \n",
       "3  First, you should take some hot water, you can...   \n",
       "4                      Fitst, boil a water in a pot.   \n",
       "\n",
       "                          bigrams  \\\n",
       "0    [time beacause, beacause to]   \n",
       "1      [in wallmart, wallmart or]   \n",
       "2      [use dovn, dovn mircowave]   \n",
       "3  [dovn mircowave, mircowave or]   \n",
       "4                    [fitst boil]   \n",
       "\n",
       "                                 unigram_suggestions  \\\n",
       "0                        [(because,  1,  271323986)]   \n",
       "1                          [(walmart,  1,  2269839)]   \n",
       "2  [(dove,  1,  3253560), (donn,  1,  299470), (d...   \n",
       "3                        [(microwave,  1,  8934594)]   \n",
       "4  [(fist,  1,  7319405), (first,  1,  578161543)...   \n",
       "\n",
       "                                  bigram_suggestions bigram_correction  \\\n",
       "0  [(because to,  1,  3213023), (time because,  1...        because to   \n",
       "1  [(in wall art,  1,  99805), (wall art or,  1, ...       in wall art   \n",
       "2  [(down microwave,  2,  1960), (use down,  1,  ...    down microwave   \n",
       "3  [(microwave or,  1,  22584), (down microwave, ...      microwave or   \n",
       "4                          [(first boil,  1,  1366)]        first boil   \n",
       "\n",
       "  unigram_correction   final_correction  \n",
       "0            because          [because]  \n",
       "1            walmart        [wall, art]  \n",
       "2               dove  [down, microwave]  \n",
       "3          microwave        [microwave]  \n",
       "4               fist            [first]  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying the above function to the selected rows\n",
    "\n",
    "mask2 = non_words2.loc[~non_words2.bigram_correction.isnull()].index\n",
    "non_words2['final_correction'] = non_words2.loc[mask2][['bigrams','bigram_correction']].apply(\n",
    "    lambda x: key_word(x[0],x[1]),axis=1)\n",
    "\n",
    "non_words2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer_id</th>\n",
       "      <th>tok_lem_POS</th>\n",
       "      <th>misspelling</th>\n",
       "      <th>sentence</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>unigram_suggestions</th>\n",
       "      <th>bigram_suggestions</th>\n",
       "      <th>bigram_correction</th>\n",
       "      <th>unigram_correction</th>\n",
       "      <th>final_correction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>630</td>\n",
       "      <td>[(i, i, NN), (am, be, VBP), (living, live, VBG...</td>\n",
       "      <td>(blvd, blvd, NNP)</td>\n",
       "      <td>i am living on ANON_NAME_0 blvd street</td>\n",
       "      <td>[name blvd, blvd street]</td>\n",
       "      <td>[(blvd,  0,  13317989)]</td>\n",
       "      <td>[(name blvd,  0,  6036), (blvd street,  0,  19...</td>\n",
       "      <td>name blvd</td>\n",
       "      <td>blvd</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>643</td>\n",
       "      <td>[(good, good, JJ), (effects, effect, NNS), (of...</td>\n",
       "      <td>(sen, sen, NN)</td>\n",
       "      <td>\"On\" is hot, and \"sen\" is springs in Japanese,...</td>\n",
       "      <td>[and sen, sen is]</td>\n",
       "      <td>[(sen,  0,  5230325)]</td>\n",
       "      <td>[(and sen,  0,  66329), (sen is,  0,  24014)]</td>\n",
       "      <td>and sen</td>\n",
       "      <td>sen</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>796</td>\n",
       "      <td>[(mismanagement, mismanagement, NN), (:, :, :)...</td>\n",
       "      <td>(est, est, NNP)</td>\n",
       "      <td>Mismanagement:\\n\\nP of S: N\\n\\nDefinition: if ...</td>\n",
       "      <td>[against est]</td>\n",
       "      <td>[(est,  0,  58112143)]</td>\n",
       "      <td>[(against est,  0,  8349)]</td>\n",
       "      <td>against est</td>\n",
       "      <td>est</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>796</td>\n",
       "      <td>[(mismanagement, mismanagement, NN), (:, :, :)...</td>\n",
       "      <td>(est, est, JJS)</td>\n",
       "      <td>Mismanagement:\\n\\nP of S: N\\n\\nDefinition: if ...</td>\n",
       "      <td>[against est]</td>\n",
       "      <td>[(est,  0,  58112143)]</td>\n",
       "      <td>[(against est,  0,  8349)]</td>\n",
       "      <td>against est</td>\n",
       "      <td>est</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>846</td>\n",
       "      <td>[(suspend, suspend, NN), (verb, verb, NNP), (d...</td>\n",
       "      <td>(esp, esp, FW)</td>\n",
       "      <td>to reprove or scold, esp.</td>\n",
       "      <td>[scold esp]</td>\n",
       "      <td>[(esp,  0,  4612780)]</td>\n",
       "      <td>[(scold esp,  0,  0)]</td>\n",
       "      <td>scold esp</td>\n",
       "      <td>esp</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20578</th>\n",
       "      <td>47525</td>\n",
       "      <td>[(in, in, IN), (italy, italy, NNP), (,, ,, ,),...</td>\n",
       "      <td>(bunuel, bunuel, NNP)</td>\n",
       "      <td>The Spaniard Luis Bunuel, whose impressive fil...</td>\n",
       "      <td>[luis bunuel, bunuel whose]</td>\n",
       "      <td>[(bunuel,  0,  48835)]</td>\n",
       "      <td>[(bunuel whose,  0,  2), (luis bunuel,  0,  0)]</td>\n",
       "      <td>bunuel whose</td>\n",
       "      <td>bunuel</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20586</th>\n",
       "      <td>47541</td>\n",
       "      <td>[(modern, modern, JJ), (family, family, NN), (...</td>\n",
       "      <td>(phil, phil, NNP)</td>\n",
       "      <td>Second family, which is the most common type, ...</td>\n",
       "      <td>[to phil, phil who]</td>\n",
       "      <td>[(phil,  0,  14637522)]</td>\n",
       "      <td>[(phil who,  0,  9010), (to phil,  0,  173337)]</td>\n",
       "      <td>phil who</td>\n",
       "      <td>phil</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20655</th>\n",
       "      <td>47839</td>\n",
       "      <td>[(relationships, relationship, NNS), (are, be,...</td>\n",
       "      <td>(esp, esp, NN)</td>\n",
       "      <td>The Canadian Oxford definition says that a sou...</td>\n",
       "      <td>[bond esp]</td>\n",
       "      <td>[(esp,  0,  4612780)]</td>\n",
       "      <td>[(bond esp,  0,  105)]</td>\n",
       "      <td>bond esp</td>\n",
       "      <td>esp</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20689</th>\n",
       "      <td>47868</td>\n",
       "      <td>[(annie, annie, NNP), (is, be, VBZ), (my, my, ...</td>\n",
       "      <td>(maths, maths, NNP)</td>\n",
       "      <td>She is good at Maths which always makes me con...</td>\n",
       "      <td>[at maths, maths which]</td>\n",
       "      <td>[(maths,  0,  3231878)]</td>\n",
       "      <td>[(at maths,  0,  7165), (maths which,  0,  2555)]</td>\n",
       "      <td>at maths</td>\n",
       "      <td>maths</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20895</th>\n",
       "      <td>48127</td>\n",
       "      <td>[(the, the, DT), (cathedral, cathedral, NN), (...</td>\n",
       "      <td>(inf, inf, JJ)</td>\n",
       "      <td>The cathedral of learning is the main buildin...</td>\n",
       "      <td>[are inf, inf chairs]</td>\n",
       "      <td>[(inf,  0,  6171461)]</td>\n",
       "      <td>[(inf chairs,  0,  95), (are inf,  0,  14413)]</td>\n",
       "      <td>inf chairs</td>\n",
       "      <td>inf</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       answer_id                                        tok_lem_POS  \\\n",
       "392          630  [(i, i, NN), (am, be, VBP), (living, live, VBG...   \n",
       "398          643  [(good, good, JJ), (effects, effect, NNS), (of...   \n",
       "502          796  [(mismanagement, mismanagement, NN), (:, :, :)...   \n",
       "503          796  [(mismanagement, mismanagement, NN), (:, :, :)...   \n",
       "517          846  [(suspend, suspend, NN), (verb, verb, NNP), (d...   \n",
       "...          ...                                                ...   \n",
       "20578      47525  [(in, in, IN), (italy, italy, NNP), (,, ,, ,),...   \n",
       "20586      47541  [(modern, modern, JJ), (family, family, NN), (...   \n",
       "20655      47839  [(relationships, relationship, NNS), (are, be,...   \n",
       "20689      47868  [(annie, annie, NNP), (is, be, VBZ), (my, my, ...   \n",
       "20895      48127  [(the, the, DT), (cathedral, cathedral, NN), (...   \n",
       "\n",
       "                 misspelling  \\\n",
       "392        (blvd, blvd, NNP)   \n",
       "398           (sen, sen, NN)   \n",
       "502          (est, est, NNP)   \n",
       "503          (est, est, JJS)   \n",
       "517           (esp, esp, FW)   \n",
       "...                      ...   \n",
       "20578  (bunuel, bunuel, NNP)   \n",
       "20586      (phil, phil, NNP)   \n",
       "20655         (esp, esp, NN)   \n",
       "20689    (maths, maths, NNP)   \n",
       "20895         (inf, inf, JJ)   \n",
       "\n",
       "                                                sentence  \\\n",
       "392               i am living on ANON_NAME_0 blvd street   \n",
       "398    \"On\" is hot, and \"sen\" is springs in Japanese,...   \n",
       "502    Mismanagement:\\n\\nP of S: N\\n\\nDefinition: if ...   \n",
       "503    Mismanagement:\\n\\nP of S: N\\n\\nDefinition: if ...   \n",
       "517                            to reprove or scold, esp.   \n",
       "...                                                  ...   \n",
       "20578  The Spaniard Luis Bunuel, whose impressive fil...   \n",
       "20586  Second family, which is the most common type, ...   \n",
       "20655  The Canadian Oxford definition says that a sou...   \n",
       "20689  She is good at Maths which always makes me con...   \n",
       "20895   The cathedral of learning is the main buildin...   \n",
       "\n",
       "                           bigrams      unigram_suggestions  \\\n",
       "392       [name blvd, blvd street]  [(blvd,  0,  13317989)]   \n",
       "398              [and sen, sen is]    [(sen,  0,  5230325)]   \n",
       "502                  [against est]   [(est,  0,  58112143)]   \n",
       "503                  [against est]   [(est,  0,  58112143)]   \n",
       "517                    [scold esp]    [(esp,  0,  4612780)]   \n",
       "...                            ...                      ...   \n",
       "20578  [luis bunuel, bunuel whose]   [(bunuel,  0,  48835)]   \n",
       "20586          [to phil, phil who]  [(phil,  0,  14637522)]   \n",
       "20655                   [bond esp]    [(esp,  0,  4612780)]   \n",
       "20689      [at maths, maths which]  [(maths,  0,  3231878)]   \n",
       "20895        [are inf, inf chairs]    [(inf,  0,  6171461)]   \n",
       "\n",
       "                                      bigram_suggestions bigram_correction  \\\n",
       "392    [(name blvd,  0,  6036), (blvd street,  0,  19...         name blvd   \n",
       "398        [(and sen,  0,  66329), (sen is,  0,  24014)]           and sen   \n",
       "502                           [(against est,  0,  8349)]       against est   \n",
       "503                           [(against est,  0,  8349)]       against est   \n",
       "517                                [(scold esp,  0,  0)]         scold esp   \n",
       "...                                                  ...               ...   \n",
       "20578    [(bunuel whose,  0,  2), (luis bunuel,  0,  0)]      bunuel whose   \n",
       "20586    [(phil who,  0,  9010), (to phil,  0,  173337)]          phil who   \n",
       "20655                             [(bond esp,  0,  105)]          bond esp   \n",
       "20689  [(at maths,  0,  7165), (maths which,  0,  2555)]          at maths   \n",
       "20895     [(inf chairs,  0,  95), (are inf,  0,  14413)]        inf chairs   \n",
       "\n",
       "      unigram_correction final_correction  \n",
       "392                 blvd               []  \n",
       "398                  sen               []  \n",
       "502                  est               []  \n",
       "503                  est               []  \n",
       "517                  esp               []  \n",
       "...                  ...              ...  \n",
       "20578             bunuel               []  \n",
       "20586               phil               []  \n",
       "20655                esp               []  \n",
       "20689              maths               []  \n",
       "20895                inf               []  \n",
       "\n",
       "[300 rows x 10 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mystery to solve - why are these final corrections blank when there is a bigram correction?\n",
    "\n",
    "non_words2.loc[(non_words2.final_correction.str.len() == 0) & (non_words2.bigram_correction.isnull()==False),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually checking items with more than one word in the final correction - bigrams with two misspellings will need to be trimmed, e.g. 'dovn mircowave'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer_id</th>\n",
       "      <th>tok_lem_POS</th>\n",
       "      <th>misspelling</th>\n",
       "      <th>sentence</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>unigram_suggestions</th>\n",
       "      <th>bigram_suggestions</th>\n",
       "      <th>bigram_correction</th>\n",
       "      <th>unigram_correction</th>\n",
       "      <th>final_correction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1956</th>\n",
       "      <td>3622</td>\n",
       "      <td>[(sultan, sultan, NNP), (alhimali, alhimali, V...</td>\n",
       "      <td>(alhimali, alhimali, VBZ)</td>\n",
       "      <td>\\n\\n\\n \\n\\nSultan alhimali\\n\\n\\n\\n The beduons...</td>\n",
       "      <td>[sultan alhimali, alhimali the]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[(all mali the,  2,  244), (sultan all mali,  ...</td>\n",
       "      <td>all mali the</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[all, mali]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1957</th>\n",
       "      <td>3622</td>\n",
       "      <td>[(sultan, sultan, NNP), (alhimali, alhimali, V...</td>\n",
       "      <td>(beduons, beduons, NNS)</td>\n",
       "      <td>\\n\\n\\n \\n\\nSultan alhimali\\n\\n\\n\\n The beduons...</td>\n",
       "      <td>[the beduons, beduons have]</td>\n",
       "      <td>[(beacons,  2,  618822), (bedouins,  2,  55824...</td>\n",
       "      <td>[(the bed on,  2,  199223), (bed on have,  2, ...</td>\n",
       "      <td>the bed on</td>\n",
       "      <td>beacons</td>\n",
       "      <td>[bed, on]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13659</th>\n",
       "      <td>31540</td>\n",
       "      <td>[(the, the, DT), (floence, floence, NN), (when...</td>\n",
       "      <td>(pronaionstion, pronaionstion, NN)</td>\n",
       "      <td>\\n\\nthe floence when i read and the right pron...</td>\n",
       "      <td>[right pronaionstion, pronaionstion for]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[(prozac option for,  5,  1), (right prozac op...</td>\n",
       "      <td>prozac option for</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[prozac, option]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8198</th>\n",
       "      <td>19783</td>\n",
       "      <td>[(the, the, DT), (white, white, NNP), (house, ...</td>\n",
       "      <td>(cigaretts, cigaretts, NN)</td>\n",
       "      <td>\\n \\nThe White House\\nWashington, D.C.20500 07...</td>\n",
       "      <td>[s cigaretts, cigaretts last]</td>\n",
       "      <td>[(cigarette,  1,  6890707), (cigarettes,  1,  ...</td>\n",
       "      <td>[(a cigarette,  2,  61054), (cigarette last,  ...</td>\n",
       "      <td>a cigarette</td>\n",
       "      <td>cigarette</td>\n",
       "      <td>[a, cigarette]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8199</th>\n",
       "      <td>19783</td>\n",
       "      <td>[(the, the, DT), (white, white, NNP), (house, ...</td>\n",
       "      <td>(ecenbarger, ecenbarger, NNP)</td>\n",
       "      <td>\\n \\nThe White House\\nWashington, D.C.20500 07...</td>\n",
       "      <td>[william ecenbarger, ecenbarger from]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[(even larger from,  3,  46153), (william even...</td>\n",
       "      <td>even larger from</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[even, larger]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9026</th>\n",
       "      <td>21601</td>\n",
       "      <td>[(yasterday, yasterday, NN), (when, when, WRB)...</td>\n",
       "      <td>(didnot, didnot, VBD)</td>\n",
       "      <td>yasterday when Iawoke up I called up my friend...</td>\n",
       "      <td>[she didnot, didnot pick]</td>\n",
       "      <td>[(dicot,  2,  89663), (dido,  2,  839057), (mi...</td>\n",
       "      <td>[(did not pick,  1,  304251), (she did not,  1...</td>\n",
       "      <td>did not pick</td>\n",
       "      <td>dicot</td>\n",
       "      <td>[did, not]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9028</th>\n",
       "      <td>21601</td>\n",
       "      <td>[(yasterday, yasterday, NN), (when, when, WRB)...</td>\n",
       "      <td>(numberand, numberand, NN)</td>\n",
       "      <td>yasterday when Iawoke up I called up my friend...</td>\n",
       "      <td>[stange numberand, numberand i]</td>\n",
       "      <td>[(numbered,  2,  4773804), (cumberland,  2,  3...</td>\n",
       "      <td>[(stage number and,  2,  18406), (number and i...</td>\n",
       "      <td>stage number and</td>\n",
       "      <td>numbered</td>\n",
       "      <td>[stage, number, and]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10658</th>\n",
       "      <td>24794</td>\n",
       "      <td>[(fantastic, fantastic, JJ), (galaxy, galaxy, ...</td>\n",
       "      <td>(earthscape, earthscape, NN)</td>\n",
       "      <td>you will see such an incredibly wonderful sigh...</td>\n",
       "      <td>[beautiful earthscape, earthscape reservation]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[(beautiful earth scape,  1,  0), (earth scape...</td>\n",
       "      <td>beautiful earth scape</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[earth, scape]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18084</th>\n",
       "      <td>40470</td>\n",
       "      <td>[(yse, yse, NN), (,, ,, ,), (iam, iam, NNP), (...</td>\n",
       "      <td>(yse, yse, NN)</td>\n",
       "      <td>yse,Iam .I am recycling some staff.</td>\n",
       "      <td>[yse iam]</td>\n",
       "      <td>[(use,  1,  719980257), (lyse,  1,  62280), (y...</td>\n",
       "      <td>[(use i am,  2,  404936)]</td>\n",
       "      <td>use i am</td>\n",
       "      <td>use</td>\n",
       "      <td>[use, i, am]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18085</th>\n",
       "      <td>40470</td>\n",
       "      <td>[(yse, yse, NN), (,, ,, ,), (iam, iam, NNP), (...</td>\n",
       "      <td>(iam, iam, NNP)</td>\n",
       "      <td>yse,Iam .I am recycling some staff.</td>\n",
       "      <td>[yse iam, iam i]</td>\n",
       "      <td>[(dam,  1,  9204248), (diam,  1,  835571), (na...</td>\n",
       "      <td>[(use i am,  2,  404936), (i am i,  1,  1735776)]</td>\n",
       "      <td>use i am</td>\n",
       "      <td>dam</td>\n",
       "      <td>[use, am]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2359 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       answer_id                                        tok_lem_POS  \\\n",
       "1956        3622  [(sultan, sultan, NNP), (alhimali, alhimali, V...   \n",
       "1957        3622  [(sultan, sultan, NNP), (alhimali, alhimali, V...   \n",
       "13659      31540  [(the, the, DT), (floence, floence, NN), (when...   \n",
       "8198       19783  [(the, the, DT), (white, white, NNP), (house, ...   \n",
       "8199       19783  [(the, the, DT), (white, white, NNP), (house, ...   \n",
       "...          ...                                                ...   \n",
       "9026       21601  [(yasterday, yasterday, NN), (when, when, WRB)...   \n",
       "9028       21601  [(yasterday, yasterday, NN), (when, when, WRB)...   \n",
       "10658      24794  [(fantastic, fantastic, JJ), (galaxy, galaxy, ...   \n",
       "18084      40470  [(yse, yse, NN), (,, ,, ,), (iam, iam, NNP), (...   \n",
       "18085      40470  [(yse, yse, NN), (,, ,, ,), (iam, iam, NNP), (...   \n",
       "\n",
       "                              misspelling  \\\n",
       "1956            (alhimali, alhimali, VBZ)   \n",
       "1957              (beduons, beduons, NNS)   \n",
       "13659  (pronaionstion, pronaionstion, NN)   \n",
       "8198           (cigaretts, cigaretts, NN)   \n",
       "8199        (ecenbarger, ecenbarger, NNP)   \n",
       "...                                   ...   \n",
       "9026                (didnot, didnot, VBD)   \n",
       "9028           (numberand, numberand, NN)   \n",
       "10658        (earthscape, earthscape, NN)   \n",
       "18084                      (yse, yse, NN)   \n",
       "18085                     (iam, iam, NNP)   \n",
       "\n",
       "                                                sentence  \\\n",
       "1956   \\n\\n\\n \\n\\nSultan alhimali\\n\\n\\n\\n The beduons...   \n",
       "1957   \\n\\n\\n \\n\\nSultan alhimali\\n\\n\\n\\n The beduons...   \n",
       "13659  \\n\\nthe floence when i read and the right pron...   \n",
       "8198   \\n \\nThe White House\\nWashington, D.C.20500 07...   \n",
       "8199   \\n \\nThe White House\\nWashington, D.C.20500 07...   \n",
       "...                                                  ...   \n",
       "9026   yasterday when Iawoke up I called up my friend...   \n",
       "9028   yasterday when Iawoke up I called up my friend...   \n",
       "10658  you will see such an incredibly wonderful sigh...   \n",
       "18084                yse,Iam .I am recycling some staff.   \n",
       "18085                yse,Iam .I am recycling some staff.   \n",
       "\n",
       "                                              bigrams  \\\n",
       "1956                  [sultan alhimali, alhimali the]   \n",
       "1957                      [the beduons, beduons have]   \n",
       "13659        [right pronaionstion, pronaionstion for]   \n",
       "8198                    [s cigaretts, cigaretts last]   \n",
       "8199            [william ecenbarger, ecenbarger from]   \n",
       "...                                               ...   \n",
       "9026                        [she didnot, didnot pick]   \n",
       "9028                  [stange numberand, numberand i]   \n",
       "10658  [beautiful earthscape, earthscape reservation]   \n",
       "18084                                       [yse iam]   \n",
       "18085                                [yse iam, iam i]   \n",
       "\n",
       "                                     unigram_suggestions  \\\n",
       "1956                                                  []   \n",
       "1957   [(beacons,  2,  618822), (bedouins,  2,  55824...   \n",
       "13659                                                 []   \n",
       "8198   [(cigarette,  1,  6890707), (cigarettes,  1,  ...   \n",
       "8199                                                  []   \n",
       "...                                                  ...   \n",
       "9026   [(dicot,  2,  89663), (dido,  2,  839057), (mi...   \n",
       "9028   [(numbered,  2,  4773804), (cumberland,  2,  3...   \n",
       "10658                                                 []   \n",
       "18084  [(use,  1,  719980257), (lyse,  1,  62280), (y...   \n",
       "18085  [(dam,  1,  9204248), (diam,  1,  835571), (na...   \n",
       "\n",
       "                                      bigram_suggestions  \\\n",
       "1956   [(all mali the,  2,  244), (sultan all mali,  ...   \n",
       "1957   [(the bed on,  2,  199223), (bed on have,  2, ...   \n",
       "13659  [(prozac option for,  5,  1), (right prozac op...   \n",
       "8198   [(a cigarette,  2,  61054), (cigarette last,  ...   \n",
       "8199   [(even larger from,  3,  46153), (william even...   \n",
       "...                                                  ...   \n",
       "9026   [(did not pick,  1,  304251), (she did not,  1...   \n",
       "9028   [(stage number and,  2,  18406), (number and i...   \n",
       "10658  [(beautiful earth scape,  1,  0), (earth scape...   \n",
       "18084                          [(use i am,  2,  404936)]   \n",
       "18085  [(use i am,  2,  404936), (i am i,  1,  1735776)]   \n",
       "\n",
       "           bigram_correction unigram_correction      final_correction  \n",
       "1956            all mali the                NaN           [all, mali]  \n",
       "1957              the bed on            beacons             [bed, on]  \n",
       "13659      prozac option for                NaN      [prozac, option]  \n",
       "8198             a cigarette          cigarette        [a, cigarette]  \n",
       "8199        even larger from                NaN        [even, larger]  \n",
       "...                      ...                ...                   ...  \n",
       "9026            did not pick              dicot            [did, not]  \n",
       "9028        stage number and           numbered  [stage, number, and]  \n",
       "10658  beautiful earth scape                NaN        [earth, scape]  \n",
       "18084               use i am                use          [use, i, am]  \n",
       "18085               use i am                dam             [use, am]  \n",
       "\n",
       "[2359 rows x 10 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What am I even doing any more?\n",
    "pd.concat(g for _, g in non_words2.groupby(\"sentence\") if len(g) > 1).loc[(non_words2.final_correction.str.len() > 1),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END OF STUCK SECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If no correction, use original word\n",
    "\n",
    "non_words2.final_correction.fillna(non_words2.misspelling.apply(lambda x: x[0]), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer_id</th>\n",
       "      <th>tok_lem_POS</th>\n",
       "      <th>misspelling</th>\n",
       "      <th>sentence</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>unigram_suggestions</th>\n",
       "      <th>bigram_suggestions</th>\n",
       "      <th>bigram_correction</th>\n",
       "      <th>unigram_correction</th>\n",
       "      <th>final_correction</th>\n",
       "      <th>final_correction_POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>[(i, i, PRP), (organized, organize, VBD), (the...</td>\n",
       "      <td>(beacause, beacause, NN)</td>\n",
       "      <td>I organized the instructions by time, beacause...</td>\n",
       "      <td>[time beacause, beacause to]</td>\n",
       "      <td>[(because,  1,  271323986)]</td>\n",
       "      <td>[(because to,  1,  3213023), (time because,  1...</td>\n",
       "      <td>because to</td>\n",
       "      <td>because</td>\n",
       "      <td>[because]</td>\n",
       "      <td>([because], NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>[(to, to, TO), (make, make, VB), (tea, tea, NN...</td>\n",
       "      <td>(wallmart, wallmart, NN)</td>\n",
       "      <td>next, you need to buy a box of tea in wallmart...</td>\n",
       "      <td>[in wallmart, wallmart or]</td>\n",
       "      <td>[(walmart,  1,  2269839)]</td>\n",
       "      <td>[(in wall art,  1,  99805), (wall art or,  1, ...</td>\n",
       "      <td>in wall art</td>\n",
       "      <td>walmart</td>\n",
       "      <td>[wall, art]</td>\n",
       "      <td>([wall, art], NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>[(first, first, RB), (,, ,, ,), (you, you, PRP...</td>\n",
       "      <td>(dovn, dovn, NN)</td>\n",
       "      <td>First, you should take some hot water, you can...</td>\n",
       "      <td>[use dovn, dovn mircowave]</td>\n",
       "      <td>[(dove,  1,  3253560), (donn,  1,  299470), (d...</td>\n",
       "      <td>[(down microwave,  2,  1960), (use down,  1,  ...</td>\n",
       "      <td>down microwave</td>\n",
       "      <td>dove</td>\n",
       "      <td>[down, microwave]</td>\n",
       "      <td>([down, microwave], NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>[(first, first, RB), (,, ,, ,), (you, you, PRP...</td>\n",
       "      <td>(mircowave, mircowave, VBP)</td>\n",
       "      <td>First, you should take some hot water, you can...</td>\n",
       "      <td>[dovn mircowave, mircowave or]</td>\n",
       "      <td>[(microwave,  1,  8934594)]</td>\n",
       "      <td>[(microwave or,  1,  22584), (down microwave, ...</td>\n",
       "      <td>microwave or</td>\n",
       "      <td>microwave</td>\n",
       "      <td>[microwave]</td>\n",
       "      <td>([microwave], VBP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>[(in, in, IN), (my, my, PRP$), (country, count...</td>\n",
       "      <td>(fitst, fitst, NNP)</td>\n",
       "      <td>Fitst, boil a water in a pot.</td>\n",
       "      <td>[fitst boil]</td>\n",
       "      <td>[(fist,  1,  7319405), (first,  1,  578161543)...</td>\n",
       "      <td>[(first boil,  1,  1366)]</td>\n",
       "      <td>first boil</td>\n",
       "      <td>fist</td>\n",
       "      <td>[first]</td>\n",
       "      <td>([first], NNP)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   answer_id                                        tok_lem_POS  \\\n",
       "0          8  [(i, i, PRP), (organized, organize, VBD), (the...   \n",
       "1         11  [(to, to, TO), (make, make, VB), (tea, tea, NN...   \n",
       "2         13  [(first, first, RB), (,, ,, ,), (you, you, PRP...   \n",
       "3         13  [(first, first, RB), (,, ,, ,), (you, you, PRP...   \n",
       "4         15  [(in, in, IN), (my, my, PRP$), (country, count...   \n",
       "\n",
       "                   misspelling  \\\n",
       "0     (beacause, beacause, NN)   \n",
       "1     (wallmart, wallmart, NN)   \n",
       "2             (dovn, dovn, NN)   \n",
       "3  (mircowave, mircowave, VBP)   \n",
       "4          (fitst, fitst, NNP)   \n",
       "\n",
       "                                            sentence  \\\n",
       "0  I organized the instructions by time, beacause...   \n",
       "1  next, you need to buy a box of tea in wallmart...   \n",
       "2  First, you should take some hot water, you can...   \n",
       "3  First, you should take some hot water, you can...   \n",
       "4                      Fitst, boil a water in a pot.   \n",
       "\n",
       "                          bigrams  \\\n",
       "0    [time beacause, beacause to]   \n",
       "1      [in wallmart, wallmart or]   \n",
       "2      [use dovn, dovn mircowave]   \n",
       "3  [dovn mircowave, mircowave or]   \n",
       "4                    [fitst boil]   \n",
       "\n",
       "                                 unigram_suggestions  \\\n",
       "0                        [(because,  1,  271323986)]   \n",
       "1                          [(walmart,  1,  2269839)]   \n",
       "2  [(dove,  1,  3253560), (donn,  1,  299470), (d...   \n",
       "3                        [(microwave,  1,  8934594)]   \n",
       "4  [(fist,  1,  7319405), (first,  1,  578161543)...   \n",
       "\n",
       "                                  bigram_suggestions bigram_correction  \\\n",
       "0  [(because to,  1,  3213023), (time because,  1...        because to   \n",
       "1  [(in wall art,  1,  99805), (wall art or,  1, ...       in wall art   \n",
       "2  [(down microwave,  2,  1960), (use down,  1,  ...    down microwave   \n",
       "3  [(microwave or,  1,  22584), (down microwave, ...      microwave or   \n",
       "4                          [(first boil,  1,  1366)]        first boil   \n",
       "\n",
       "  unigram_correction   final_correction     final_correction_POS  \n",
       "0            because          [because]          ([because], NN)  \n",
       "1            walmart        [wall, art]        ([wall, art], NN)  \n",
       "2               dove  [down, microwave]  ([down, microwave], NN)  \n",
       "3          microwave        [microwave]       ([microwave], VBP)  \n",
       "4               fist            [first]           ([first], NNP)  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create correction_POS column\n",
    "\n",
    "non_words2['final_correction_POS'] = list(zip(non_words2.final_correction, non_words2.misspelling.apply(lambda x: x[2])))\n",
    "non_words2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incorporating corrections into `pelic_df`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THIS WON'T WORK UNTIL EARLIER ISSUE RESOLVED AND `final_correction_POS` contains a tuple in each row (no lists in position x[0])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-52c7751d9fdd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Creating dictionary for mappying - key = incorrect spelling, value = correct spelling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmisspell_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnon_words2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_correction_POS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnon_words2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtok_lem_POS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mto_dict\u001b[0;34m(self, into)\u001b[0m\n\u001b[1;32m   1556\u001b[0m         \u001b[0;31m# GH16122\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1557\u001b[0m         \u001b[0minto_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstandardize_mapping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1558\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0minto_c\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1560\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "# Creating dictionary for mappying - key = incorrect spelling, value = correct spelling\n",
    "\n",
    "misspell_dict = pd.Series(non_words2.final_correction_POS.values,non_words2.tok_lem_POS).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misspell_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incorporating back into pelic_df\n",
    "\n",
    "pelic_df['tok_POS_corrected'] = pelic_df['tok_lem_POS'].apply\\\n",
    "(lambda row: [misspell_dict[(x[0].lower(),x[1],x[2])] if (x[0].lower(),x[1],x[2]) in misspell_dict else (x[0],x[2]) for x in row])\n",
    "\n",
    "# One minor issue is that this will make misspelled items lower case when originally upper case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking with 'becuase'\n",
    "\n",
    "print(pelic_df.loc[pelic_df.text.str.contains('becuase')].iloc[1,11]) #uncorrected\n",
    "print(pelic_df.loc[pelic_df.text.str.contains('becuase')].iloc[1,12]) #corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that many approrpriate corrections have been made, including _beccuase_ -> _because_ , _nise_ -> _nice_ , and _lovily_ -> _lovely_ .  \n",
    "Importantly, incorrect spellings that are actual words, e.g. _hem_ (should be _him_ in this case) are not corrected. In addition, as context is not considered, there will be some inaccuracies, e.g. _realy_ (marked as an adj) -> _real_ rather than _really_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pelic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out new PELIC_compiled.csv\n",
    "\n",
    "pelic_df.to_csv('PELIC_compiled_spellcorrected.csv', encoding='utf-8', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle new pelic_df dataframe\n",
    "\n",
    "pelic_df.to_pickle('pelic_spellcorrected.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If preferred, this entire spelling correctin process can also be applied to [`answer.csv`]() instead of `PELIC_compiled`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Corrected-spelling)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
