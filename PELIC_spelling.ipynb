{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PELIC spelling\n",
    "\n",
    "This notebook adds further processing to `PELIC_compiled.csv`  in the [`PELIC-dataset`](https://github.com/ELI-Data-Mining-Group/PELIC-dataset) repo by creating a column of tok_POS whose spelling has been automatically corrected.\n",
    "\n",
    "**Notebook contents:**\n",
    "- [Building `non_words_df`](#Building-non_words_df)\n",
    "- [Building `misspell_df`](#Building-misspell_df)\n",
    "- [Possible segmentation](#Applying-segmentation)\n",
    "- [Applying spelling correction](#Applying-spelling-correction)\n",
    "- [Incorporating corrections into `pelic_df`](#Incorporating-corrections-into-pelic_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building non_words_df\n",
    "In this section, we build a dataframe, `non_words_df`, which collects all of the non-words from the PELIC dataset (in `PELIC_compiled.csv`). The final dataframe has the following columns:\n",
    "- `non_word`: tuples with the non-words and their parts of speech\n",
    "- `sentence`: the complete sentence containing the non-word to provide context\n",
    "- `answer_id`: the id of the text they come from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "import nltk\n",
    "import random\n",
    "from pelitk import lex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anon_id</th>\n",
       "      <th>L1</th>\n",
       "      <th>gender</th>\n",
       "      <th>course_id</th>\n",
       "      <th>level_id</th>\n",
       "      <th>class_id</th>\n",
       "      <th>question_id</th>\n",
       "      <th>version</th>\n",
       "      <th>text_len</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tok_lem_POS</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eq0</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>Male</td>\n",
       "      <td>149</td>\n",
       "      <td>4</td>\n",
       "      <td>g</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>177</td>\n",
       "      <td>I met my friend Nife while I was studying in a...</td>\n",
       "      <td>[I, met, my, friend, Nife, while, I, was, stud...</td>\n",
       "      <td>((I, i, PRP), (met, meet, VBD), (my, my, PRP$)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>am8</td>\n",
       "      <td>Thai</td>\n",
       "      <td>Female</td>\n",
       "      <td>149</td>\n",
       "      <td>4</td>\n",
       "      <td>g</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>137</td>\n",
       "      <td>Ten years ago, I met a women on the train betw...</td>\n",
       "      <td>[Ten, years, ago, ,, I, met, a, women, on, the...</td>\n",
       "      <td>((Ten, ten, CD), (years, year, NNS), (ago, ago...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dk5</td>\n",
       "      <td>Turkish</td>\n",
       "      <td>Female</td>\n",
       "      <td>115</td>\n",
       "      <td>4</td>\n",
       "      <td>w</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>In my country we usually don't use tea bags. F...</td>\n",
       "      <td>[In, my, country, we, usually, do, n't, use, t...</td>\n",
       "      <td>((In, in, IN), (my, my, PRP$), (country, count...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dk5</td>\n",
       "      <td>Turkish</td>\n",
       "      <td>Female</td>\n",
       "      <td>115</td>\n",
       "      <td>4</td>\n",
       "      <td>w</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>I organized the instructions by time.</td>\n",
       "      <td>[I, organized, the, instructions, by, time, .]</td>\n",
       "      <td>((I, i, PRP), (organized, organize, VBD), (the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ad1</td>\n",
       "      <td>Korean</td>\n",
       "      <td>Female</td>\n",
       "      <td>115</td>\n",
       "      <td>4</td>\n",
       "      <td>w</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>59</td>\n",
       "      <td>First, prepare a port, loose tea, and cup.\\nSe...</td>\n",
       "      <td>[First, ,, prepare, a, port, ,, loose, tea, ,,...</td>\n",
       "      <td>((First, first, RB), (,, ,, ,), (prepare, prep...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          anon_id       L1  gender course_id level_id class_id question_id  \\\n",
       "answer_id                                                                    \n",
       "1             eq0   Arabic    Male       149        4        g           5   \n",
       "2             am8     Thai  Female       149        4        g           5   \n",
       "3             dk5  Turkish  Female       115        4        w          12   \n",
       "4             dk5  Turkish  Female       115        4        w          13   \n",
       "5             ad1   Korean  Female       115        4        w          12   \n",
       "\n",
       "          version  text_len  \\\n",
       "answer_id                     \n",
       "1               1       177   \n",
       "2               1       137   \n",
       "3               1        64   \n",
       "4               1         6   \n",
       "5               1        59   \n",
       "\n",
       "                                                        text  \\\n",
       "answer_id                                                      \n",
       "1          I met my friend Nife while I was studying in a...   \n",
       "2          Ten years ago, I met a women on the train betw...   \n",
       "3          In my country we usually don't use tea bags. F...   \n",
       "4                      I organized the instructions by time.   \n",
       "5          First, prepare a port, loose tea, and cup.\\nSe...   \n",
       "\n",
       "                                                      tokens  \\\n",
       "answer_id                                                      \n",
       "1          [I, met, my, friend, Nife, while, I, was, stud...   \n",
       "2          [Ten, years, ago, ,, I, met, a, women, on, the...   \n",
       "3          [In, my, country, we, usually, do, n't, use, t...   \n",
       "4             [I, organized, the, instructions, by, time, .]   \n",
       "5          [First, ,, prepare, a, port, ,, loose, tea, ,,...   \n",
       "\n",
       "                                                 tok_lem_POS  \n",
       "answer_id                                                     \n",
       "1          ((I, i, PRP), (met, meet, VBD), (my, my, PRP$)...  \n",
       "2          ((Ten, ten, CD), (years, year, NNS), (ago, ago...  \n",
       "3          ((In, in, IN), (my, my, PRP$), (country, count...  \n",
       "4          ((I, i, PRP), (organized, organize, VBD), (the...  \n",
       "5          ((First, first, RB), (,, ,, ,), (prepare, prep...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in PELIC_compiled.csv\n",
    "\n",
    "pelic_df = pd.read_csv(\"../PELIC-dataset/PELIC_compiled.csv\", index_col = 'answer_id', # answer_id is unique\n",
    "                      dtype = {'level_id':'object','question_id':'object','version':'object','course_id':'object'}, # str not ints\n",
    "                               converters={'tokens':literal_eval,'tok_lem_POS':literal_eval}) # read in as lists\n",
    "pelic_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The focus here is the `tok_lem_POS` column, but all columns will be kept as the entire df will be written out at the end of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating small dataframe to be used for finding non-words\n",
    "\n",
    "non_words = pelic_df[['text','tok_lem_POS']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** For spelling correction, it is necessary to decide what list of words will be used for determining if a word is real or not.\n",
    "\n",
    "Here, we use the [`SCOWL_condensed.txt`](https://github.com/ELI-Data-Mining-Group/PELIC-spelling/blob/master/SCOWL_condensed.txt) file which is a combination of wordlists available for download at http://wordlist.aspell.net/. We include items from all the dictionaries _except_ the abbreviations dictionary. For a detailed look at the compilation of this dictionary, please see the [SCOWL_wordlist](https://github.com/ELI-Data-Mining-Group/PELIC-spelling/blob/master/SCOWL_wordlist.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['resublime', 'nonabidingly', 'idocrases', 'Lebar', 'lotic']\n"
     ]
    }
   ],
   "source": [
    "#Reading in SCOWL_condensed as a set as a lookup list for spelling (500k words)\n",
    "\n",
    "scowl = set(open(\"SCOWL_condensed.txt\", \"r\").read().split('\\n'))\n",
    "print(random.sample(scowl,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "497552"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scowl = set([x.lower() for x in scowl])\n",
    "len(scowl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a list of words which should be considered words but which were previously being labelled as non-words. These items have been manually added to this list based on output later in this notebook. Most of these items are food items, names, or abbreviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233\n",
      "['adha', 'adj', 'ahamed', 'alaikum', 'anonurlpage', 'antiretroviral', 'arpa', 'beyonce', 'bibimbap', 'bio', 'biodiesel', 'bioethanol', 'bulgogi', 'bundang', 'cafe', 'carnaval', 'cds', 'cf', 'co', 'comscore', 'cyber', 'ddukboggi', 'def', 'dr', 'eg', 'eid', 'electrospray', 'entrees', 'erectus', 'etc', 'fiance', 'fiancee', 'fiter', 'fitir', 'fitr', 'fl', 'freediving', 'fukubukuro', 'geolinguist', 'hikikomori', 'hp', 'ibt', 'iq', 'iriver', 'jetta', 'jul', 'kabsa', 'kaled', 'kawader', 'km', 'leisureville', 'll', 'maamool', 'mayumi', 'mcdonalds', 'min', 'mongongo', 'nc', 'neuro', 'nian', 'notting', 'okroshka', 'onsen', 'pajeon', 'pbt', 'pc', 'pcs', 'pp', 'pudim', 'puket', 'samear', 'shui', 'sq', 'st', 'staycation', 'sth', 'taoyuan', 'toefl', 'trans', 'transgene', 'tv', 'unsub', 'va', 'vol', 'vs', 'webaholic', 'webaholics', 'webaholism', 'wenjing', 'woong', 'yaoming', 'ying', 'yingdong', 'yugong', 'yuval', 'zi', 'abha', 'achuar', 'ae', 'afandi', 'aladha', 'alfater', 'alfeter', 'alfetr', 'alfter', 'alftr', 'ansan', 'apci', 'arial', 'ayumu', 'aziz', 'bartercard', 'bbc', 'bbq', 'beckham', 'bennigan', 'bmi', 'burkina', 'busan', 'camela', 'caral', 'ceos', 'cmu', 'cnn', 'cpu', 'cyworld', 'daegu', 'dammam', 'dc', 'dvd', 'dvds', 'eaid', 'esl', 'etat', 'ets', 'etsuko', 'eun', 'fargana', 'fda', 'federer', 'feng', 'frisbee', 'gaviotas', 'gmo', 'gpa', 'gunsan', 'hadza', 'hanshin', 'hd', 'hofstra', 'hsk', 'hyun', 'iaad', 'ielts', 'incheon', 'jang', 'japchae', 'jazan', 'jeddah', 'jeju', 'jiri', 'jong', 'jr', 'junine', 'kabsah', 'kakike', 'kanye', 'kimbab', 'kmt', 'koreas', 'krabi', 'ksa', 'kyung', 'lcd', 'lelan', 'lenovo', 'lg', 'lippman', 'longman', 'matsui', 'meltzer', 'mishori', 'mlb', 'morakot', 'mp', 'msn', 'mt', 'myung', 'nadal', 'najran', 'namwon', 'nano', 'nba', 'neimark', 'ngondo', 'noha', 'noor', 'nouwen', 'nyc', 'obon', 'occitane', 'oishi', 'paterno', 'pattak', 'pattee', 'pausini', 'pnc', 'qdoba', 'ritu', 'roh', 'ronaldo', 'rsa', 'sakarya', 'saletan', 'saudia', 'sawiris', 'sc', 'sf', 'southside', 'sujin', 'tgif', 'toeic', 'upmc', 'usd', 'usda', 'vaio', 'wto', 'xbox', 'yokoso', 'yut', 'zawia', 'zhou', 'zong']\n"
     ]
    }
   ],
   "source": [
    "scowl_supp = open(\"SCOWL_supp.txt\", \"r\").read().split(',')\n",
    "scowl_supp = [x[2:-1] for x in scowl_supp]\n",
    "print(len(scowl_supp))\n",
    "print(scowl_supp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py:5303: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "# Lower case all toks\n",
    "\n",
    "non_words.tok_lem_POS = non_words.tok_lem_POS.apply(lambda row: [(x[0].lower(),x[1],x[2]) for x in row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find non-words\n",
    "\n",
    "def spell_check(tok_lem_POS_list):\n",
    "    word_list = scowl # Choose word_list here. Default is scowl described above.\n",
    "    not_in_word_list = []\n",
    "    for tok_lem_POS in tok_lem_POS_list:\n",
    "        if tok_lem_POS[0] not in word_list and tok_lem_POS[0] not in scowl_supp:\n",
    "            not_in_word_list.append(tok_lem_POS)\n",
    "    return not_in_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# Apply spell check function to find all misspelled-words. \n",
    "\n",
    "non_words['misspelled_words'] = non_words.tok_lem_POS.apply(spell_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tok_lem_POS</th>\n",
       "      <th>misspelled_words</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I met my friend Nife while I was studying in a...</td>\n",
       "      <td>[(i, i, PRP), (met, meet, VBD), (my, my, PRP$)...</td>\n",
       "      <td>[(., ., .), (., ., .), (., ., .), (;, ;, :), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ten years ago, I met a women on the train betw...</td>\n",
       "      <td>[(ten, ten, CD), (years, year, NNS), (ago, ago...</td>\n",
       "      <td>[(,, ,, ,), (,, ,, ,), (., ., .), (;, ;, :), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In my country we usually don't use tea bags. F...</td>\n",
       "      <td>[(in, in, IN), (my, my, PRP$), (country, count...</td>\n",
       "      <td>[(., ., .), (,, ,, ,), (., ., .), (., ., .), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I organized the instructions by time.</td>\n",
       "      <td>[(i, i, PRP), (organized, organize, VBD), (the...</td>\n",
       "      <td>[(., ., .)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>First, prepare a port, loose tea, and cup.\\nSe...</td>\n",
       "      <td>[(first, first, RB), (,, ,, ,), (prepare, prep...</td>\n",
       "      <td>[(,, ,, ,), (,, ,, ,), (,, ,, ,), (., ., .), (...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        text  \\\n",
       "answer_id                                                      \n",
       "1          I met my friend Nife while I was studying in a...   \n",
       "2          Ten years ago, I met a women on the train betw...   \n",
       "3          In my country we usually don't use tea bags. F...   \n",
       "4                      I organized the instructions by time.   \n",
       "5          First, prepare a port, loose tea, and cup.\\nSe...   \n",
       "\n",
       "                                                 tok_lem_POS  \\\n",
       "answer_id                                                      \n",
       "1          [(i, i, PRP), (met, meet, VBD), (my, my, PRP$)...   \n",
       "2          [(ten, ten, CD), (years, year, NNS), (ago, ago...   \n",
       "3          [(in, in, IN), (my, my, PRP$), (country, count...   \n",
       "4          [(i, i, PRP), (organized, organize, VBD), (the...   \n",
       "5          [(first, first, RB), (,, ,, ,), (prepare, prep...   \n",
       "\n",
       "                                            misspelled_words  \n",
       "answer_id                                                     \n",
       "1          [(., ., .), (., ., .), (., ., .), (;, ;, :), (...  \n",
       "2          [(,, ,, ,), (,, ,, ,), (., ., .), (;, ;, :), (...  \n",
       "3          [(., ., .), (,, ,, ,), (., ., .), (., ., .), (...  \n",
       "4                                                [(., ., .)]  \n",
       "5          [(,, ,, ,), (,, ,, ,), (,, ,, ,), (., ., .), (...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_words.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding context to the dataframe\n",
    "Seeing the mistakes in the context of a sentence will allow for better manual checking if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# Sent-tokenizing the text\n",
    "\n",
    "non_words['sentence'] = non_words['text'].apply(lambda x: nltk.sent_tokenize(x))\n",
    "\n",
    "# And delete text column which is no longer needed\n",
    "\n",
    "del non_words['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1182\n",
      "['half-time', 'upper-middle', 'lower-back', 'light-brown', 'one-parent', 'operating-room', 'post-impressionist', 'intra-abdominal', 'role-play', 'cost-effective']\n"
     ]
    }
   ],
   "source": [
    "# Checking for hyphenated words tagged as misspellings because SCOWL doesn't contain hypenated words\n",
    "\n",
    "hyphenated = set([x[0] for x in [x for y in non_words.misspelled_words.to_list() for x in y] if '-' in x[0]])\n",
    "print(len(hyphenated))\n",
    "print(list(hyphenated)[:10])\n",
    "\n",
    "# These need to be removed from the non-words dataframe if composed of valid words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['', \"'\"],\n",
       " ['', '***', '****'],\n",
       " ['', '+'],\n",
       " ['', '.'],\n",
       " ['',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  ''],\n",
       " [\"'\", ''],\n",
       " ['.', ''],\n",
       " ['/', ''],\n",
       " ['\\\\\\\\', ''],\n",
       " ['^', '^'],\n",
       " ['al', 'qaida'],\n",
       " ['austro', 'hungarian'],\n",
       " ['cd', 'rom'],\n",
       " ['co', 'authored'],\n",
       " ['co', 'ed'],\n",
       " ['co', 'educational'],\n",
       " ['co', 'exist'],\n",
       " ['co', 'existence'],\n",
       " ['co', 'founded'],\n",
       " ['co', 'founder'],\n",
       " ['co', 'founders'],\n",
       " ['co', 'host'],\n",
       " ['co', 'op'],\n",
       " ['co', 'operate'],\n",
       " ['co', 'operation'],\n",
       " ['co', 'pay'],\n",
       " ['co', 'pilot'],\n",
       " ['co', 'sleeping'],\n",
       " ['co', 'star'],\n",
       " ['co', 'worker'],\n",
       " ['co', 'workers'],\n",
       " ['co', 'written'],\n",
       " ['co', 'wrote'],\n",
       " ['mah', 'jong'],\n",
       " ['mid', '80s'],\n",
       " ['pay', 'tv'],\n",
       " ['roly', 'poly'],\n",
       " ['socio', 'cultural'],\n",
       " ['socio', 'economic'],\n",
       " ['trans', 'fat'],\n",
       " ['vis', 'a', 'vis'],\n",
       " ['wal', 'mart']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyphenated items whose components are not in scowl - possible misspellings or punctuation strings\n",
    "\n",
    "sorted([y for y in [x.split('-') for x in hyphenated] if y[0] not in scowl or y[1] not in scowl])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After manual checking, all the hypenated words are punctuation, real words (or true productive use of affixes) and can be removed from the non-words df.\n",
    "\n",
    "The following two cells \n",
    "1. remove all the hypenated words from the dataframe\n",
    "2. remove all words that don't contain a letter\n",
    "\n",
    "However, as all hyphenated word are fine, we will instead just eliminate all words that are not purely composed of letters. This will have the effect of removing the following categories from the dataframe:\n",
    "- punctuation\n",
    "- hyphenated words (e.g. well-known)\n",
    "- contractions (e.g. 'll, 've)\n",
    "- years (e.g. 1950s)\n",
    "- ordinals (e.g. 1st, 2nd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing hypenated words\n",
    "\n",
    "# non_words.misspelled_words = non_words.misspelled_words.apply(lambda row: [x for x in row if x[0] not in hyphenated])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing items that are only numbers or punctuation\n",
    "# .isalpha() cannot be used without 'any' as this also removes hyphenated words\n",
    "\n",
    "# non_words.misspelled_words = non_words.misspelled_words.apply(lambda row: [x for x in row if any(y.isalpha() for y in x[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680998\n",
      "20394\n"
     ]
    }
   ],
   "source": [
    "# Checking initial length of non_words list\n",
    "\n",
    "print(len([x for y in non_words.misspelled_words.to_list() for x in y]))\n",
    "print(len(set([x for y in non_words.misspelled_words.to_list() for x in y])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py:5303: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "# Removing items that are not purely alpha\n",
    "\n",
    "non_words.misspelled_words = non_words.misspelled_words.apply(lambda row: [x for x in row if x[0].isalpha()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26750\n",
      "15788\n"
     ]
    }
   ],
   "source": [
    "# Checking affect of removal\n",
    "\n",
    "print(len([x for y in non_words.misspelled_words.to_list() for x in y]))\n",
    "print(len(set([x for y in non_words.misspelled_words.to_list() for x in y])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing proper names - NNP, NNPS\n",
    "\n",
    "# non_words.misspelled_words = non_words.misspelled_words.apply(lambda row: [x for x in row if x[2] != 'NNP' and x[1] != 'NNPS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After manual checking, it was decided to keep in items tagged as NNP and NNPS as some items were in fact mistagged and were general capitalized nouns (NN) which were misspelled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26750\n",
      "15788\n"
     ]
    }
   ],
   "source": [
    "# Checking affect of removal\n",
    "\n",
    "print(len([x for y in non_words.misspelled_words.to_list() for x in y]))\n",
    "print(len(set([x for y in non_words.misspelled_words.to_list() for x in y])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing all words with length 1\n",
    "\n",
    "non_words.misspelled_words = non_words.misspelled_words.apply(lambda row: [x for x in row if len(x[0]) > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26738\n",
      "15779\n"
     ]
    }
   ],
   "source": [
    "# Checking affect of removal\n",
    "\n",
    "print(len([x for y in non_words.misspelled_words.to_list() for x in y]))\n",
    "print(len(set([x for y in non_words.misspelled_words.to_list() for x in y])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tok_lem_POS</th>\n",
       "      <th>misspelled_words</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[(i, i, PRP), (met, meet, VBD), (my, my, PRP$)...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[I met my friend Nife while I was studying in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[(ten, ten, CD), (years, year, NNS), (ago, ago...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Ten years ago, I met a women on the train bet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[(in, in, IN), (my, my, PRP$), (country, count...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[In my country we usually don't use tea bags.,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[(i, i, PRP), (organized, organize, VBD), (the...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[I organized the instructions by time.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[(first, first, RB), (,, ,, ,), (prepare, prep...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[First, prepare a port, loose tea, and cup., S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tok_lem_POS misspelled_words  \\\n",
       "answer_id                                                                       \n",
       "1          [(i, i, PRP), (met, meet, VBD), (my, my, PRP$)...               []   \n",
       "2          [(ten, ten, CD), (years, year, NNS), (ago, ago...               []   \n",
       "3          [(in, in, IN), (my, my, PRP$), (country, count...               []   \n",
       "4          [(i, i, PRP), (organized, organize, VBD), (the...               []   \n",
       "5          [(first, first, RB), (,, ,, ,), (prepare, prep...               []   \n",
       "\n",
       "                                                    sentence  \n",
       "answer_id                                                     \n",
       "1          [I met my friend Nife while I was studying in ...  \n",
       "2          [Ten years ago, I met a women on the train bet...  \n",
       "3          [In my country we usually don't use tea bags.,...  \n",
       "4                    [I organized the instructions by time.]  \n",
       "5          [First, prepare a port, loose tea, and cup., S...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_words.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new dataframe so that each misspelling token is a separate row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing rows with no misspellings\n",
    "\n",
    "non_words2 = non_words.loc[non_words.misspelled_words.str.len() > 0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploding the lists in misspelled words so that each misspelling gets its own row\n",
    "\n",
    "non_words2 = non_words2.explode('misspelled_words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping the answer_id (which is no longer unique) as a separate column\n",
    "\n",
    "non_words2 = non_words2.reset_index(drop = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding a bigrams column, i.e. one token left and right of the misspelled word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a tokenized version of the sentence without punctuation and with the index for each token\n",
    "\n",
    "non_words2['enumerated'] = non_words2.sentence.apply(str).apply(lex.re_tokenize).apply(lambda x: list(enumerate(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to extract the bigrams (1 word either side of misspelling)\n",
    "\n",
    "def get_bigrams(misspelled_word, enumerated_list):\n",
    "    if len(enumerated_list) <2:\n",
    "        return []\n",
    "    for tup in enumerated_list:\n",
    "        if tup[1] == misspelled_word[0]:\n",
    "            if tup[0] == 0:\n",
    "                bigram = ' '.join([x[1] for x in (enumerated_list[tup[0]],enumerated_list[tup[0]+1])])\n",
    "                return [bigram]\n",
    "            if tup[0] == len(enumerated_list)-1:\n",
    "                bigram = ' '.join([x[1] for x in (enumerated_list[tup[0]-1],enumerated_list[tup[0]])])\n",
    "                return [bigram]\n",
    "            else:\n",
    "                bigram1 = ' '.join([x[1] for x in (enumerated_list[tup[0]-1],enumerated_list[tup[0]])])\n",
    "                bigram2 = ' '.join([x[1] for x in (enumerated_list[tup[0]],enumerated_list[tup[0]+1])])\n",
    "                return [bigram1, bigram2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'every'), (1, 'paragragh'), (2, 's'), (3, 'instructions'), (4, 'depend'), (5, 'on'), (6, 'a'), (7, 'main'), (8, 'idea'), (9, 'but'), (10, 'i'), (11, 'often'), (12, 'organize'), (13, 'the'), (14, 'instruction'), (15, 'by'), (16, 'time'), (17, 'because'), (18, 'i'), (19, 'think'), (20, 'organized'), (21, 'by'), (22, 'time'), (23, 'is'), (24, 'good'), (25, 'looking'), (26, 'for'), (27, 'readers')]\n",
      "\n",
      " ('every', 'every', 'DT') ('depend', 'depend', 'VBP') ('readers', 'reader', 'NNS')\n"
     ]
    }
   ],
   "source": [
    "# Testing the function\n",
    "\n",
    "test_list = non_words2.iloc[5,4]\n",
    "print(test_list)\n",
    "\n",
    "first_item = non_words2.iloc[5,1][0] # first item in list\n",
    "middle_item = non_words2.iloc[5,1][4] # item in in middle of list\n",
    "last_item = non_words2.iloc[5,1][29] # item at end of list\n",
    "print('\\n',first_item, middle_item, last_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['every paragragh']\n",
      "['instructions depend', 'depend on']\n",
      "['for readers']\n"
     ]
    }
   ],
   "source": [
    "print(get_bigrams(first_item,test_list))\n",
    "print(get_bigrams(middle_item,test_list))\n",
    "print(get_bigrams(last_item,test_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the above function\n",
    "non_words2['bigrams'] = non_words2[['misspelled_words','enumerated']].apply(lambda x: get_bigrams(x[0],x[1]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'reading'),\n",
       " (1, 'word'),\n",
       " (2, 'magazine'),\n",
       " (3, 'n'),\n",
       " (4, 'njaketribune'),\n",
       " (5, 'is'),\n",
       " (6, 'the'),\n",
       " (7, 'most'),\n",
       " (8, 'popular'),\n",
       " (9, 'magazine'),\n",
       " (10, 'in'),\n",
       " (11, 'pittsburgh'),\n",
       " (12, 'reading'),\n",
       " (13, 'complex'),\n",
       " (14, 'n'),\n",
       " (15, 'nword'),\n",
       " (16, 'complex'),\n",
       " (17, 'n'),\n",
       " (18, 'nwhat'),\n",
       " (19, 'a'),\n",
       " (20, 'compplex'),\n",
       " (21, 'system'),\n",
       " (22, 'reading'),\n",
       " (23, 'scene'),\n",
       " (24, 'n'),\n",
       " (25, 'nthe'),\n",
       " (26, 'mountain'),\n",
       " (27, 'has'),\n",
       " (28, 'z'),\n",
       " (29, 'beautiful'),\n",
       " (30, 'scene')]"
      ]
     },
     "execution_count": 546,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_words2.loc[non_words2['bigrams'].isnull()].iloc[1,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer_id</th>\n",
       "      <th>tok_lem_POS</th>\n",
       "      <th>misspelled_words</th>\n",
       "      <th>sentence</th>\n",
       "      <th>enumerated</th>\n",
       "      <th>bigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>[(to, to, TO), (fail, fail, VB), (a, a, DT), (...</td>\n",
       "      <td>(failling, failling, NN)</td>\n",
       "      <td>[To Fail a Test\\n\\nFailling on a test is simpl...</td>\n",
       "      <td>[(0, to), (1, fail), (2, a), (3, test), (4, n)...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>498</td>\n",
       "      <td>[(reading, read, VBG), (1, 1, CD), (., ., .), ...</td>\n",
       "      <td>(jaketribune, jaketribune, NNP)</td>\n",
       "      <td>[Reading 1., Word : magazine\\n\\nJakeTribune is...</td>\n",
       "      <td>[(0, reading), (1, word), (2, magazine), (3, n...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>596</td>\n",
       "      <td>[(commercial, commercial, JJ), (:, :, :), (tod...</td>\n",
       "      <td>(usualy, usualy, VB)</td>\n",
       "      <td>[commercial:\\nToday there are many methods for...</td>\n",
       "      <td>[(0, commercial), (1, ntoday), (2, there), (3,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>856</td>\n",
       "      <td>[(the, the, DT), (first, first, JJ), (short, s...</td>\n",
       "      <td>(poète, poète, NNP)</td>\n",
       "      <td>[The first short movie of the American directo...</td>\n",
       "      <td>[(0, the), (1, first), (2, short), (3, movie),...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>991</td>\n",
       "      <td>[(comparing, compare, VBG), (guardians, guardi...</td>\n",
       "      <td>(considera, considera, NNP)</td>\n",
       "      <td>[Comparing Guardians\\n\\nGuardians ANON_NAME_0 ...</td>\n",
       "      <td>[(0, comparing), (1, guardians), (2, n), (3, n...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26017</th>\n",
       "      <td>47416</td>\n",
       "      <td>[(lithium, lithium, NN), ((, (, (), (n, n, JJ)...</td>\n",
       "      <td>(electolyte, electolyte, VBP)</td>\n",
       "      <td>[lithium(n, uncountable) a soft silver-white m...</td>\n",
       "      <td>[(0, lithium), (1, n), (2, uncountable), (3, a...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26400</th>\n",
       "      <td>48020</td>\n",
       "      <td>[(dear, dear, IN), (mr., mr., NNP), (anon_name...</td>\n",
       "      <td>(monira, monira, NNP)</td>\n",
       "      <td>[Dear Mr. ANON_NAME_0\\nAs I am a pulmonary reh...</td>\n",
       "      <td>[(0, dear), (1, mr), (2, anon), (3, name), (4,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26574</th>\n",
       "      <td>48260</td>\n",
       "      <td>[(dear, dear, NNP), (xavi, xavi, NNP), (,, ,, ...</td>\n",
       "      <td>(fadel, fadel, NNP)</td>\n",
       "      <td>[Dear Xavi,\\n\\nI am very delighted to hear fro...</td>\n",
       "      <td>[(0, dear), (1, xavi), (2, n), (3, ni), (4, am...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26604</th>\n",
       "      <td>48285</td>\n",
       "      <td>[(dear, dear, IN), (mr., mr., NNP), (xavi, xav...</td>\n",
       "      <td>(abdulkarim, abdulkarim, NNP)</td>\n",
       "      <td>[Dear Mr. xavi\\n\\nThank you for your email and...</td>\n",
       "      <td>[(0, dear), (1, mr), (2, xavi), (3, n), (4, nt...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26651</th>\n",
       "      <td>48343</td>\n",
       "      <td>[(london, london, NNP), (2012, 2012, CD), (:, ...</td>\n",
       "      <td>(bcc, bcc, NNP)</td>\n",
       "      <td>[London 2012: Michael Phelps focused on pool a...</td>\n",
       "      <td>[(0, london), (1, michael), (2, phelps), (3, f...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>570 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       answer_id                                        tok_lem_POS  \\\n",
       "29            29  [(to, to, TO), (fail, fail, VB), (a, a, DT), (...   \n",
       "433          498  [(reading, read, VBG), (1, 1, CD), (., ., .), ...   \n",
       "470          596  [(commercial, commercial, JJ), (:, :, :), (tod...   \n",
       "644          856  [(the, the, DT), (first, first, JJ), (short, s...   \n",
       "731          991  [(comparing, compare, VBG), (guardians, guardi...   \n",
       "...          ...                                                ...   \n",
       "26017      47416  [(lithium, lithium, NN), ((, (, (), (n, n, JJ)...   \n",
       "26400      48020  [(dear, dear, IN), (mr., mr., NNP), (anon_name...   \n",
       "26574      48260  [(dear, dear, NNP), (xavi, xavi, NNP), (,, ,, ...   \n",
       "26604      48285  [(dear, dear, IN), (mr., mr., NNP), (xavi, xav...   \n",
       "26651      48343  [(london, london, NNP), (2012, 2012, CD), (:, ...   \n",
       "\n",
       "                      misspelled_words  \\\n",
       "29            (failling, failling, NN)   \n",
       "433    (jaketribune, jaketribune, NNP)   \n",
       "470               (usualy, usualy, VB)   \n",
       "644                (poète, poète, NNP)   \n",
       "731        (considera, considera, NNP)   \n",
       "...                                ...   \n",
       "26017    (electolyte, electolyte, VBP)   \n",
       "26400            (monira, monira, NNP)   \n",
       "26574              (fadel, fadel, NNP)   \n",
       "26604    (abdulkarim, abdulkarim, NNP)   \n",
       "26651                  (bcc, bcc, NNP)   \n",
       "\n",
       "                                                sentence  \\\n",
       "29     [To Fail a Test\\n\\nFailling on a test is simpl...   \n",
       "433    [Reading 1., Word : magazine\\n\\nJakeTribune is...   \n",
       "470    [commercial:\\nToday there are many methods for...   \n",
       "644    [The first short movie of the American directo...   \n",
       "731    [Comparing Guardians\\n\\nGuardians ANON_NAME_0 ...   \n",
       "...                                                  ...   \n",
       "26017  [lithium(n, uncountable) a soft silver-white m...   \n",
       "26400  [Dear Mr. ANON_NAME_0\\nAs I am a pulmonary reh...   \n",
       "26574  [Dear Xavi,\\n\\nI am very delighted to hear fro...   \n",
       "26604  [Dear Mr. xavi\\n\\nThank you for your email and...   \n",
       "26651  [London 2012: Michael Phelps focused on pool a...   \n",
       "\n",
       "                                              enumerated bigrams  \n",
       "29     [(0, to), (1, fail), (2, a), (3, test), (4, n)...    None  \n",
       "433    [(0, reading), (1, word), (2, magazine), (3, n...    None  \n",
       "470    [(0, commercial), (1, ntoday), (2, there), (3,...    None  \n",
       "644    [(0, the), (1, first), (2, short), (3, movie),...    None  \n",
       "731    [(0, comparing), (1, guardians), (2, n), (3, n...    None  \n",
       "...                                                  ...     ...  \n",
       "26017  [(0, lithium), (1, n), (2, uncountable), (3, a...    None  \n",
       "26400  [(0, dear), (1, mr), (2, anon), (3, name), (4,...    None  \n",
       "26574  [(0, dear), (1, xavi), (2, n), (3, ni), (4, am...    None  \n",
       "26604  [(0, dear), (1, mr), (2, xavi), (3, n), (4, nt...    None  \n",
       "26651  [(0, london), (1, michael), (2, phelps), (3, f...    None  \n",
       "\n",
       "[570 rows x 6 columns]"
      ]
     },
     "execution_count": 544,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_words2.loc[non_words2['bigrams'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer_id</th>\n",
       "      <th>tok_lem_POS</th>\n",
       "      <th>misspelled_words</th>\n",
       "      <th>sentence</th>\n",
       "      <th>enumerated</th>\n",
       "      <th>bigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>[(i, i, PRP), (organized, organize, VBD), (the...</td>\n",
       "      <td>(beacause, beacause, NN)</td>\n",
       "      <td>[I organized the instructions by time, beacaus...</td>\n",
       "      <td>[(0, i), (1, organized), (2, the), (3, instruc...</td>\n",
       "      <td>[time beacause, beacause to]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>[(to, to, TO), (make, make, VB), (tea, tea, NN...</td>\n",
       "      <td>(wallmart, wallmart, NN)</td>\n",
       "      <td>[To make tea, nothing is easier, even if somet...</td>\n",
       "      <td>[(0, to), (1, make), (2, tea), (3, nothing), (...</td>\n",
       "      <td>[in wallmart, wallmart or]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>[(first, first, RB), (,, ,, ,), (you, you, PRP...</td>\n",
       "      <td>(dovn, dovn, NN)</td>\n",
       "      <td>[First, you should take some hot water, you ca...</td>\n",
       "      <td>[(0, first), (1, you), (2, should), (3, take),...</td>\n",
       "      <td>[use dovn, dovn mircowave]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>[(first, first, RB), (,, ,, ,), (you, you, PRP...</td>\n",
       "      <td>(mircowave, mircowave, VBP)</td>\n",
       "      <td>[First, you should take some hot water, you ca...</td>\n",
       "      <td>[(0, first), (1, you), (2, should), (3, take),...</td>\n",
       "      <td>[dovn mircowave, mircowave or]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>[(in, in, IN), (my, my, PRP$), (country, count...</td>\n",
       "      <td>(fitst, fitst, NNP)</td>\n",
       "      <td>[In my country, make a tea is very easy becaus...</td>\n",
       "      <td>[(0, in), (1, my), (2, country), (3, make), (4...</td>\n",
       "      <td>[bags fitst, fitst boil]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   answer_id                                        tok_lem_POS  \\\n",
       "0          8  [(i, i, PRP), (organized, organize, VBD), (the...   \n",
       "1         11  [(to, to, TO), (make, make, VB), (tea, tea, NN...   \n",
       "2         13  [(first, first, RB), (,, ,, ,), (you, you, PRP...   \n",
       "3         13  [(first, first, RB), (,, ,, ,), (you, you, PRP...   \n",
       "4         15  [(in, in, IN), (my, my, PRP$), (country, count...   \n",
       "\n",
       "              misspelled_words  \\\n",
       "0     (beacause, beacause, NN)   \n",
       "1     (wallmart, wallmart, NN)   \n",
       "2             (dovn, dovn, NN)   \n",
       "3  (mircowave, mircowave, VBP)   \n",
       "4          (fitst, fitst, NNP)   \n",
       "\n",
       "                                            sentence  \\\n",
       "0  [I organized the instructions by time, beacaus...   \n",
       "1  [To make tea, nothing is easier, even if somet...   \n",
       "2  [First, you should take some hot water, you ca...   \n",
       "3  [First, you should take some hot water, you ca...   \n",
       "4  [In my country, make a tea is very easy becaus...   \n",
       "\n",
       "                                          enumerated  \\\n",
       "0  [(0, i), (1, organized), (2, the), (3, instruc...   \n",
       "1  [(0, to), (1, make), (2, tea), (3, nothing), (...   \n",
       "2  [(0, first), (1, you), (2, should), (3, take),...   \n",
       "3  [(0, first), (1, you), (2, should), (3, take),...   \n",
       "4  [(0, in), (1, my), (2, country), (3, make), (4...   \n",
       "\n",
       "                          bigrams  \n",
       "0    [time beacause, beacause to]  \n",
       "1      [in wallmart, wallmart or]  \n",
       "2      [use dovn, dovn mircowave]  \n",
       "3  [dovn mircowave, mircowave or]  \n",
       "4        [bags fitst, fitst boil]  "
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking final non_words2 dataframe\n",
    "\n",
    "non_words2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26738\n",
      "15779\n"
     ]
    }
   ],
   "source": [
    "# Total number of non-words (tokens)\n",
    "print(len(non_words2))\n",
    "\n",
    "# Total number of non-words (types)\n",
    "print(non_words2.misspelled_words.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a dataframe of misspellings\n",
    "In the `non-words2` dataframe above, each row is an occurrence of a misspelling (i.e. _tokens_ ). We also want a dataframe where each row is a misspelling _type_ with frequency information attached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-542-78d57d9dcdd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtotal_misspellings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnon_words2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'misspelled_words'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtotal_bigrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnon_words2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bigrams'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#flattened list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtotal_bigrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtotal_bigrams\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-542-78d57d9dcdd7>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtotal_misspellings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnon_words2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'misspelled_words'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtotal_bigrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnon_words2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bigrams'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#flattened list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtotal_bigrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtotal_bigrams\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "# Gathering the total misspellings and bigrams\n",
    "\n",
    "total_misspellings = [x for x in non_words2['misspelled_words']]\n",
    "total_bigrams = [x for y in non_words2['bigrams'] for x in y] #flattened list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['time beacause', 'beacause to'],\n",
       " ['in wallmart', 'wallmart or'],\n",
       " ['use dovn', 'dovn mircowave'],\n",
       " ['dovn mircowave', 'mircowave or'],\n",
       " ['bags fitst', 'fitst boil']]"
      ]
     },
     "execution_count": 541,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_bigrams[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating frequency dictionaries for unigrams and bigrams\n",
    "\n",
    "misspell_freq_dict = {}\n",
    "for word in total_misspellings:\n",
    "    if word not in misspell_freq_dict:\n",
    "        misspell_freq_dict[word] = 1\n",
    "    else:\n",
    "        misspell_freq_dict[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misspell_bigram_freq_dict = {}\n",
    "for bigram in total_misspellings:\n",
    "    if word not in misspell_freq_dict:\n",
    "        misspell_freq_dict[word] = 1\n",
    "    else:\n",
    "        misspell_freq_dict[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tinniger', 'tinniger', 'NN'), ('jonhn', 'jonhn', 'NNP'), ('nitice', 'nitice', 'VB'), ('metala', 'metala', 'NN'), ('tohe', 'tohe', 'JJ')]\n"
     ]
    }
   ],
   "source": [
    "print(random.sample(list(misspell_freq_dict),5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15779"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove duplicates\n",
    "\n",
    "final_misspellings = sorted(list(set(total_misspellings)))\n",
    "len(final_misspellings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aa</td>\n",
       "      <td>aa</td>\n",
       "      <td>NNP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aa</td>\n",
       "      <td>aa</td>\n",
       "      <td>VB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aabout</td>\n",
       "      <td>aabout</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aad</td>\n",
       "      <td>aad</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aain</td>\n",
       "      <td>aain</td>\n",
       "      <td>VBP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0       1    2\n",
       "0      aa      aa  NNP\n",
       "1      aa      aa   VB\n",
       "2  aabout  aabout   IN\n",
       "3     aad     aad   JJ\n",
       "4    aain    aain  VBP"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Constructing misspell_df\n",
    "\n",
    "misspell_df = pd.DataFrame(final_misspellings)\n",
    "misspell_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming columns to match other DataFrames in this notebook\n",
    "\n",
    "misspell_df.rename(columns = {0: 'misspelling',1:'lemma',2:'POS'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>misspelling</th>\n",
       "      <th>lemma</th>\n",
       "      <th>POS</th>\n",
       "      <th>tok_lem_POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aa</td>\n",
       "      <td>aa</td>\n",
       "      <td>NNP</td>\n",
       "      <td>(aa, aa, NNP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aa</td>\n",
       "      <td>aa</td>\n",
       "      <td>VB</td>\n",
       "      <td>(aa, aa, VB)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aabout</td>\n",
       "      <td>aabout</td>\n",
       "      <td>IN</td>\n",
       "      <td>(aabout, aabout, IN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aad</td>\n",
       "      <td>aad</td>\n",
       "      <td>JJ</td>\n",
       "      <td>(aad, aad, JJ)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aain</td>\n",
       "      <td>aain</td>\n",
       "      <td>VBP</td>\n",
       "      <td>(aain, aain, VBP)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  misspelling   lemma  POS           tok_lem_POS\n",
       "0          aa      aa  NNP         (aa, aa, NNP)\n",
       "1          aa      aa   VB          (aa, aa, VB)\n",
       "2      aabout  aabout   IN  (aabout, aabout, IN)\n",
       "3         aad     aad   JJ        (aad, aad, JJ)\n",
       "4        aain    aain  VBP     (aain, aain, VBP)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recreating tok_lem_POS column to match dictionary\n",
    "\n",
    "misspell_df['tok_lem_POS'] = list(zip(misspell_df.misspelling, misspell_df.lemma, misspell_df.POS))\n",
    "misspell_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping dictionary to DataFrame\n",
    "\n",
    "misspell_df['freq'] = misspell_df['tok_lem_POS'].map(misspell_freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting by frequency\n",
    "\n",
    "misspell_df = misspell_df.sort_values(by=['freq'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>misspelling</th>\n",
       "      <th>lemma</th>\n",
       "      <th>POS</th>\n",
       "      <th>tok_lem_POS</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alot</td>\n",
       "      <td>alot</td>\n",
       "      <td>NN</td>\n",
       "      <td>(alot, alot, NN)</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>studing</td>\n",
       "      <td>studing</td>\n",
       "      <td>VBG</td>\n",
       "      <td>(studing, studing, VBG)</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tofel</td>\n",
       "      <td>tofel</td>\n",
       "      <td>NNP</td>\n",
       "      <td>(tofel, tofel, NNP)</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eilperin</td>\n",
       "      <td>eilperin</td>\n",
       "      <td>NNP</td>\n",
       "      <td>(eilperin, eilperin, NNP)</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>seollal</td>\n",
       "      <td>seollal</td>\n",
       "      <td>NNP</td>\n",
       "      <td>(seollal, seollal, NNP)</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  misspelling     lemma  POS                tok_lem_POS  freq\n",
       "0        alot      alot   NN           (alot, alot, NN)   127\n",
       "1     studing   studing  VBG    (studing, studing, VBG)    74\n",
       "2       tofel     tofel  NNP        (tofel, tofel, NNP)    66\n",
       "3    eilperin  eilperin  NNP  (eilperin, eilperin, NNP)    48\n",
       "4     seollal   seollal  NNP    (seollal, seollal, NNP)    47"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Resetting index\n",
    "misspell_df = misspell_df.reset_index(drop = True)\n",
    "misspell_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scowl_supp\n",
    "The following is the basis for the 'scowl_supp' list used earlier. Here, errors with a frequency of 10 or more were manually checked, and if determined to be a real word, were added to the scowl_supp list. There were originally 267 items which met this criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>misspelling</th>\n",
       "      <th>lemma</th>\n",
       "      <th>POS</th>\n",
       "      <th>tok_lem_POS</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alot</td>\n",
       "      <td>alot</td>\n",
       "      <td>NN</td>\n",
       "      <td>(alot, alot, NN)</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>studing</td>\n",
       "      <td>studing</td>\n",
       "      <td>VBG</td>\n",
       "      <td>(studing, studing, VBG)</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tofel</td>\n",
       "      <td>tofel</td>\n",
       "      <td>NNP</td>\n",
       "      <td>(tofel, tofel, NNP)</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eilperin</td>\n",
       "      <td>eilperin</td>\n",
       "      <td>NNP</td>\n",
       "      <td>(eilperin, eilperin, NNP)</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>seollal</td>\n",
       "      <td>seollal</td>\n",
       "      <td>NNP</td>\n",
       "      <td>(seollal, seollal, NNP)</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>befor</td>\n",
       "      <td>befor</td>\n",
       "      <td>NNP</td>\n",
       "      <td>(befor, befor, NNP)</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>arbus</td>\n",
       "      <td>arbus</td>\n",
       "      <td>NNP</td>\n",
       "      <td>(arbus, arbus, NNP)</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>favorit</td>\n",
       "      <td>favorit</td>\n",
       "      <td>JJ</td>\n",
       "      <td>(favorit, favorit, JJ)</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>peaple</td>\n",
       "      <td>peaple</td>\n",
       "      <td>NN</td>\n",
       "      <td>(peaple, peaple, NN)</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>gov</td>\n",
       "      <td>gov</td>\n",
       "      <td>NNP</td>\n",
       "      <td>(gov, gov, NNP)</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>144 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    misspelling     lemma  POS                tok_lem_POS  freq\n",
       "0          alot      alot   NN           (alot, alot, NN)   127\n",
       "1       studing   studing  VBG    (studing, studing, VBG)    74\n",
       "2         tofel     tofel  NNP        (tofel, tofel, NNP)    66\n",
       "3      eilperin  eilperin  NNP  (eilperin, eilperin, NNP)    48\n",
       "4       seollal   seollal  NNP    (seollal, seollal, NNP)    47\n",
       "..          ...       ...  ...                        ...   ...\n",
       "139       befor     befor  NNP        (befor, befor, NNP)    10\n",
       "140       arbus     arbus  NNP        (arbus, arbus, NNP)    10\n",
       "141     favorit   favorit   JJ     (favorit, favorit, JJ)    10\n",
       "142      peaple    peaple   NN       (peaple, peaple, NN)    10\n",
       "143         gov       gov  NNP            (gov, gov, NNP)    10\n",
       "\n",
       "[144 rows x 5 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(misspell_df.loc[misspell_df.freq >= 10]))\n",
    "misspell_df.loc[misspell_df.freq >= 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible segmentation\n",
    "\n",
    "Selected segmenter and spellchecker: SymSpell https://github.com/mammothb/symspellpy\n",
    "\n",
    "There is a dictionary file which which needs to be installed (saved to repo):\n",
    "[frequency_dictionary_en_82_765.txt](https://symspellpy.readthedocs.io/en/latest/users/installing.html)\n",
    "\n",
    "To install symspellpy the first time, use pip in command line: `pip install -U symspellpy`\n",
    "\n",
    "Prior to spelling correct, we first consider using the segmenter. This is a potentially useful first step as misspellings like 'alot' or 'dogmeat' will be separated into 'a lot' and 'dog meat' rather than corrected to a single word like 'lot'.  \n",
    "\n",
    "However, when segementing misspellings, the segmenter over performs, segmenting non-words into real words where it was clearly not intended, e.g. _improtant_ into _imp rot ant_ or _befor_ into _be for_. As such, the segmenting will not be automated. \n",
    "\n",
    "Instead, one manual segmentation will be carried out: _alot_ -> _a lot_ since _alot_ is the most common misspelling remaining in our dataframe (127 occurrences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 23135851162),\n",
       " ('of', 13151942776),\n",
       " ('and', 12997637966),\n",
       " ('to', 12136980858),\n",
       " ('a', 9081174698)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting up symspell\n",
    "\n",
    "from itertools import islice\n",
    "import pkg_resources\n",
    "from symspellpy import SymSpell\n",
    "from symspellpy import Verbosity\n",
    "sym_spell = SymSpell()\n",
    "dictionary_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "sym_spell.load_dictionary(dictionary_path, 0, 1)\n",
    "\n",
    "# Print out first 5 elements to demonstrate that dictionary is successfully loaded\n",
    "list(islice(sym_spell.words.items(), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing segmenter with 'alot' and 'dogmeat'\n",
    "\n",
    "# Set max_dictionary_edit_distance to avoid spelling correction\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=0, prefix_length=7)\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "\n",
    "# It is also possible to display frequency with result.distance_sum and edit distance with .log_prob_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating function for applying the above code\n",
    "\n",
    "def get_segments(word):\n",
    "    segments = sym_spell.word_segmentation(word)\n",
    "    if len(segments.corrected_string.split(' ')) > 1 \\\n",
    "    and segments.corrected_string.split(' ')[0] in scowl and segments.corrected_string.split(' ')[1] in scowl:\n",
    "        return segments.corrected_string\n",
    "    else:\n",
    "        return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog meat\n",
      "fireplace\n",
      "becuase\n"
     ]
    }
   ],
   "source": [
    "# Testing function\n",
    "\n",
    "print(get_segments('dogmeat')) # Should be segmented\n",
    "print(get_segments('fireplace')) # Should not be segmented\n",
    "print(get_segments('becuase')) # Should not be segmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>misspelling</th>\n",
       "      <th>lemma</th>\n",
       "      <th>POS</th>\n",
       "      <th>tok_lem_POS</th>\n",
       "      <th>freq</th>\n",
       "      <th>segments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alot</td>\n",
       "      <td>alot</td>\n",
       "      <td>NN</td>\n",
       "      <td>(alot, alot, NN)</td>\n",
       "      <td>127</td>\n",
       "      <td>a lot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>studing</td>\n",
       "      <td>studing</td>\n",
       "      <td>VBG</td>\n",
       "      <td>(studing, studing, VBG)</td>\n",
       "      <td>74</td>\n",
       "      <td>stu ding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tofel</td>\n",
       "      <td>tofel</td>\n",
       "      <td>NNP</td>\n",
       "      <td>(tofel, tofel, NNP)</td>\n",
       "      <td>66</td>\n",
       "      <td>tofel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eilperin</td>\n",
       "      <td>eilperin</td>\n",
       "      <td>NNP</td>\n",
       "      <td>(eilperin, eilperin, NNP)</td>\n",
       "      <td>48</td>\n",
       "      <td>eilperin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>seollal</td>\n",
       "      <td>seollal</td>\n",
       "      <td>NNP</td>\n",
       "      <td>(seollal, seollal, NNP)</td>\n",
       "      <td>47</td>\n",
       "      <td>se olla l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>goverment</td>\n",
       "      <td>goverment</td>\n",
       "      <td>NN</td>\n",
       "      <td>(goverment, goverment, NN)</td>\n",
       "      <td>47</td>\n",
       "      <td>g over men t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>decasia</td>\n",
       "      <td>decasia</td>\n",
       "      <td>NNP</td>\n",
       "      <td>(decasia, decasia, NNP)</td>\n",
       "      <td>45</td>\n",
       "      <td>dec asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>iam</td>\n",
       "      <td>iam</td>\n",
       "      <td>NNP</td>\n",
       "      <td>(iam, iam, NNP)</td>\n",
       "      <td>43</td>\n",
       "      <td>i am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>misurata</td>\n",
       "      <td>misurata</td>\n",
       "      <td>NNP</td>\n",
       "      <td>(misurata, misurata, NNP)</td>\n",
       "      <td>37</td>\n",
       "      <td>mi surat a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>esi</td>\n",
       "      <td>esi</td>\n",
       "      <td>NNP</td>\n",
       "      <td>(esi, esi, NNP)</td>\n",
       "      <td>34</td>\n",
       "      <td>esi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  misspelling      lemma  POS                 tok_lem_POS  freq      segments\n",
       "0        alot       alot   NN            (alot, alot, NN)   127         a lot\n",
       "1     studing    studing  VBG     (studing, studing, VBG)    74      stu ding\n",
       "2       tofel      tofel  NNP         (tofel, tofel, NNP)    66         tofel\n",
       "3    eilperin   eilperin  NNP   (eilperin, eilperin, NNP)    48      eilperin\n",
       "4     seollal    seollal  NNP     (seollal, seollal, NNP)    47     se olla l\n",
       "5   goverment  goverment   NN  (goverment, goverment, NN)    47  g over men t\n",
       "6     decasia    decasia  NNP     (decasia, decasia, NNP)    45      dec asia\n",
       "7         iam        iam  NNP             (iam, iam, NNP)    43          i am\n",
       "8    misurata   misurata  NNP   (misurata, misurata, NNP)    37    mi surat a\n",
       "9         esi        esi  NNP             (esi, esi, NNP)    34           esi"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying the function to create a new column\n",
    "\n",
    "misspell_df['segments'] =  misspell_df['misspelling'].apply(get_segments)\n",
    "misspell_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting this new column as segmentation creates false segments of misspelled words\n",
    "\n",
    "del misspell_df['segments']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying spelling correction\n",
    "\n",
    "In some ways SymSpell is not ideal as sentence context is not considered, only general frequencies. However, other well-known spellcheckers (hunspell, pyspell, etc.) use the same strategy - frequency based criteria for suggestions, without considering immediate cotext. As such, we have followed this common practice, but it is important to remember that accuracy of corrected tokens will not be 100% and must be taken into consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "because, 1, 271323986\n"
     ]
    }
   ],
   "source": [
    "# Testing spelling suggestions with 'becuase'\n",
    "\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "dictionary_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "# term_index is the column of the term and count_index is the column of the term frequency\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "\n",
    "input_term = \"becuase\"\n",
    "suggestions = sym_spell.lookup(input_term, Verbosity.CLOSEST, max_edit_distance=2, #Edit distance can be adjusted\n",
    "                               transfer_casing=True, #Optional argument set to ignore case\n",
    "                              include_unknown=True) #Return same word if unknown\n",
    "for suggestion in suggestions:\n",
    "    print(suggestion)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating function for finding unigram suggestions\n",
    "\n",
    "def get_suggestions(word):\n",
    "    if len(word) >= 4:\n",
    "        suggestions = sym_spell.lookup(word, Verbosity.CLOSEST,max_edit_distance=2, transfer_casing=True)\n",
    "    else:\n",
    "        suggestions = sym_spell.lookup(word, Verbosity.CLOSEST,max_edit_distance=1, transfer_casing=True)\n",
    "    return [str(x).split(',') for x in suggestions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['because', ' 1', ' 271323986']]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing function\n",
    "\n",
    "get_suggestions('becuase')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: The function has a variable edit distance: words of length 4 or more get edit distance of 2, shorter words get edit distance of 1. These preferences can be adjusted in the function if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "because of, 1, 3481714\n"
     ]
    }
   ],
   "source": [
    "# Testing spelling suggestions with 'becuase of'\n",
    "max_edit_distance_dictionary = 2\n",
    "prefix_length = 7\n",
    "sym_spell = SymSpell(max_edit_distance_dictionary, prefix_length)\n",
    "bigram_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\")\n",
    "if not sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1):\n",
    "    print(\"Dictionary file not found\")\n",
    "if not sym_spell.load_bigram_dictionary(bigram_path, term_index=0, count_index=2):\n",
    "    print(\"Bigram dictionary file not found\")\n",
    "input_term = 'becuase of'\n",
    "max_edit_distance_lookup = 2\n",
    "suggestions = sym_spell.lookup_compound(input_term, max_edit_distance_lookup)\n",
    "for suggestion in suggestions:\n",
    "    print(suggestion) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating function for finding bigram suggestions\n",
    "\n",
    "def get_bigram_suggestions(bigram):\n",
    "    suggestions = sym_spell.lookup_compound(bigram, max_edit_distance_lookup)\n",
    "    for suggestion in suggestions:\n",
    "        return [str(x).split(',') for x in suggestions] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['work hard', ' 2', ' 53229']]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing function\n",
    "get_bigram_suggestions('worq harg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>misspelling</th>\n",
       "      <th>lemma</th>\n",
       "      <th>POS</th>\n",
       "      <th>tok_lem_POS</th>\n",
       "      <th>freq</th>\n",
       "      <th>suggestions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alot</td>\n",
       "      <td>alot</td>\n",
       "      <td>NN</td>\n",
       "      <td>(alot, alot, NN)</td>\n",
       "      <td>127</td>\n",
       "      <td>[[lot,  1,  106405208], [slot,  1,  21602762],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>studing</td>\n",
       "      <td>studing</td>\n",
       "      <td>VBG</td>\n",
       "      <td>(studing, studing, VBG)</td>\n",
       "      <td>74</td>\n",
       "      <td>[[studying,  1,  9763653], [studding,  1,  345...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tofel</td>\n",
       "      <td>tofel</td>\n",
       "      <td>NNP</td>\n",
       "      <td>(tofel, tofel, NNP)</td>\n",
       "      <td>66</td>\n",
       "      <td>[[towel,  1,  4901668]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eilperin</td>\n",
       "      <td>eilperin</td>\n",
       "      <td>NNP</td>\n",
       "      <td>(eilperin, eilperin, NNP)</td>\n",
       "      <td>48</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>seollal</td>\n",
       "      <td>seollal</td>\n",
       "      <td>NNP</td>\n",
       "      <td>(seollal, seollal, NNP)</td>\n",
       "      <td>47</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  misspelling     lemma  POS                tok_lem_POS  freq  \\\n",
       "0        alot      alot   NN           (alot, alot, NN)   127   \n",
       "1     studing   studing  VBG    (studing, studing, VBG)    74   \n",
       "2       tofel     tofel  NNP        (tofel, tofel, NNP)    66   \n",
       "3    eilperin  eilperin  NNP  (eilperin, eilperin, NNP)    48   \n",
       "4     seollal   seollal  NNP    (seollal, seollal, NNP)    47   \n",
       "\n",
       "                                         suggestions  \n",
       "0  [[lot,  1,  106405208], [slot,  1,  21602762],...  \n",
       "1  [[studying,  1,  9763653], [studding,  1,  345...  \n",
       "2                            [[towel,  1,  4901668]]  \n",
       "3                                                 []  \n",
       "4                                                 []  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying functions to create new column\n",
    "\n",
    "misspell_df['suggestions'] =  misspell_df['misspelling'].apply(get_suggestions)\n",
    "misspell_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tok_lem_POS</th>\n",
       "      <th>misspelled_words</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[(i, i, PRP), (met, meet, VBD), (my, my, PRP$)...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[I met my friend Nife while I was studying in ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tok_lem_POS misspelled_words  \\\n",
       "answer_id                                                                       \n",
       "1          [(i, i, PRP), (met, meet, VBD), (my, my, PRP$)...               []   \n",
       "\n",
       "                                                    sentence  \n",
       "answer_id                                                     \n",
       "1          [I met my friend Nife while I was studying in ...  "
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_words.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above, the bigram function is first passed. If no suggestions are found, the unigram function is then passed. If there is still no suggestion, the original word is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1753"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking how many items without suggestions\n",
    "\n",
    "len(misspell_df.loc[(misspell_df.suggestions.str.len() == 0),:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Items with no suggestions - these will be left in their original form though manual corrections could be applied if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new column with just the most likely correction (based on frequency)\n",
    "\n",
    "misspell_df['correction'] = [x[0][0] if len(x) != 0 else np.NaN for x in misspell_df['suggestions']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If no correction, use original word\n",
    "\n",
    "misspell_df.correction.fillna(misspell_df.misspelling, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>misspelling</th>\n",
       "      <th>lemma</th>\n",
       "      <th>POS</th>\n",
       "      <th>tok_lem_POS</th>\n",
       "      <th>freq</th>\n",
       "      <th>suggestions</th>\n",
       "      <th>correction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alot</td>\n",
       "      <td>alot</td>\n",
       "      <td>NN</td>\n",
       "      <td>(alot, alot, NN)</td>\n",
       "      <td>127</td>\n",
       "      <td>[[lot,  1,  106405208], [slot,  1,  21602762],...</td>\n",
       "      <td>lot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  misspelling lemma POS       tok_lem_POS  freq  \\\n",
       "0        alot  alot  NN  (alot, alot, NN)   127   \n",
       "\n",
       "                                         suggestions correction  \n",
       "0  [[lot,  1,  106405208], [slot,  1,  21602762],...        lot  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misspell_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>misspelling</th>\n",
       "      <th>lemma</th>\n",
       "      <th>POS</th>\n",
       "      <th>tok_lem_POS</th>\n",
       "      <th>freq</th>\n",
       "      <th>suggestions</th>\n",
       "      <th>correction</th>\n",
       "      <th>correction_POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alot</td>\n",
       "      <td>alot</td>\n",
       "      <td>NN</td>\n",
       "      <td>(alot, alot, NN)</td>\n",
       "      <td>127</td>\n",
       "      <td>[[lot,  1,  106405208], [slot,  1,  21602762],...</td>\n",
       "      <td>lot</td>\n",
       "      <td>(lot, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>studing</td>\n",
       "      <td>studing</td>\n",
       "      <td>VBG</td>\n",
       "      <td>(studing, studing, VBG)</td>\n",
       "      <td>74</td>\n",
       "      <td>[[studying,  1,  9763653], [studding,  1,  345...</td>\n",
       "      <td>studying</td>\n",
       "      <td>(studying, VBG)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tofel</td>\n",
       "      <td>tofel</td>\n",
       "      <td>NNP</td>\n",
       "      <td>(tofel, tofel, NNP)</td>\n",
       "      <td>66</td>\n",
       "      <td>[[towel,  1,  4901668]]</td>\n",
       "      <td>towel</td>\n",
       "      <td>(towel, NNP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eilperin</td>\n",
       "      <td>eilperin</td>\n",
       "      <td>NNP</td>\n",
       "      <td>(eilperin, eilperin, NNP)</td>\n",
       "      <td>48</td>\n",
       "      <td>[]</td>\n",
       "      <td>eilperin</td>\n",
       "      <td>(eilperin, NNP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>seollal</td>\n",
       "      <td>seollal</td>\n",
       "      <td>NNP</td>\n",
       "      <td>(seollal, seollal, NNP)</td>\n",
       "      <td>47</td>\n",
       "      <td>[]</td>\n",
       "      <td>seollal</td>\n",
       "      <td>(seollal, NNP)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  misspelling     lemma  POS                tok_lem_POS  freq  \\\n",
       "0        alot      alot   NN           (alot, alot, NN)   127   \n",
       "1     studing   studing  VBG    (studing, studing, VBG)    74   \n",
       "2       tofel     tofel  NNP        (tofel, tofel, NNP)    66   \n",
       "3    eilperin  eilperin  NNP  (eilperin, eilperin, NNP)    48   \n",
       "4     seollal   seollal  NNP    (seollal, seollal, NNP)    47   \n",
       "\n",
       "                                         suggestions correction  \\\n",
       "0  [[lot,  1,  106405208], [slot,  1,  21602762],...        lot   \n",
       "1  [[studying,  1,  9763653], [studding,  1,  345...   studying   \n",
       "2                            [[towel,  1,  4901668]]      towel   \n",
       "3                                                 []   eilperin   \n",
       "4                                                 []    seollal   \n",
       "\n",
       "    correction_POS  \n",
       "0        (lot, NN)  \n",
       "1  (studying, VBG)  \n",
       "2     (towel, NNP)  \n",
       "3  (eilperin, NNP)  \n",
       "4   (seollal, NNP)  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create correction_POS column\n",
    "\n",
    "misspell_df['correction_POS'] = list(zip(misspell_df.correction, misspell_df.POS))\n",
    "misspell_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As described earlier - one manual correction for 'alot' will be added\n",
    "\n",
    "misspell_df.iloc[0,6] = 'a lot'\n",
    "misspell_df.at[0, 'correction_POS'] = ('a','DT'),('lot','NN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting unnecessary columns with duplicate information already contained in the tuple\n",
    "\n",
    "del misspell_df['lemma']\n",
    "del misspell_df['POS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>misspelling</th>\n",
       "      <th>tok_lem_POS</th>\n",
       "      <th>freq</th>\n",
       "      <th>suggestions</th>\n",
       "      <th>correction</th>\n",
       "      <th>correction_POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alot</td>\n",
       "      <td>(alot, alot, NN)</td>\n",
       "      <td>127</td>\n",
       "      <td>[[lot,  1,  106405208], [slot,  1,  21602762],...</td>\n",
       "      <td>a lot</td>\n",
       "      <td>((a, DT), (lot, NN))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>studing</td>\n",
       "      <td>(studing, studing, VBG)</td>\n",
       "      <td>74</td>\n",
       "      <td>[[studying,  1,  9763653], [studding,  1,  345...</td>\n",
       "      <td>studying</td>\n",
       "      <td>(studying, VBG)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tofel</td>\n",
       "      <td>(tofel, tofel, NNP)</td>\n",
       "      <td>66</td>\n",
       "      <td>[[towel,  1,  4901668]]</td>\n",
       "      <td>towel</td>\n",
       "      <td>(towel, NNP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eilperin</td>\n",
       "      <td>(eilperin, eilperin, NNP)</td>\n",
       "      <td>48</td>\n",
       "      <td>[]</td>\n",
       "      <td>eilperin</td>\n",
       "      <td>(eilperin, NNP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>seollal</td>\n",
       "      <td>(seollal, seollal, NNP)</td>\n",
       "      <td>47</td>\n",
       "      <td>[]</td>\n",
       "      <td>seollal</td>\n",
       "      <td>(seollal, NNP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>goverment</td>\n",
       "      <td>(goverment, goverment, NN)</td>\n",
       "      <td>47</td>\n",
       "      <td>[[government,  1,  206582673]]</td>\n",
       "      <td>government</td>\n",
       "      <td>(government, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>decasia</td>\n",
       "      <td>(decasia, decasia, NNP)</td>\n",
       "      <td>45</td>\n",
       "      <td>[[decani,  2,  18542]]</td>\n",
       "      <td>decani</td>\n",
       "      <td>(decani, NNP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>iam</td>\n",
       "      <td>(iam, iam, NNP)</td>\n",
       "      <td>43</td>\n",
       "      <td>[[am,  1,  576436203], [aim,  1,  28951240], [...</td>\n",
       "      <td>am</td>\n",
       "      <td>(am, NNP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>misurata</td>\n",
       "      <td>(misurata, misurata, NNP)</td>\n",
       "      <td>37</td>\n",
       "      <td>[]</td>\n",
       "      <td>misurata</td>\n",
       "      <td>(misurata, NNP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>esi</td>\n",
       "      <td>(esi, esi, NNP)</td>\n",
       "      <td>34</td>\n",
       "      <td>[[est,  1,  58112143], [psi,  1,  6101411], [e...</td>\n",
       "      <td>est</td>\n",
       "      <td>(est, NNP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>seol</td>\n",
       "      <td>(seol, seol, NNP)</td>\n",
       "      <td>31</td>\n",
       "      <td>[[sell,  1,  137696613], [seal,  1,  16274039]...</td>\n",
       "      <td>sell</td>\n",
       "      <td>(sell, NNP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>nickell</td>\n",
       "      <td>(nickell, nickell, NNP)</td>\n",
       "      <td>31</td>\n",
       "      <td>[[nickel,  1,  6105089], [nickels,  1,  587964]]</td>\n",
       "      <td>nickel</td>\n",
       "      <td>(nickel, NNP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>thaksin</td>\n",
       "      <td>(thaksin, thaksin, NNP)</td>\n",
       "      <td>30</td>\n",
       "      <td>[[takin,  2,  377723]]</td>\n",
       "      <td>takin</td>\n",
       "      <td>(takin, NNP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>grammer</td>\n",
       "      <td>(grammer, grammer, NN)</td>\n",
       "      <td>29</td>\n",
       "      <td>[[grammar,  1,  8019137], [grammes,  1,  13549...</td>\n",
       "      <td>grammar</td>\n",
       "      <td>(grammar, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>beatiful</td>\n",
       "      <td>(beatiful, beatiful, JJ)</td>\n",
       "      <td>28</td>\n",
       "      <td>[[beautiful,  1,  58503804]]</td>\n",
       "      <td>beautiful</td>\n",
       "      <td>(beautiful, JJ)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>okland</td>\n",
       "      <td>(okland, okland, NNP)</td>\n",
       "      <td>28</td>\n",
       "      <td>[[oakland,  1,  11011611]]</td>\n",
       "      <td>oakland</td>\n",
       "      <td>(oakland, NNP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>becuase</td>\n",
       "      <td>(becuase, becuase, NN)</td>\n",
       "      <td>28</td>\n",
       "      <td>[[because,  1,  271323986]]</td>\n",
       "      <td>because</td>\n",
       "      <td>(because, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>finaly</td>\n",
       "      <td>(finaly, finaly, NNP)</td>\n",
       "      <td>27</td>\n",
       "      <td>[[final,  1,  97649084], [finally,  1,  489990...</td>\n",
       "      <td>final</td>\n",
       "      <td>(final, NNP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>elwindi</td>\n",
       "      <td>(elwindi, elwindi, NNP)</td>\n",
       "      <td>26</td>\n",
       "      <td>[]</td>\n",
       "      <td>elwindi</td>\n",
       "      <td>(elwindi, NNP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>oss</td>\n",
       "      <td>(oss, oss, NNP)</td>\n",
       "      <td>24</td>\n",
       "      <td>[[loss,  1,  83015916], [ass,  1,  62060498], ...</td>\n",
       "      <td>loss</td>\n",
       "      <td>(loss, NNP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>differents</td>\n",
       "      <td>(differents, differents, NNS)</td>\n",
       "      <td>23</td>\n",
       "      <td>[[different,  1,  179794224]]</td>\n",
       "      <td>different</td>\n",
       "      <td>(different, NNS)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>becouse</td>\n",
       "      <td>(becouse, becouse, IN)</td>\n",
       "      <td>23</td>\n",
       "      <td>[[because,  1,  271323986]]</td>\n",
       "      <td>because</td>\n",
       "      <td>(because, IN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>tgi</td>\n",
       "      <td>(tgi, tgi, NNP)</td>\n",
       "      <td>23</td>\n",
       "      <td>[[tai,  1,  4011354], [tui,  1,  461356], [twi...</td>\n",
       "      <td>tai</td>\n",
       "      <td>(tai, NNP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>resturant</td>\n",
       "      <td>(resturant, resturant, NN)</td>\n",
       "      <td>23</td>\n",
       "      <td>[[restaurant,  1,  48255033]]</td>\n",
       "      <td>restaurant</td>\n",
       "      <td>(restaurant, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>sould</td>\n",
       "      <td>(sould, sould, VBP)</td>\n",
       "      <td>22</td>\n",
       "      <td>[[would,  1,  572644147], [should,  1,  402028...</td>\n",
       "      <td>would</td>\n",
       "      <td>(would, VBP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>shoud</td>\n",
       "      <td>(shoud, shoud, VBP)</td>\n",
       "      <td>22</td>\n",
       "      <td>[[should,  1,  402028056], [shout,  1,  396586...</td>\n",
       "      <td>should</td>\n",
       "      <td>(should, VBP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>sep</td>\n",
       "      <td>(sep, sep, NNP)</td>\n",
       "      <td>22</td>\n",
       "      <td>[[sep,  0,  107145206]]</td>\n",
       "      <td>sep</td>\n",
       "      <td>(sep, NNP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>lifes</td>\n",
       "      <td>(lifes, life, NN)</td>\n",
       "      <td>22</td>\n",
       "      <td>[[life,  1,  306559205], [lines,  1,  76806341...</td>\n",
       "      <td>life</td>\n",
       "      <td>(life, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ave</td>\n",
       "      <td>(ave, ave, NNP)</td>\n",
       "      <td>22</td>\n",
       "      <td>[[ave,  0,  34706502]]</td>\n",
       "      <td>ave</td>\n",
       "      <td>(ave, NNP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>newyork</td>\n",
       "      <td>(newyork, newyork, NNP)</td>\n",
       "      <td>21</td>\n",
       "      <td>[[network,  2,  225218991], [newport,  2,  938...</td>\n",
       "      <td>network</td>\n",
       "      <td>(network, NNP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>befor</td>\n",
       "      <td>(befor, befor, NN)</td>\n",
       "      <td>21</td>\n",
       "      <td>[[before,  1,  277546019]]</td>\n",
       "      <td>before</td>\n",
       "      <td>(before, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>somthing</td>\n",
       "      <td>(somthing, somthing, VBG)</td>\n",
       "      <td>21</td>\n",
       "      <td>[[something,  1,  131836210], [soothing,  1,  ...</td>\n",
       "      <td>something</td>\n",
       "      <td>(something, VBG)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>elshreef</td>\n",
       "      <td>(elshreef, elshreef, NNP)</td>\n",
       "      <td>20</td>\n",
       "      <td>[]</td>\n",
       "      <td>elshreef</td>\n",
       "      <td>(elshreef, NNP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>heungbu</td>\n",
       "      <td>(heungbu, heungbu, NNP)</td>\n",
       "      <td>19</td>\n",
       "      <td>[]</td>\n",
       "      <td>heungbu</td>\n",
       "      <td>(heungbu, NNP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>jun</td>\n",
       "      <td>(jun, jun, NNP)</td>\n",
       "      <td>18</td>\n",
       "      <td>[[jun,  0,  89154690]]</td>\n",
       "      <td>jun</td>\n",
       "      <td>(jun, NNP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>tution</td>\n",
       "      <td>(tution, tution, NN)</td>\n",
       "      <td>18</td>\n",
       "      <td>[[tuition,  1,  9860033]]</td>\n",
       "      <td>tuition</td>\n",
       "      <td>(tuition, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>becouse</td>\n",
       "      <td>(becouse, becouse, NN)</td>\n",
       "      <td>18</td>\n",
       "      <td>[[because,  1,  271323986]]</td>\n",
       "      <td>because</td>\n",
       "      <td>(because, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>kennywood</td>\n",
       "      <td>(kennywood, kennywood, NNP)</td>\n",
       "      <td>18</td>\n",
       "      <td>[]</td>\n",
       "      <td>kennywood</td>\n",
       "      <td>(kennywood, NNP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>jop</td>\n",
       "      <td>(jop, jop, NN)</td>\n",
       "      <td>18</td>\n",
       "      <td>[[top,  1,  484213771], [job,  1,  177706929],...</td>\n",
       "      <td>top</td>\n",
       "      <td>(top, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>referes</td>\n",
       "      <td>(referes, referes, VBZ)</td>\n",
       "      <td>18</td>\n",
       "      <td>[[refers,  1,  11869181], [referee,  1,  29419...</td>\n",
       "      <td>refers</td>\n",
       "      <td>(refers, VBZ)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>everytime</td>\n",
       "      <td>(everytime, everytime, NN)</td>\n",
       "      <td>18</td>\n",
       "      <td>[[overtime,  2,  4095339]]</td>\n",
       "      <td>overtime</td>\n",
       "      <td>(overtime, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>apartament</td>\n",
       "      <td>(apartament, apartament, NN)</td>\n",
       "      <td>18</td>\n",
       "      <td>[[apartment,  1,  30771172]]</td>\n",
       "      <td>apartment</td>\n",
       "      <td>(apartment, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>chunhyang</td>\n",
       "      <td>(chunhyang, chunhyang, NNP)</td>\n",
       "      <td>18</td>\n",
       "      <td>[]</td>\n",
       "      <td>chunhyang</td>\n",
       "      <td>(chunhyang, NNP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>appartment</td>\n",
       "      <td>(appartment, appartment, NN)</td>\n",
       "      <td>17</td>\n",
       "      <td>[[apartment,  1,  30771172]]</td>\n",
       "      <td>apartment</td>\n",
       "      <td>(apartment, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>eventhough</td>\n",
       "      <td>(eventhough, eventhough, IN)</td>\n",
       "      <td>17</td>\n",
       "      <td>[]</td>\n",
       "      <td>eventhough</td>\n",
       "      <td>(eventhough, IN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>tvs</td>\n",
       "      <td>(tvs, tvs, NNP)</td>\n",
       "      <td>17</td>\n",
       "      <td>[[tvs,  0,  12782654]]</td>\n",
       "      <td>tvs</td>\n",
       "      <td>(tvs, NNP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>maready</td>\n",
       "      <td>(maready, maready, NNP)</td>\n",
       "      <td>17</td>\n",
       "      <td>[[macready,  1,  56471]]</td>\n",
       "      <td>macready</td>\n",
       "      <td>(macready, NNP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>confortable</td>\n",
       "      <td>(confortable, confortable, JJ)</td>\n",
       "      <td>17</td>\n",
       "      <td>[[comfortable,  1,  19520487], [conformable,  ...</td>\n",
       "      <td>comfortable</td>\n",
       "      <td>(comfortable, JJ)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>freind</td>\n",
       "      <td>(freind, freind, NN)</td>\n",
       "      <td>17</td>\n",
       "      <td>[[friend,  1,  154527125]]</td>\n",
       "      <td>friend</td>\n",
       "      <td>(friend, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>tfgp</td>\n",
       "      <td>(tfgp, tfgp, NNP)</td>\n",
       "      <td>17</td>\n",
       "      <td>[[top,  2,  484213771], [trip,  2,  48175593],...</td>\n",
       "      <td>top</td>\n",
       "      <td>(top, NNP)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    misspelling                     tok_lem_POS  freq  \\\n",
       "0          alot                (alot, alot, NN)   127   \n",
       "1       studing         (studing, studing, VBG)    74   \n",
       "2         tofel             (tofel, tofel, NNP)    66   \n",
       "3      eilperin       (eilperin, eilperin, NNP)    48   \n",
       "4       seollal         (seollal, seollal, NNP)    47   \n",
       "5     goverment      (goverment, goverment, NN)    47   \n",
       "6       decasia         (decasia, decasia, NNP)    45   \n",
       "7           iam                 (iam, iam, NNP)    43   \n",
       "8      misurata       (misurata, misurata, NNP)    37   \n",
       "9           esi                 (esi, esi, NNP)    34   \n",
       "10         seol               (seol, seol, NNP)    31   \n",
       "11      nickell         (nickell, nickell, NNP)    31   \n",
       "12      thaksin         (thaksin, thaksin, NNP)    30   \n",
       "13      grammer          (grammer, grammer, NN)    29   \n",
       "14     beatiful        (beatiful, beatiful, JJ)    28   \n",
       "15       okland           (okland, okland, NNP)    28   \n",
       "16      becuase          (becuase, becuase, NN)    28   \n",
       "17       finaly           (finaly, finaly, NNP)    27   \n",
       "18      elwindi         (elwindi, elwindi, NNP)    26   \n",
       "19          oss                 (oss, oss, NNP)    24   \n",
       "20   differents   (differents, differents, NNS)    23   \n",
       "21      becouse          (becouse, becouse, IN)    23   \n",
       "22          tgi                 (tgi, tgi, NNP)    23   \n",
       "23    resturant      (resturant, resturant, NN)    23   \n",
       "26        sould             (sould, sould, VBP)    22   \n",
       "28        shoud             (shoud, shoud, VBP)    22   \n",
       "27          sep                 (sep, sep, NNP)    22   \n",
       "25        lifes               (lifes, life, NN)    22   \n",
       "24          ave                 (ave, ave, NNP)    22   \n",
       "29      newyork         (newyork, newyork, NNP)    21   \n",
       "30        befor              (befor, befor, NN)    21   \n",
       "31     somthing       (somthing, somthing, VBG)    21   \n",
       "32     elshreef       (elshreef, elshreef, NNP)    20   \n",
       "33      heungbu         (heungbu, heungbu, NNP)    19   \n",
       "40          jun                 (jun, jun, NNP)    18   \n",
       "38       tution            (tution, tution, NN)    18   \n",
       "42      becouse          (becouse, becouse, NN)    18   \n",
       "41    kennywood     (kennywood, kennywood, NNP)    18   \n",
       "39          jop                  (jop, jop, NN)    18   \n",
       "37      referes         (referes, referes, VBZ)    18   \n",
       "36    everytime      (everytime, everytime, NN)    18   \n",
       "35   apartament    (apartament, apartament, NN)    18   \n",
       "34    chunhyang     (chunhyang, chunhyang, NNP)    18   \n",
       "43   appartment    (appartment, appartment, NN)    17   \n",
       "44   eventhough    (eventhough, eventhough, IN)    17   \n",
       "45          tvs                 (tvs, tvs, NNP)    17   \n",
       "46      maready         (maready, maready, NNP)    17   \n",
       "47  confortable  (confortable, confortable, JJ)    17   \n",
       "48       freind            (freind, freind, NN)    17   \n",
       "49         tfgp               (tfgp, tfgp, NNP)    17   \n",
       "\n",
       "                                          suggestions   correction  \\\n",
       "0   [[lot,  1,  106405208], [slot,  1,  21602762],...        a lot   \n",
       "1   [[studying,  1,  9763653], [studding,  1,  345...     studying   \n",
       "2                             [[towel,  1,  4901668]]        towel   \n",
       "3                                                  []     eilperin   \n",
       "4                                                  []      seollal   \n",
       "5                      [[government,  1,  206582673]]   government   \n",
       "6                              [[decani,  2,  18542]]       decani   \n",
       "7   [[am,  1,  576436203], [aim,  1,  28951240], [...           am   \n",
       "8                                                  []     misurata   \n",
       "9   [[est,  1,  58112143], [psi,  1,  6101411], [e...          est   \n",
       "10  [[sell,  1,  137696613], [seal,  1,  16274039]...         sell   \n",
       "11   [[nickel,  1,  6105089], [nickels,  1,  587964]]       nickel   \n",
       "12                             [[takin,  2,  377723]]        takin   \n",
       "13  [[grammar,  1,  8019137], [grammes,  1,  13549...      grammar   \n",
       "14                       [[beautiful,  1,  58503804]]    beautiful   \n",
       "15                         [[oakland,  1,  11011611]]      oakland   \n",
       "16                        [[because,  1,  271323986]]      because   \n",
       "17  [[final,  1,  97649084], [finally,  1,  489990...        final   \n",
       "18                                                 []      elwindi   \n",
       "19  [[loss,  1,  83015916], [ass,  1,  62060498], ...         loss   \n",
       "20                      [[different,  1,  179794224]]    different   \n",
       "21                        [[because,  1,  271323986]]      because   \n",
       "22  [[tai,  1,  4011354], [tui,  1,  461356], [twi...          tai   \n",
       "23                      [[restaurant,  1,  48255033]]   restaurant   \n",
       "26  [[would,  1,  572644147], [should,  1,  402028...        would   \n",
       "28  [[should,  1,  402028056], [shout,  1,  396586...       should   \n",
       "27                            [[sep,  0,  107145206]]          sep   \n",
       "25  [[life,  1,  306559205], [lines,  1,  76806341...         life   \n",
       "24                             [[ave,  0,  34706502]]          ave   \n",
       "29  [[network,  2,  225218991], [newport,  2,  938...      network   \n",
       "30                         [[before,  1,  277546019]]       before   \n",
       "31  [[something,  1,  131836210], [soothing,  1,  ...    something   \n",
       "32                                                 []     elshreef   \n",
       "33                                                 []      heungbu   \n",
       "40                             [[jun,  0,  89154690]]          jun   \n",
       "38                          [[tuition,  1,  9860033]]      tuition   \n",
       "42                        [[because,  1,  271323986]]      because   \n",
       "41                                                 []    kennywood   \n",
       "39  [[top,  1,  484213771], [job,  1,  177706929],...          top   \n",
       "37  [[refers,  1,  11869181], [referee,  1,  29419...       refers   \n",
       "36                         [[overtime,  2,  4095339]]     overtime   \n",
       "35                       [[apartment,  1,  30771172]]    apartment   \n",
       "34                                                 []    chunhyang   \n",
       "43                       [[apartment,  1,  30771172]]    apartment   \n",
       "44                                                 []   eventhough   \n",
       "45                             [[tvs,  0,  12782654]]          tvs   \n",
       "46                           [[macready,  1,  56471]]     macready   \n",
       "47  [[comfortable,  1,  19520487], [conformable,  ...  comfortable   \n",
       "48                         [[friend,  1,  154527125]]       friend   \n",
       "49  [[top,  2,  484213771], [trip,  2,  48175593],...          top   \n",
       "\n",
       "          correction_POS  \n",
       "0   ((a, DT), (lot, NN))  \n",
       "1        (studying, VBG)  \n",
       "2           (towel, NNP)  \n",
       "3        (eilperin, NNP)  \n",
       "4         (seollal, NNP)  \n",
       "5       (government, NN)  \n",
       "6          (decani, NNP)  \n",
       "7              (am, NNP)  \n",
       "8        (misurata, NNP)  \n",
       "9             (est, NNP)  \n",
       "10           (sell, NNP)  \n",
       "11         (nickel, NNP)  \n",
       "12          (takin, NNP)  \n",
       "13         (grammar, NN)  \n",
       "14       (beautiful, JJ)  \n",
       "15        (oakland, NNP)  \n",
       "16         (because, NN)  \n",
       "17          (final, NNP)  \n",
       "18        (elwindi, NNP)  \n",
       "19           (loss, NNP)  \n",
       "20      (different, NNS)  \n",
       "21         (because, IN)  \n",
       "22            (tai, NNP)  \n",
       "23      (restaurant, NN)  \n",
       "26          (would, VBP)  \n",
       "28         (should, VBP)  \n",
       "27            (sep, NNP)  \n",
       "25            (life, NN)  \n",
       "24            (ave, NNP)  \n",
       "29        (network, NNP)  \n",
       "30          (before, NN)  \n",
       "31      (something, VBG)  \n",
       "32       (elshreef, NNP)  \n",
       "33        (heungbu, NNP)  \n",
       "40            (jun, NNP)  \n",
       "38         (tuition, NN)  \n",
       "42         (because, NN)  \n",
       "41      (kennywood, NNP)  \n",
       "39             (top, NN)  \n",
       "37         (refers, VBZ)  \n",
       "36        (overtime, NN)  \n",
       "35       (apartment, NN)  \n",
       "34      (chunhyang, NNP)  \n",
       "43       (apartment, NN)  \n",
       "44      (eventhough, IN)  \n",
       "45            (tvs, NNP)  \n",
       "46       (macready, NNP)  \n",
       "47     (comfortable, JJ)  \n",
       "48          (friend, NN)  \n",
       "49            (top, NNP)  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misspell_df.sort_values(by=['freq'], ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incorporating corrections into `pelic_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15779\n",
      "13854\n"
     ]
    }
   ],
   "source": [
    "# First removing items from misspell_df where no correction will take place\n",
    "\n",
    "print(len(misspell_df))\n",
    "misspell_df = misspell_df.loc[misspell_df.misspelling != misspell_df.correction]\n",
    "print(len(misspell_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>misspelling</th>\n",
       "      <th>tok_lem_POS</th>\n",
       "      <th>freq</th>\n",
       "      <th>suggestions</th>\n",
       "      <th>correction</th>\n",
       "      <th>correction_POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alot</td>\n",
       "      <td>(alot, alot, NN)</td>\n",
       "      <td>127</td>\n",
       "      <td>[[lot,  1,  106405208], [slot,  1,  21602762],...</td>\n",
       "      <td>a lot</td>\n",
       "      <td>((a, DT), (lot, NN))</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>studing</td>\n",
       "      <td>(studing, studing, VBG)</td>\n",
       "      <td>74</td>\n",
       "      <td>[[studying,  1,  9763653], [studding,  1,  345...</td>\n",
       "      <td>studying</td>\n",
       "      <td>(studying, VBG)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tofel</td>\n",
       "      <td>(tofel, tofel, NNP)</td>\n",
       "      <td>66</td>\n",
       "      <td>[[towel,  1,  4901668]]</td>\n",
       "      <td>towel</td>\n",
       "      <td>(towel, NNP)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>goverment</td>\n",
       "      <td>(goverment, goverment, NN)</td>\n",
       "      <td>47</td>\n",
       "      <td>[[government,  1,  206582673]]</td>\n",
       "      <td>government</td>\n",
       "      <td>(government, NN)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>decasia</td>\n",
       "      <td>(decasia, decasia, NNP)</td>\n",
       "      <td>45</td>\n",
       "      <td>[[decani,  2,  18542]]</td>\n",
       "      <td>decani</td>\n",
       "      <td>(decani, NNP)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  misspelling                 tok_lem_POS  freq  \\\n",
       "0        alot            (alot, alot, NN)   127   \n",
       "1     studing     (studing, studing, VBG)    74   \n",
       "2       tofel         (tofel, tofel, NNP)    66   \n",
       "5   goverment  (goverment, goverment, NN)    47   \n",
       "6     decasia     (decasia, decasia, NNP)    45   \n",
       "\n",
       "                                         suggestions  correction  \\\n",
       "0  [[lot,  1,  106405208], [slot,  1,  21602762],...       a lot   \n",
       "1  [[studying,  1,  9763653], [studding,  1,  345...    studying   \n",
       "2                            [[towel,  1,  4901668]]       towel   \n",
       "5                     [[government,  1,  206582673]]  government   \n",
       "6                             [[decani,  2,  18542]]      decani   \n",
       "\n",
       "         correction_POS  \n",
       "0  ((a, DT), (lot, NN))  \n",
       "1       (studying, VBG)  \n",
       "2          (towel, NNP)  \n",
       "5      (government, NN)  \n",
       "6         (decani, NNP)  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misspell_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dictionary for mappying - key = incorrect spelling, value = correct spelling\n",
    "\n",
    "misspell_dict = pd.Series(misspell_df.correction_POS.values,misspell_df.tok_lem_POS).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('alot', 'alot', 'NN'): (('a', 'DT'), ('lot', 'NN')),\n",
       " ('studing', 'studing', 'VBG'): ('studying', 'VBG'),\n",
       " ('tofel', 'tofel', 'NNP'): ('towel', 'NNP'),\n",
       " ('goverment', 'goverment', 'NN'): ('government', 'NN'),\n",
       " ('decasia', 'decasia', 'NNP'): ('decani', 'NNP'),\n",
       " ('iam', 'iam', 'NNP'): ('am', 'NNP'),\n",
       " ('esi', 'esi', 'NNP'): ('est', 'NNP'),\n",
       " ('seol', 'seol', 'NNP'): ('sell', 'NNP'),\n",
       " ('nickell', 'nickell', 'NNP'): ('nickel', 'NNP'),\n",
       " ('thaksin', 'thaksin', 'NNP'): ('takin', 'NNP'),\n",
       " ('grammer', 'grammer', 'NN'): ('grammar', 'NN'),\n",
       " ('beatiful', 'beatiful', 'JJ'): ('beautiful', 'JJ'),\n",
       " ('okland', 'okland', 'NNP'): ('oakland', 'NNP'),\n",
       " ('becuase', 'becuase', 'NN'): ('because', 'NN'),\n",
       " ('finaly', 'finaly', 'NNP'): ('final', 'NNP'),\n",
       " ('oss', 'oss', 'NNP'): ('loss', 'NNP'),\n",
       " ('differents', 'differents', 'NNS'): ('different', 'NNS'),\n",
       " ('becouse', 'becouse', 'IN'): ('because', 'IN'),\n",
       " ('tgi', 'tgi', 'NNP'): ('tai', 'NNP'),\n",
       " ('resturant', 'resturant', 'NN'): ('restaurant', 'NN'),\n",
       " ('lifes', 'life', 'NN'): ('life', 'NN'),\n",
       " ('sould', 'sould', 'VBP'): ('would', 'VBP'),\n",
       " ('shoud', 'shoud', 'VBP'): ('should', 'VBP'),\n",
       " ('newyork', 'newyork', 'NNP'): ('network', 'NNP'),\n",
       " ('befor', 'befor', 'NN'): ('before', 'NN'),\n",
       " ('somthing', 'somthing', 'VBG'): ('something', 'VBG'),\n",
       " ('apartament', 'apartament', 'NN'): ('apartment', 'NN'),\n",
       " ('everytime', 'everytime', 'NN'): ('overtime', 'NN'),\n",
       " ('referes', 'referes', 'VBZ'): ('refers', 'VBZ'),\n",
       " ('tution', 'tution', 'NN'): ('tuition', 'NN'),\n",
       " ('jop', 'jop', 'NN'): ('top', 'NN'),\n",
       " ('becouse', 'becouse', 'NN'): ('because', 'NN'),\n",
       " ('appartment', 'appartment', 'NN'): ('apartment', 'NN'),\n",
       " ('maready', 'maready', 'NNP'): ('macready', 'NNP'),\n",
       " ('confortable', 'confortable', 'JJ'): ('comfortable', 'JJ'),\n",
       " ('freind', 'freind', 'NN'): ('friend', 'NN'),\n",
       " ('tfgp', 'tfgp', 'NNP'): ('top', 'NNP'),\n",
       " ('begining', 'begining', 'NN'): ('beginning', 'NN'),\n",
       " ('defition', 'defition', 'NN'): ('edition', 'NN'),\n",
       " ('becasue', 'becasue', 'NN'): ('because', 'NN'),\n",
       " ('madain', 'madain', 'NNP'): ('main', 'NNP'),\n",
       " ('dsc', 'dsc', 'NNP'): ('dec', 'NNP'),\n",
       " ('freinds', 'freinds', 'NNS'): ('friends', 'NNS'),\n",
       " ('healty', 'healty', 'NN'): ('health', 'NN'),\n",
       " ('coffe', 'coffe', 'NN'): ('coffee', 'NN'),\n",
       " ('sul', 'sul', 'NNP'): ('sun', 'NNP'),\n",
       " ('addtion', 'addtion', 'NN'): ('addition', 'NN'),\n",
       " ('roomate', 'roomate', 'NN'): ('roommate', 'NN'),\n",
       " ('mrt', 'mrt', 'NNP'): ('art', 'NNP'),\n",
       " ('frisby', 'frisby', 'NN'): ('frisky', 'NN'),\n",
       " ('sentance', 'sentance', 'NN'): ('sentence', 'NN'),\n",
       " ('stangor', 'stangor', 'NNP'): ('stanford', 'NNP'),\n",
       " ('mpd', 'mpd', 'NNP'): ('mid', 'NNP'),\n",
       " ('poeple', 'poeple', 'NN'): ('people', 'NN'),\n",
       " ('frind', 'frind', 'NN'): ('find', 'NN'),\n",
       " ('tapol', 'tapol', 'NNP'): ('tool', 'NNP'),\n",
       " ('grammer', 'grammer', 'NNP'): ('grammar', 'NNP'),\n",
       " ('ustr', 'ustr', 'NNP'): ('user', 'NNP'),\n",
       " ('insomia', 'insomia', 'NNP'): ('insomnia', 'NNP'),\n",
       " ('improtant', 'improtant', 'JJ'): ('important', 'JJ'),\n",
       " ('nowdays', 'nowdays', 'NNS'): ('nowadays', 'NNS'),\n",
       " ('dailer', 'dailer', 'NNP'): ('mailer', 'NNP'),\n",
       " ('monthes', 'monthes', 'NNS'): ('months', 'NNS'),\n",
       " ('auther', 'auther', 'NN'): ('author', 'NN'),\n",
       " ('occured', 'occur', 'VBN'): ('occurred', 'VBN'),\n",
       " ('befor', 'befor', 'IN'): ('before', 'IN'),\n",
       " ('kapssh', 'kapssh', 'NNP'): ('kaposi', 'NNP'),\n",
       " ('languge', 'languge', 'NN'): ('language', 'NN'),\n",
       " ('resons', 'resons', 'NNS'): ('reasons', 'NNS'),\n",
       " ('frinds', 'frinds', 'NNS'): ('friends', 'NNS'),\n",
       " ('suger', 'suger', 'NN'): ('super', 'NN'),\n",
       " ('experince', 'experince', 'NN'): ('experience', 'NN'),\n",
       " ('everythings', 'everythings', 'NNS'): ('everything', 'NNS'),\n",
       " ('beacuse', 'beacuse', 'IN'): ('because', 'IN'),\n",
       " ('citys', 'citys', 'NN'): ('city', 'NN'),\n",
       " ('eatting', 'eatting', 'VBG'): ('eating', 'VBG'),\n",
       " ('belived', 'belived', 'VBN'): ('believed', 'VBN'),\n",
       " ('familly', 'familly', 'RB'): ('family', 'RB'),\n",
       " ('marrige', 'marrige', 'NN'): ('marriage', 'NN'),\n",
       " ('favorit', 'favorit', 'NN'): ('favourite', 'NN'),\n",
       " ('differnt', 'differnt', 'JJ'): ('different', 'JJ'),\n",
       " ('ccac', 'ccac', 'NNP'): ('can', 'NNP'),\n",
       " ('unversity', 'unversity', 'NN'): ('university', 'NN'),\n",
       " ('wonderfull', 'wonderfull', 'JJ'): ('wonderful', 'JJ'),\n",
       " ('scool', 'scool', 'NN'): ('school', 'NN'),\n",
       " ('wornell', 'wornell', 'NNP'): ('cornell', 'NNP'),\n",
       " ('llu', 'llu', 'NNP'): ('flu', 'NNP'),\n",
       " ('apartement', 'apartement', 'NN'): ('apartment', 'NN'),\n",
       " ('impotant', 'impotant', 'JJ'): ('important', 'JJ'),\n",
       " ('childern', 'childern', 'NN'): ('children', 'NN'),\n",
       " ('stockmann', 'stockmann', 'NNP'): ('stockman', 'NNP'),\n",
       " ('desicion', 'desicion', 'NN'): ('decision', 'NN'),\n",
       " ('becuase', 'becuase', 'NNP'): ('because', 'NNP'),\n",
       " ('shose', 'shose', 'NN'): ('those', 'NN'),\n",
       " ('postive', 'postive', 'JJ'): ('positive', 'JJ'),\n",
       " ('childrens', 'child', 'NNS'): ('children', 'NNS'),\n",
       " ('resturants', 'resturants', 'NNS'): ('restaurants', 'NNS'),\n",
       " ('sociaty', 'sociaty', 'NN'): ('society', 'NN'),\n",
       " ('contry', 'contry', 'NN'): ('country', 'NN'),\n",
       " ('fastfood', 'fastfood', 'NN'): ('eastwood', 'NN'),\n",
       " ('chiken', 'chiken', 'NN'): ('chicken', 'NN'),\n",
       " ('iwap', 'iwap', 'NNP'): ('swap', 'NNP'),\n",
       " ('diffrent', 'diffrent', 'JJ'): ('different', 'JJ'),\n",
       " ('befor', 'befor', 'NNP'): ('before', 'NNP'),\n",
       " ('arbus', 'arbus', 'NNP'): ('argus', 'NNP'),\n",
       " ('favorit', 'favorit', 'JJ'): ('favourite', 'JJ'),\n",
       " ('peaple', 'peaple', 'NN'): ('people', 'NN'),\n",
       " ('augest', 'augest', 'NNP'): ('august', 'NNP'),\n",
       " ('zongo', 'zongo', 'NNP'): ('congo', 'NNP'),\n",
       " ('spinhof', 'spinhof', 'NNP'): ('spinoff', 'NNP'),\n",
       " ('ncis', 'ncis', 'NNP'): ('acis', 'NNP'),\n",
       " ('peeters', 'peeters', 'NNP'): ('peters', 'NNP'),\n",
       " ('skywalker', 'skywalker', 'NNP'): ('skywalk', 'NNP'),\n",
       " ('becase', 'becase', 'NN'): ('because', 'NN'),\n",
       " ('happend', 'happend', 'VBP'): ('happen', 'VBP'),\n",
       " ('stiglitz', 'stiglitz', 'NNP'): ('stieglitz', 'NNP'),\n",
       " ('rmb', 'rmb', 'NNP'): ('rob', 'NNP'),\n",
       " ('becuse', 'becuse', 'IN'): ('because', 'IN'),\n",
       " ('heenes', 'heenes', 'NNP'): ('teens', 'NNP'),\n",
       " ('cks', 'cks', 'NNP'): ('cos', 'NNP'),\n",
       " ('mounth', 'mounth', 'NN'): ('month', 'NN'),\n",
       " ('johntel', 'johntel', 'NNP'): ('jointed', 'NNP'),\n",
       " ('aseda', 'aseda', 'NNP'): ('used', 'NNP'),\n",
       " ('goverments', 'goverments', 'NNS'): ('governments', 'NNS'),\n",
       " ('ip', 'ip', 'NNP'): ('in', 'NNP'),\n",
       " ('houng', 'houng', 'NNP'): ('young', 'NNP'),\n",
       " ('htm', 'htm', 'NN'): ('him', 'NN'),\n",
       " ('bix', 'bix', 'NNP'): ('big', 'NNP'),\n",
       " ('sr', 'sr', 'NNP'): ('or', 'NNP'),\n",
       " ('graddol', 'graddol', 'NNP'): ('gradual', 'NNP'),\n",
       " ('panera', 'panera', 'NNP'): ('paper', 'NNP'),\n",
       " ('vuelta', 'vuelta', 'NNP'): ('delta', 'NNP'),\n",
       " ('realy', 'realy', 'JJ'): ('real', 'JJ'),\n",
       " ('msg', 'msg', 'NNP'): ('mag', 'NNP'),\n",
       " ('polution', 'polution', 'NN'): ('solution', 'NN'),\n",
       " ('favorate', 'favorate', 'NN'): ('favourite', 'NN'),\n",
       " ('cbs', 'cbs', 'NNP'): ('lbs', 'NNP'),\n",
       " ('knst', 'knst', 'NNP'): ('knit', 'NNP'),\n",
       " ('evry', 'evry', 'JJ'): ('very', 'JJ'),\n",
       " ('realy', 'realy', 'VBP'): ('real', 'VBP'),\n",
       " ('guilin', 'guilin', 'NNP'): ('dublin', 'NNP'),\n",
       " ('shek', 'shek', 'NNP'): ('she', 'NNP'),\n",
       " ('gsm', 'gsm', 'NNP'): ('gym', 'NNP'),\n",
       " ('biederbecke', 'biederbecke', 'NNP'): ('beiderbecke', 'NNP'),\n",
       " ('suv', 'suv', 'NNP'): ('sun', 'NNP'),\n",
       " ('youself', 'youself', 'PRP'): ('yourself', 'PRP'),\n",
       " ('pronounciation', 'pronounciation', 'NN'): ('pronunciation', 'NN'),\n",
       " ('nepotizm', 'nepotizm', 'NN'): ('nepotism', 'NN'),\n",
       " ('garro', 'garro', 'NNP'): ('garry', 'NNP'),\n",
       " ('tatyana', 'tatyana', 'NNP'): ('tatiana', 'NNP'),\n",
       " ('hyogo', 'hyogo', 'NNP'): ('logo', 'NNP'),\n",
       " ('ee', 'ee', 'NNP'): ('be', 'NNP'),\n",
       " ('tesh', 'tesh', 'NNP'): ('test', 'NNP'),\n",
       " ('marriot', 'marriot', 'NNP'): ('marriott', 'NNP'),\n",
       " ('iam', 'iam', 'NN'): ('am', 'NN'),\n",
       " ('advertisment', 'advertisment', 'NN'): ('advertisement', 'NN'),\n",
       " ('huth', 'huth', 'NNP'): ('ruth', 'NNP'),\n",
       " ('pg', 'pg', 'NNP'): ('pm', 'NNP'),\n",
       " ('weigand', 'weigand', 'NNP'): ('wetland', 'NNP'),\n",
       " ('austrailia', 'austrailia', 'NNP'): ('australia', 'NNP'),\n",
       " ('aupair', 'aupair', 'NN'): ('repair', 'NN'),\n",
       " ('utd', 'utd', 'NNP'): ('ltd', 'NNP'),\n",
       " ('zaki', 'zaki', 'NNP'): ('saki', 'NNP'),\n",
       " ('usualy', 'usualy', 'JJ'): ('usually', 'JJ'),\n",
       " ('advantege', 'advantege', 'NN'): ('advantage', 'NN'),\n",
       " ('easly', 'easly', 'RB'): ('easy', 'RB'),\n",
       " ('intersting', 'intersting', 'VBG'): ('interesting', 'VBG'),\n",
       " ('occured', 'occur', 'VBD'): ('occurred', 'VBD'),\n",
       " ('heared', 'heared', 'VBN'): ('heard', 'VBN'),\n",
       " ('egcg', 'egcg', 'NNP'): ('egg', 'NNP'),\n",
       " ('espacially', 'espacially', 'RB'): ('especially', 'RB'),\n",
       " ('decaprio', 'decaprio', 'NNP'): ('dicaprio', 'NNP'),\n",
       " ('wikimedia', 'wikimedia', 'NNP'): ('wikipedia', 'NNP'),\n",
       " ('thes', 'thes', 'NNS'): ('the', 'NNS'),\n",
       " ('becuse', 'becuse', 'NN'): ('because', 'NN'),\n",
       " ('kbo', 'kbo', 'NNP'): ('ibo', 'NNP'),\n",
       " ('madina', 'madina', 'NNP'): ('marina', 'NNP'),\n",
       " ('chrest', 'chrest', 'NNP'): ('christ', 'NNP'),\n",
       " ('choosed', 'choosed', 'VBD'): ('choose', 'VBD'),\n",
       " ('oweis', 'oweis', 'NNP'): ('owens', 'NNP'),\n",
       " ('gi', 'gi', 'NN'): ('i', 'NN'),\n",
       " ('kohji', 'kohji', 'NNP'): ('kohl', 'NNP'),\n",
       " ('chiken', 'chiken', 'NNP'): ('chicken', 'NNP'),\n",
       " ('friens', 'friens', 'NNS'): ('friend', 'NNS'),\n",
       " ('ludwing', 'ludwing', 'VBG'): ('ludwig', 'VBG'),\n",
       " ('schmid', 'schmid', 'NNP'): ('schmidt', 'NNP'),\n",
       " ('experiance', 'experiance', 'NN'): ('experience', 'NN'),\n",
       " ('contries', 'contries', 'NNS'): ('countries', 'NNS'),\n",
       " ('airplan', 'airplan', 'NN'): ('airplay', 'NN'),\n",
       " ('lotteria', 'lotteria', 'NNP'): ('lottery', 'NNP'),\n",
       " ('alihan', 'alihan', 'NNP'): ('asian', 'NNP'),\n",
       " ('xavi', 'xavi', 'NNP'): ('xvi', 'NNP'),\n",
       " ('xiang', 'xiang', 'NNP'): ('xian', 'NNP'),\n",
       " ('msc', 'msc', 'NNP'): ('mac', 'NNP'),\n",
       " ('oishii', 'oishii', 'NNP'): ('rishi', 'NNP'),\n",
       " ('aldaha', 'aldaha', 'NNP'): ('alpha', 'NNP'),\n",
       " ('taitung', 'taitung', 'NNP'): ('waiting', 'NNP'),\n",
       " ('recieve', 'recieve', 'VB'): ('receive', 'VB'),\n",
       " ('unforgetable', 'unforgetable', 'JJ'): ('unforgettable', 'JJ'),\n",
       " ('raimunda', 'raimunda', 'NNP'): ('raymundo', 'NNP'),\n",
       " ('jao', 'jao', 'NNP'): ('jan', 'NNP'),\n",
       " ('neccessary', 'neccessary', 'JJ'): ('necessary', 'JJ'),\n",
       " ('acording', 'acording', 'VBG'): ('according', 'VBG'),\n",
       " ('menson', 'menson', 'NNP'): ('benson', 'NNP'),\n",
       " ('hatim', 'hatim', 'NNP'): ('hakim', 'NNP'),\n",
       " ('hassa', 'hassa', 'NNP'): ('hausa', 'NNP'),\n",
       " ('begining', 'begining', 'VBG'): ('beginning', 'VBG'),\n",
       " ('messi', 'messi', 'NNP'): ('mess', 'NNP'),\n",
       " ('beleive', 'beleive', 'VBP'): ('believe', 'VBP'),\n",
       " ('crowleys', 'crowleys', 'NNP'): ('crowley', 'NNP'),\n",
       " ('benifits', 'benifits', 'NNS'): ('benefits', 'NNS'),\n",
       " ('taksin', 'taksin', 'NNP'): ('takin', 'NNP'),\n",
       " ('pmp', 'pmp', 'NNP'): ('pm', 'NNP'),\n",
       " ('anvisa', 'anvisa', 'NNP'): ('visa', 'NNP'),\n",
       " ('ansert', 'ansert', 'NNP'): ('insert', 'NNP'),\n",
       " ('conclution', 'conclution', 'NN'): ('conclusion', 'NN'),\n",
       " ('ther', 'ther', 'NN'): ('the', 'NN'),\n",
       " ('pepole', 'pepole', 'NN'): ('people', 'NN'),\n",
       " ('dwen', 'dwen', 'NNP'): ('den', 'NNP'),\n",
       " ('writting', 'writting', 'VBG'): ('writing', 'VBG'),\n",
       " ('openion', 'openion', 'NN'): ('opinion', 'NN'),\n",
       " ('winslet', 'winslet', 'NNP'): ('inlet', 'NNP'),\n",
       " ('yuna', 'yuna', 'NNP'): ('una', 'NNP'),\n",
       " ('xie', 'xie', 'NNP'): ('die', 'NNP'),\n",
       " ('intersted', 'intersted', 'VBN'): ('interested', 'VBN'),\n",
       " ('convserve', 'convserve', 'VB'): ('conserve', 'VB'),\n",
       " ('simo', 'simo', 'NNP'): ('simon', 'NNP'),\n",
       " ('aprtment', 'aprtment', 'NN'): ('apartment', 'NN'),\n",
       " ('computech', 'computech', 'NNP'): ('computer', 'NNP'),\n",
       " ('deren', 'deren', 'NNP'): ('derek', 'NNP'),\n",
       " ('alot', 'alot', 'VBN'): ('lot', 'VBN'),\n",
       " ('similiar', 'similiar', 'JJ'): ('similar', 'JJ'),\n",
       " ('amircan', 'amircan', 'JJ'): ('american', 'JJ'),\n",
       " ('ebru', 'ebru', 'NNP'): ('ecru', 'NNP'),\n",
       " ('countery', 'countery', 'NN'): ('country', 'NN'),\n",
       " ('macdonalds', 'macdonalds', 'NNP'): ('macdonald', 'NNP'),\n",
       " ('nowdays', 'nowdays', 'NNP'): ('nowadays', 'NNP'),\n",
       " ('yokai', 'yokai', 'NNP'): ('nokia', 'NNP'),\n",
       " ('agressive', 'agressive', 'JJ'): ('aggressive', 'JJ'),\n",
       " ('malysia', 'malysia', 'NNP'): ('malaysia', 'NNP'),\n",
       " ('jusung', 'jusung', 'NNP'): ('using', 'NNP'),\n",
       " ('waha', 'waha', 'NNP'): ('aha', 'NNP'),\n",
       " ('aguilera', 'aguilera', 'NNP'): ('aguilar', 'NNP'),\n",
       " ('jiu', 'jiu', 'NNP'): ('jim', 'NNP'),\n",
       " ('vpn', 'vpn', 'NNP'): ('van', 'NNP'),\n",
       " ('kwo', 'kwo', 'NNP'): ('two', 'NNP'),\n",
       " ('yoweri', 'yoweri', 'NNP'): ('power', 'NNP'),\n",
       " ('pixar', 'pixar', 'NNP'): ('par', 'NNP'),\n",
       " ('voa', 'voa', 'NNP'): ('via', 'NNP'),\n",
       " ('neol', 'neol', 'NNP'): ('neil', 'NNP'),\n",
       " ('thr', 'thr', 'JJ'): ('the', 'JJ'),\n",
       " ('benefites', 'benefites', 'NNS'): ('benefits', 'NNS'),\n",
       " ('fouad', 'fouad', 'NNP'): ('found', 'NNP'),\n",
       " ('hanbok', 'hanbok', 'NNP'): ('handbook', 'NNP'),\n",
       " ('beiler', 'beiler', 'NNP'): ('boiler', 'NNP'),\n",
       " ('morden', 'morden', 'JJ'): ('borden', 'JJ'),\n",
       " ('heared', 'heared', 'VBD'): ('heard', 'VBD'),\n",
       " ('melwood', 'melwood', 'NNP'): ('elwood', 'NNP'),\n",
       " ('helth', 'helth', 'NN'): ('health', 'NN'),\n",
       " ('helva', 'helva', 'NNP'): ('helga', 'NNP'),\n",
       " ('pstn', 'pstn', 'NNP'): ('post', 'NNP'),\n",
       " ('frieden', 'frieden', 'NNP'): ('friedan', 'NNP'),\n",
       " ('habilis', 'habilis', 'NN'): ('habits', 'NN'),\n",
       " ('reson', 'reson', 'NN'): ('reason', 'NN'),\n",
       " ('brandn', 'brandn', 'NNP'): ('brand', 'NNP'),\n",
       " ('ddui', 'ddui', 'NN'): ('due', 'NN'),\n",
       " ('securaty', 'securaty', 'NN'): ('security', 'NN'),\n",
       " ('acadimic', 'acadimic', 'JJ'): ('academic', 'JJ'),\n",
       " ('goverment', 'goverment', 'JJ'): ('government', 'JJ'),\n",
       " ('ture', 'ture', 'NN'): ('sure', 'NN'),\n",
       " ('everthing', 'everthing', 'VBG'): ('everything', 'VBG'),\n",
       " ('bulding', 'bulding', 'NN'): ('building', 'NN'),\n",
       " ('mimo', 'mimo', 'NNP'): ('memo', 'NNP'),\n",
       " ('guntis', 'guntis', 'NNP'): ('until', 'NNP'),\n",
       " ('milion', 'milion', 'NN'): ('million', 'NN'),\n",
       " ('namsan', 'namsan', 'NNP'): ('nissan', 'NNP'),\n",
       " ('thoub', 'thoub', 'NN'): ('thou', 'NN'),\n",
       " ('activites', 'activites', 'NNS'): ('activities', 'NNS'),\n",
       " ('enlish', 'enlish', 'JJ'): ('english', 'JJ'),\n",
       " ('bacause', 'bacause', 'NN'): ('because', 'NN'),\n",
       " ('hosoya', 'hosoya', 'NNP'): ('hosea', 'NNP'),\n",
       " ('togather', 'togather', 'NN'): ('together', 'NN'),\n",
       " ('cdc', 'cdc', 'NNP'): ('cd', 'NNP'),\n",
       " ('iguaçu', 'iguaçu', 'NNP'): ('iguazu', 'NNP'),\n",
       " ('baket', 'baket', 'NN'): ('basket', 'NN'),\n",
       " ('sangwook', 'sangwook', 'NNP'): ('songbook', 'NNP'),\n",
       " ('icu', 'icu', 'NNP'): ('ice', 'NNP'),\n",
       " ('baklavs', 'baklavs', 'NNP'): ('baklava', 'NNP'),\n",
       " ('usally', 'usally', 'RB'): ('usually', 'RB'),\n",
       " ('preson', 'preson', 'NN'): ('person', 'NN'),\n",
       " ('zidane', 'zidane', 'NNP'): ('diane', 'NNP'),\n",
       " ('fark', 'fark', 'NNP'): ('park', 'NNP'),\n",
       " ('centr', 'centr', 'NN'): ('centre', 'NN'),\n",
       " ('fanthasia', 'fanthasia', 'NNP'): ('fantasia', 'NNP'),\n",
       " ('tring', 'tring', 'VBG'): ('thing', 'VBG'),\n",
       " ('alfitr', 'alfitr', 'NNP'): ('allier', 'NNP'),\n",
       " ('osechi', 'osechi', 'NNP'): ('sochi', 'NNP'),\n",
       " ('kony', 'kony', 'NNP'): ('sony', 'NNP'),\n",
       " ('buffay', 'buffay', 'NNP'): ('buffy', 'NNP'),\n",
       " ('kotkin', 'kotkin', 'NNP'): ('motrin', 'NNP'),\n",
       " ('gandy', 'gandy', 'NNP'): ('andy', 'NNP'),\n",
       " ('fcasd', 'fcasd', 'NNP'): ('case', 'NNP'),\n",
       " ('evrything', 'evrything', 'VBG'): ('everything', 'VBG'),\n",
       " ('brillient', 'brillient', 'VBN'): ('brilliant', 'VBN'),\n",
       " ('farc', 'farc', 'NNP'): ('far', 'NNP'),\n",
       " ('bridege', 'bridege', 'NN'): ('bridge', 'NN'),\n",
       " ('gliese', 'gliese', 'JJ'): ('lies', 'JJ'),\n",
       " ('womens', 'womens', 'NNS'): ('women', 'NNS'),\n",
       " ('knowlege', 'knowlege', 'NN'): ('knowledge', 'NN'),\n",
       " ('familys', 'family', 'NN'): ('family', 'NN'),\n",
       " ('ockland', 'ockland', 'NNP'): ('oakland', 'NNP'),\n",
       " ('taaj', 'taaj', 'NNP'): ('taal', 'NNP'),\n",
       " ('kinki', 'kinki', 'NNP'): ('kinky', 'NNP'),\n",
       " ('diaphragmic', 'diaphragmic', 'JJ'): ('diaphragm', 'JJ'),\n",
       " ('tk', 'tk', 'NNP'): ('to', 'NNP'),\n",
       " ('falled', 'falled', 'VBD'): ('called', 'VBD'),\n",
       " ('brillient', 'brillient', 'NN'): ('brilliant', 'NN'),\n",
       " ('synethesia', 'synethesia', 'NN'): ('synthesis', 'NN'),\n",
       " ('differnt', 'differnt', 'NN'): ('different', 'NN'),\n",
       " ('chandeler', 'chandeler', 'NNP'): ('chandler', 'NNP'),\n",
       " ('leem', 'leem', 'NNP'): ('lee', 'NNP'),\n",
       " ('sasu', 'sasu', 'NNP'): ('sash', 'NNP'),\n",
       " ('lb', 'lb', 'NN'): ('la', 'NN'),\n",
       " ('runze', 'runze', 'NNP'): ('rune', 'NNP'),\n",
       " ('saes', 'saes', 'NNP'): ('sales', 'NNP'),\n",
       " ('charactors', 'charactors', 'NNS'): ('characters', 'NNS'),\n",
       " ('tian', 'tian', 'NNP'): ('than', 'NNP'),\n",
       " ('toful', 'toful', 'NNP'): ('tofu', 'NNP'),\n",
       " ('diffirent', 'diffirent', 'JJ'): ('different', 'JJ'),\n",
       " ('diferent', 'diferent', 'JJ'): ('different', 'JJ'),\n",
       " ('finshed', 'finshed', 'VBD'): ('finished', 'VBD'),\n",
       " ('diffrent', 'diffrent', 'NN'): ('different', 'NN'),\n",
       " ('firends', 'firends', 'NNS'): ('friends', 'NNS'),\n",
       " ('riechmann', 'riechmann', 'NNP'): ('riemann', 'NNP'),\n",
       " ('tofle', 'tofle', 'NNP'): ('toile', 'NNP'),\n",
       " ('cafo', 'cafo', 'NNP'): ('cafe', 'NNP'),\n",
       " ('ordor', 'ordor', 'SYM'): ('order', 'SYM'),\n",
       " ('exessive', 'exessive', 'JJ'): ('excessive', 'JJ'),\n",
       " ('carrey', 'carrey', 'NNP'): ('carry', 'NNP'),\n",
       " ('saladino', 'saladino', 'NNP'): ('saladin', 'NNP'),\n",
       " ('worring', 'worring', 'VBG'): ('working', 'VBG'),\n",
       " ('kia', 'kia', 'NNP'): ('via', 'NNP'),\n",
       " ('kfupm', 'kfupm', 'NNP'): ('fum', 'NNP'),\n",
       " ('keren', 'keren', 'NNP'): ('karen', 'NNP'),\n",
       " ('programe', 'programe', 'NN'): ('program', 'NN'),\n",
       " ('hki', 'hki', 'NNP'): ('ski', 'NNP'),\n",
       " ('bauby', 'bauby', 'NNP'): ('baby', 'NNP'),\n",
       " ('wbc', 'wbc', 'NNP'): ('wac', 'NNP'),\n",
       " ('clooney', 'clooney', 'NNP'): ('clone', 'NNP'),\n",
       " ('ipods', 'ipod', 'NNS'): ('ipod', 'NNS'),\n",
       " ('performence', 'performence', 'NN'): ('performance', 'NN'),\n",
       " ('heum', 'heum', 'NN'): ('hum', 'NN'),\n",
       " ('shakira', 'shakira', 'NNP'): ('shaking', 'NNP'),\n",
       " ('helpfull', 'helpfull', 'JJ'): ('helpful', 'JJ'),\n",
       " ('sevral', 'sevral', 'JJ'): ('several', 'JJ'),\n",
       " ('thao', 'thao', 'NNP'): ('that', 'NNP'),\n",
       " ('taoban', 'taoban', 'NNP'): ('taiwan', 'NNP'),\n",
       " ('completly', 'completly', 'RB'): ('completely', 'RB'),\n",
       " ('pensylvania', 'pensylvania', 'NNP'): ('pennsylvania', 'NNP'),\n",
       " ('compus', 'compus', 'NN'): ('campus', 'NN'),\n",
       " ('dgree', 'dgree', 'NN'): ('degree', 'NN'),\n",
       " ('axé', 'axé', 'JJ'): ('axe', 'JJ'),\n",
       " ('importent', 'importent', 'JJ'): ('important', 'JJ'),\n",
       " ('popluar', 'popluar', 'JJ'): ('popular', 'JJ'),\n",
       " ('volver', 'volver', 'NNP'): ('solver', 'NNP'),\n",
       " ('imformation', 'imformation', 'NN'): ('information', 'NN'),\n",
       " ('ppt', 'ppt', 'NNP'): ('put', 'NNP'),\n",
       " ('vtr', 'vtr', 'NNP'): ('var', 'NNP'),\n",
       " ('infront', 'infront', 'NN'): ('front', 'NN'),\n",
       " ('usualy', 'usualy', 'VBP'): ('usually', 'VBP'),\n",
       " ('pittcat', 'pittcat', 'NNP'): ('pittman', 'NNP'),\n",
       " ('hyeon', 'hyeon', 'NNP'): ('hyson', 'NNP'),\n",
       " ('backman', 'backman', 'NNP'): ('hackman', 'NNP'),\n",
       " ('shmak', 'shmak', 'NNP'): ('speak', 'NNP'),\n",
       " ('humen', 'humen', 'NNS'): ('human', 'NNS'),\n",
       " ('homwork', 'homwork', 'NN'): ('homework', 'NN'),\n",
       " ('interst', 'interst', 'NN'): ('interest', 'NN'),\n",
       " ('qatif', 'qatif', 'NNP'): ('latin', 'NNP'),\n",
       " ('hatice', 'hatice', 'NNP'): ('notice', 'NNP'),\n",
       " ('papalli', 'papalli', 'NNP'): ('papal', 'NNP'),\n",
       " ('especialy', 'especialy', 'NN'): ('especially', 'NN'),\n",
       " ('aniston', 'aniston', 'NNP'): ('winston', 'NNP'),\n",
       " ('kahlo', 'kahlo', 'NNP'): ('yahoo', 'NNP'),\n",
       " ('bigum', 'bigum', 'NNP'): ('begum', 'NNP'),\n",
       " ('seng', 'seng', 'NN'): ('send', 'NN'),\n",
       " ('grasha', 'grasha', 'NNP'): ('graph', 'NNP'),\n",
       " ('blass', 'blass', 'NNP'): ('class', 'NNP'),\n",
       " ('unaids', 'unaids', 'NNP'): ('units', 'NNP'),\n",
       " ('dj', 'dj', 'NNP'): ('do', 'NNP'),\n",
       " ('chuan', 'chuan', 'NN'): ('chan', 'NN'),\n",
       " ('recomand', 'recomand', 'VBP'): ('recommend', 'VBP'),\n",
       " ('kbs', 'kbs', 'NNP'): ('lbs', 'NNP'),\n",
       " ('recommond', 'recommond', 'VBP'): ('recommend', 'VBP'),\n",
       " ('wipo', 'wipo', 'NNP'): ('wipe', 'NNP'),\n",
       " ('guanipa', 'guanipa', 'NNP'): ('grandpa', 'NNP'),\n",
       " ('whome', 'whome', 'NN'): ('home', 'NN'),\n",
       " ('pdas', 'pdas', 'NNP'): ('pads', 'NNP'),\n",
       " ('sentosa', 'sentosa', 'NNP'): ('santos', 'NNP'),\n",
       " ('seongsu', 'seongsu', 'NNP'): ('songs', 'NNP'),\n",
       " ('rac', 'rac', 'NN'): ('mac', 'NN'),\n",
       " ('radin', 'radin', 'NNP'): ('radio', 'NNP'),\n",
       " ('customes', 'customes', 'NNS'): ('customer', 'NNS'),\n",
       " ('aoyama', 'aoyama', 'NNP'): ('toyama', 'NNP'),\n",
       " ('jogoya', 'jogoya', 'NNP'): ('nagoya', 'NNP'),\n",
       " ('halback', 'halback', 'NNP'): ('halfback', 'NNP'),\n",
       " ('hackell', 'hackell', 'NNP'): ('haskell', 'NNP'),\n",
       " ('parise', 'parise', 'NNP'): ('paris', 'NNP'),\n",
       " ('haagen', 'haagen', 'NNP'): ('hagen', 'NNP'),\n",
       " ('rausk', 'rausk', 'NNP'): ('rusk', 'NNP'),\n",
       " ('esam', 'esam', 'NNP'): ('sam', 'NNP'),\n",
       " ('raynolds', 'raynolds', 'NNP'): ('reynolds', 'NNP'),\n",
       " ('beutifull', 'beutifull', 'JJ'): ('beautiful', 'JJ'),\n",
       " ('junhee', 'junhee', 'NNP'): ('june', 'NNP'),\n",
       " ('holmberg', 'holmberg', 'NNP'): ('goldberg', 'NNP'),\n",
       " ('aa', 'aa', 'NNP'): ('a', 'NNP'),\n",
       " ('studyed', 'studyed', 'VBN'): ('studied', 'VBN'),\n",
       " ('desigh', 'desigh', 'NNP'): ('design', 'NNP'),\n",
       " ('nartey', 'nartey', 'NNP'): ('party', 'NNP'),\n",
       " ('activision', 'activision', 'NNP'): ('activation', 'NNP'),\n",
       " ('coutry', 'coutry', 'NN'): ('country', 'NN'),\n",
       " ('maizena', 'maizena', 'NNP'): ('maiden', 'NNP'),\n",
       " ('niaaa', 'niaaa', 'NNP'): ('diana', 'NNP'),\n",
       " ('moretti', 'moretti', 'NNP'): ('loretta', 'NNP'),\n",
       " ('masood', 'masood', 'NNP'): ('mood', 'NNP'),\n",
       " ('macbook', 'macbook', 'NNP'): ('matchbook', 'NNP'),\n",
       " ('mirkin', 'mirkin', 'NNP'): ('firkin', 'NNP'),\n",
       " ('nyuh', 'nyuh', 'NNP'): ('nut', 'NNP'),\n",
       " ('milions', 'milions', 'NNS'): ('millions', 'NNS'),\n",
       " ('stadying', 'stadying', 'VBG'): ('studying', 'VBG'),\n",
       " ('nouwens', 'nouwens', 'NNP'): ('owens', 'NNP'),\n",
       " ('millitary', 'millitary', 'NN'): ('military', 'NN'),\n",
       " ('ajda', 'ajda', 'NNP'): ('ada', 'NNP'),\n",
       " ('sucessful', 'sucessful', 'JJ'): ('successful', 'JJ'),\n",
       " ('csat', 'csat', 'NNP'): ('chat', 'NNP'),\n",
       " ('af', 'af', 'NNP'): ('of', 'NNP'),\n",
       " ('denktas', 'denktas', 'NNP'): ('dental', 'NNP'),\n",
       " ('yueh', 'yueh', 'SYM'): ('such', 'SYM'),\n",
       " ('nfl', 'nfl', 'NNP'): ('nil', 'NNP'),\n",
       " ('culteres', 'culteres', 'NNS'): ('cultures', 'NNS'),\n",
       " ('cvd', 'cvd', 'NNP'): ('cd', 'NNP'),\n",
       " ('deprission', 'deprission', 'NN'): ('depression', 'NN'),\n",
       " ('soju', 'soju', 'NNP'): ('sou', 'NNP'),\n",
       " ('bekham', 'bekham', 'NNP'): ('beam', 'NNP'),\n",
       " ('forigners', 'forigners', 'NNS'): ('foreigners', 'NNS'),\n",
       " ('wagman', 'wagman', 'NNP'): ('bagman', 'NNP'),\n",
       " ('beutiful', 'beutiful', 'JJ'): ('beautiful', 'JJ'),\n",
       " ('coutries', 'coutries', 'NNS'): ('countries', 'NNS'),\n",
       " ('theather', 'theather', 'NN'): ('heather', 'NN'),\n",
       " ('beleive', 'beleive', 'VB'): ('believe', 'VB'),\n",
       " ('ahmet', 'ahmet', 'NNP'): ('ahmed', 'NNP'),\n",
       " ('wnat', 'wnat', 'VBP'): ('what', 'VBP'),\n",
       " ('sentece', 'sentece', 'NN'): ('sentence', 'NN'),\n",
       " ('thowb', 'thowb', 'NNP'): ('how', 'NNP'),\n",
       " ('nalot', 'nalot', 'NNP'): ('not', 'NNP'),\n",
       " ('guatteri', 'guatteri', 'NNP'): ('gutter', 'NNP'),\n",
       " ('forign', 'forign', 'JJ'): ('foreign', 'JJ'),\n",
       " ('kidman', 'kidman', 'NNP'): ('kirman', 'NNP'),\n",
       " ('margrot', 'margrot', 'NNP'): ('margot', 'NNP'),\n",
       " ('poors', 'poors', 'NNS'): ('poor', 'NNS'),\n",
       " ('studiying', 'studiying', 'VBG'): ('studying', 'VBG'),\n",
       " ('milgram', 'milgram', 'NNP'): ('diagram', 'NNP'),\n",
       " ('ksu', 'ksu', 'NNP'): ('usu', 'NNP'),\n",
       " ('são', 'são', 'NNP'): ('so', 'NNP'),\n",
       " ('cuold', 'cuold', 'NN'): ('could', 'NN'),\n",
       " ('poeple', 'poeple', 'NNS'): ('people', 'NNS'),\n",
       " ('achive', 'achive', 'VB'): ('archive', 'VB'),\n",
       " ('seolak', 'seolak', 'NNP'): ('solar', 'NNP'),\n",
       " ('pleace', 'pleace', 'NN'): ('please', 'NN'),\n",
       " ('benifit', 'benifit', 'NN'): ('benefit', 'NN'),\n",
       " ('erdent', 'erdent', 'NNP'): ('ardent', 'NNP'),\n",
       " ('belived', 'belived', 'VBD'): ('believed', 'VBD'),\n",
       " ('vist', 'vist', 'VB'): ('list', 'VB'),\n",
       " ('rainning', 'rainning', 'VBG'): ('raining', 'VBG'),\n",
       " ('tradional', 'tradional', 'JJ'): ('traditional', 'JJ'),\n",
       " ('eeoc', 'eeoc', 'NNP'): ('dec', 'NNP'),\n",
       " ('ktx', 'ktx', 'NNP'): ('kex', 'NNP'),\n",
       " ('eatten', 'eatten', 'VBN'): ('eaten', 'VBN'),\n",
       " ('sause', 'sause', 'NN'): ('cause', 'NN'),\n",
       " ('expecially', 'expecially', 'RB'): ('especially', 'RB'),\n",
       " ('discribe', 'discribe', 'VB'): ('describe', 'VB'),\n",
       " ('desease', 'desease', 'NN'): ('disease', 'NN'),\n",
       " ('unfortunatly', 'unfortunatly', 'RB'): ('unfortunately', 'RB'),\n",
       " ('trahms', 'trahms', 'NNP'): ('brahms', 'NNP'),\n",
       " ('siddiq', 'siddiq', 'NNP'): ('kiddie', 'NNP'),\n",
       " ('decieded', 'decieded', 'VBD'): ('decided', 'VBD'),\n",
       " ('raz', 'raz', 'NNP'): ('ray', 'NNP'),\n",
       " ('nuilang', 'nuilang', 'NNP'): ('nailing', 'NNP'),\n",
       " ('cm', 'cm', 'NN'): ('pm', 'NN'),\n",
       " ('nagihan', 'nagihan', 'NNP'): ('nathan', 'NNP'),\n",
       " ('iwas', 'iwas', 'NNP'): ('was', 'NNP'),\n",
       " ('childern', 'childern', 'JJ'): ('children', 'JJ'),\n",
       " ('moai', 'moai', 'NNP'): ('mai', 'NNP'),\n",
       " ('ghabban', 'ghabban', 'NNP'): ('shabbat', 'NNP'),\n",
       " ('giggs', 'giggs', 'NNP'): ('gigs', 'NNP'),\n",
       " ('regulary', 'regulary', 'JJ'): ('regular', 'JJ'),\n",
       " ('dazs', 'dazs', 'NNP'): ('days', 'NNP'),\n",
       " ('drived', 'drived', 'VBD'): ('drive', 'VBD'),\n",
       " ('drowing', 'drowing', 'VBG'): ('growing', 'VBG'),\n",
       " ('turlington', 'turlington', 'NNP'): ('burlington', 'NNP'),\n",
       " ('gollum', 'gollum', 'NNP'): ('gallup', 'NNP'),\n",
       " ('chosun', 'chosun', 'NNP'): ('chosen', 'NNP'),\n",
       " ('remeber', 'remeber', 'VB'): ('remember', 'VB'),\n",
       " ('gauer', 'gauer', 'NNP'): ('bauer', 'NNP'),\n",
       " ('chilhood', 'chilhood', 'NN'): ('childhood', 'NN'),\n",
       " ('yueh', 'yueh', 'NNP'): ('such', 'NNP'),\n",
       " ('jik', 'jik', 'NN'): ('jim', 'NN'),\n",
       " ('jieqi', 'jieqi', 'NNP'): ('jedi', 'NNP'),\n",
       " ('doughter', 'doughter', 'NN'): ('daughter', 'NN'),\n",
       " ('trodition', 'trodition', 'NN'): ('tradition', 'NN'),\n",
       " ('geting', 'geting', 'VBG'): ('getting', 'VBG'),\n",
       " ('brh', 'brh', 'NNP'): ('bra', 'NNP'),\n",
       " ('jia', 'jia', 'NNP'): ('via', 'NNP'),\n",
       " ('troble', 'troble', 'JJ'): ('trouble', 'JJ'),\n",
       " ('thoes', 'thoes', 'JJ'): ('those', 'JJ'),\n",
       " ('whos', 'whos', 'NN'): ('who', 'NN'),\n",
       " ('scholl', 'scholl', 'NN'): ('school', 'NN'),\n",
       " ('gotrah', 'gotrah', 'NNP'): ('gotta', 'NNP'),\n",
       " ('ct', 'ct', 'NNP'): ('it', 'NNP'),\n",
       " ('responsabilities', 'responsabilities', 'NNS'): ('responsibilities', 'NNS'),\n",
       " ('responsability', 'responsability', 'NN'): ('responsibility', 'NN'),\n",
       " ('ubicate', 'ubicate', 'JJ'): ('urinate', 'JJ'),\n",
       " ('transpotation', 'transpotation', 'NN'): ('transportation', 'NN'),\n",
       " ('nigative', 'nigative', 'JJ'): ('negative', 'JJ'),\n",
       " ('transportaion', 'transportaion', 'NN'): ('transportation', 'NN'),\n",
       " ('uesd', 'uesd', 'VBP'): ('used', 'VBP'),\n",
       " ('minogue', 'minogue', 'NNP'): ('mingle', 'NNP'),\n",
       " ('uniqe', 'uniqe', 'JJ'): ('unique', 'JJ'),\n",
       " ('kenobi', 'kenobi', 'NNP'): ('keno', 'NNP'),\n",
       " ('transfomation', 'transfomation', 'NN'): ('transformation', 'NN'),\n",
       " ('therfore', 'therfore', 'NNP'): ('therefore', 'NNP'),\n",
       " ('uae', 'uae', 'NNP'): ('use', 'NNP'),\n",
       " ('intuitives', 'intuitives', 'NNS'): ('intuitive', 'NNS'),\n",
       " ('reserch', 'reserch', 'NN'): ('research', 'NN'),\n",
       " ('chiken', 'chiken', 'JJ'): ('chicken', 'JJ'),\n",
       " ('bodrum', 'bodrum', 'NNP'): ('forum', 'NNP'),\n",
       " ('goverment', 'goverment', 'NNP'): ('government', 'NNP'),\n",
       " ('iss', 'iss', 'NNP'): ('is', 'NNP'),\n",
       " ('governement', 'governement', 'NN'): ('government', 'NN'),\n",
       " ('recieved', 'recieved', 'VBD'): ('received', 'VBD'),\n",
       " ('eui', 'eui', 'NNP'): ('eur', 'NNP'),\n",
       " ('paragragh', 'paragragh', 'NN'): ('paragraph', 'NN'),\n",
       " ('mypyramid', 'mypyramid', 'NNP'): ('pyramid', 'NNP'),\n",
       " ('yourk', 'yourk', 'NNP'): ('your', 'NNP'),\n",
       " ('persone', 'persone', 'NN'): ('person', 'NN'),\n",
       " ('wav', 'wav', 'NN'): ('was', 'NN'),\n",
       " ('analytes', 'analyte', 'NNS'): ('analyses', 'NNS'),\n",
       " ('rissotto', 'rissotto', 'NN'): ('risotto', 'NN'),\n",
       " ('posco', 'posco', 'NNP'): ('poco', 'NNP'),\n",
       " ('ondol', 'ondol', 'NNP'): ('london', 'NNP'),\n",
       " ('sabrata', 'sabrata', 'NNP'): ('sabbath', 'NNP'),\n",
       " ('unsucced', 'unsucced', 'VBD'): ('unsuited', 'VBD'),\n",
       " ('aybala', 'aybala', 'NNP'): ('ayala', 'NNP'),\n",
       " ('feets', 'feets', 'NNS'): ('feet', 'NNS'),\n",
       " ('feild', 'feild', 'NN'): ('field', 'NN'),\n",
       " ('toei', 'toei', 'NNP'): ('toe', 'NNP'),\n",
       " ('vegtables', 'vegtables', 'NNS'): ('vegetables', 'NNS'),\n",
       " ('universtiy', 'universtiy', 'NNP'): ('university', 'NNP'),\n",
       " ('qassim', 'qassim', 'NNP'): ('passim', 'NNP'),\n",
       " ('changings', 'changings', 'NNS'): ('changing', 'NNS'),\n",
       " ('sasko', 'sasko', 'NNP'): ('sask', 'NNP'),\n",
       " ('temparature', 'temparature', 'NN'): ('temperature', 'NN'),\n",
       " ('dicision', 'dicision', 'NN'): ('decision', 'NN'),\n",
       " ('linfen', 'linfen', 'NNP'): ('linen', 'NNP'),\n",
       " ('enought', 'enought', 'JJ'): ('enough', 'JJ'),\n",
       " ('zhinu', 'zhinu', 'NNP'): ('think', 'NNP'),\n",
       " ('diffierent', 'diffierent', 'JJ'): ('different', 'JJ'),\n",
       " ('contruction', 'contruction', 'NN'): ('construction', 'NN'),\n",
       " ('cengiz', 'cengiz', 'NNP'): ('engin', 'NNP'),\n",
       " ('devorce', 'devorce', 'NN'): ('divorce', 'NN'),\n",
       " ('saleries', 'saleries', 'NNS'): ('salaries', 'NNS'),\n",
       " ('beacuse', 'beacuse', 'NN'): ('because', 'NN'),\n",
       " ('suadi', 'suadi', 'NNP'): ('saudi', 'NNP'),\n",
       " ('vedio', 'vedio', 'NN'): ('vedic', 'NN'),\n",
       " ('hilman', 'hilman', 'NNP'): ('gilman', 'NNP'),\n",
       " ('bacause', 'bacause', 'IN'): ('because', 'IN'),\n",
       " ('librty', 'librty', 'NNP'): ('liberty', 'NNP'),\n",
       " ('santino', 'santino', 'NNP'): ('latino', 'NNP'),\n",
       " ('badroom', 'badroom', 'NN'): ('bedroom', 'NN'),\n",
       " ('vanila', 'vanila', 'NNP'): ('vanilla', 'NNP'),\n",
       " ('enjoied', 'enjoied', 'VBD'): ('enjoyed', 'VBD'),\n",
       " ('englsh', 'englsh', 'NNP'): ('english', 'NNP'),\n",
       " ('diffcult', 'diffcult', 'NN'): ('difficult', 'NN'),\n",
       " ('bady', 'bady', 'NN'): ('body', 'NN'),\n",
       " ('steph', 'steph', 'NNP'): ('step', 'NNP'),\n",
       " ('safa', 'safa', 'NN'): ('safe', 'NN'),\n",
       " ('abdo', 'abdo', 'NNP'): ('ado', 'NNP'),\n",
       " ('wymard', 'wymard', 'NNP'): ('yard', 'NNP'),\n",
       " ('cbt', 'cbt', 'NNP'): ('cut', 'NNP'),\n",
       " ('didnot', 'didnot', 'VBP'): ('idiot', 'VBP'),\n",
       " ('unkown', 'unkown', 'JJ'): ('unknown', 'JJ'),\n",
       " ('becasue', 'becasue', 'NNP'): ('because', 'NNP'),\n",
       " ('puplic', 'puplic', 'JJ'): ('public', 'JJ'),\n",
       " ('sae', 'sae', 'NNP'): ('see', 'NNP'),\n",
       " ('vcd', 'vcd', 'NNP'): ('cd', 'NNP'),\n",
       " ('alfiter', 'alfiter', 'NNP'): ('after', 'NNP'),\n",
       " ('becuase', 'becuase', 'VB'): ('because', 'VB'),\n",
       " ('sterotypes', 'sterotypes', 'NNS'): ('stereotypes', 'NNS'),\n",
       " ('ilets', 'ilets', 'NNP'): ('lets', 'NNP'),\n",
       " ('preperation', 'preperation', 'NN'): ('preparation', 'NN'),\n",
       " ('shawal', 'shawal', 'NNP'): ('shawl', 'NNP'),\n",
       " ('einsten', 'einsten', 'NNP'): ('einstein', 'NNP'),\n",
       " ('shero', 'shero', 'NNP'): ('hero', 'NNP'),\n",
       " ('dificult', 'dificult', 'NN'): ('difficult', 'NN'),\n",
       " ('littele', 'littele', 'NNP'): ('little', 'NNP'),\n",
       " ('nawuma', 'nawuma', 'NNP'): ('nahum', 'NNP'),\n",
       " ('shephard', 'shephard', 'NNP'): ('shepherd', 'NNP'),\n",
       " ('avoide', 'avoide', 'VB'): ('avoid', 'VB'),\n",
       " ('larg', 'larg', 'JJ'): ('large', 'JJ'),\n",
       " ('usefull', 'usefull', 'JJ'): ('useful', 'JJ'),\n",
       " ('clouse', 'clouse', 'NNP'): ('close', 'NNP'),\n",
       " ('fifa', 'fifa', 'NNP'): ('fife', 'NNP'),\n",
       " ('bayildi', 'bayildi', 'NNP'): ('bacilli', 'NNP'),\n",
       " ('rosling', 'rosling', 'VBG'): ('rolling', 'VBG'),\n",
       " ('beginnig', 'beginnig', 'NN'): ('beginning', 'NN'),\n",
       " ('laroche', 'laroche', 'NNP'): ('roche', 'NNP'),\n",
       " ('aldaffa', 'aldaffa', 'NNP'): ('alfalfa', 'NNP'),\n",
       " ('collegues', 'collegues', 'NNS'): ('colleges', 'NNS'),\n",
       " ('fajr', 'fajr', 'NNP'): ('far', 'NNP'),\n",
       " ('uu', 'uu', 'NNP'): ('us', 'NNP'),\n",
       " ('postech', 'postech', 'NNP'): ('posted', 'NNP'),\n",
       " ('ahn', 'ahn', 'NNP'): ('an', 'NNP'),\n",
       " ('sheeps', 'sheep', 'NNS'): ('sheets', 'NNS'),\n",
       " ('postion', 'postion', 'NN'): ('position', 'NN'),\n",
       " ('ch', 'ch', 'NNP'): ('cd', 'NNP'),\n",
       " ('isha', 'isha', 'NNP'): ('aisha', 'NNP'),\n",
       " ('sisiter', 'sisiter', 'NN'): ('sister', 'NN'),\n",
       " ('mypyramid', 'mypyramid', 'NN'): ('pyramid', 'NN'),\n",
       " ('weaknes', 'weaknes', 'NNS'): ('weakness', 'NNS'),\n",
       " ('hemsphere', 'hemsphere', 'RB'): ('hemisphere', 'RB'),\n",
       " ('pausch', 'pausch', 'NNP'): ('paunch', 'NNP'),\n",
       " ('htn', 'htn', 'NNP'): ('hon', 'NNP'),\n",
       " ('zhen', 'zhen', 'NNP'): ('when', 'NNP'),\n",
       " ('secound', 'secound', 'NN'): ('second', 'NN'),\n",
       " ('proffesional', 'proffesional', 'JJ'): ('professional', 'JJ'),\n",
       " ('becasue', 'becasue', 'IN'): ('because', 'IN'),\n",
       " ('tzu', 'tzu', 'NNP'): ('thu', 'NNP'),\n",
       " ('basical', 'basical', 'JJ'): ('basic', 'JJ'),\n",
       " ('companys', 'company', 'NN'): ('company', 'NN'),\n",
       " ('embarassed', 'embarassed', 'VBN'): ('embarrassed', 'VBN'),\n",
       " ('uas', 'uas', 'NNP'): ('as', 'NNP'),\n",
       " ('sunseok', 'sunseok', 'NNP'): ('sunset', 'NNP'),\n",
       " ('gramatical', 'gramatical', 'JJ'): ('grammatical', 'JJ'),\n",
       " ('adventages', 'adventages', 'NNS'): ('advantages', 'NNS'),\n",
       " ('emt', 'emt', 'NNP'): ('est', 'NNP'),\n",
       " ('tryed', 'tryed', 'VBD'): ('tried', 'VBD'),\n",
       " ('usb', 'usb', 'NNP'): ('us', 'NNP'),\n",
       " ('encourge', 'encourge', 'VB'): ('encourage', 'VB'),\n",
       " ('helthy', 'helthy', 'JJ'): ('healthy', 'JJ'),\n",
       " ('iss', 'iss', 'NN'): ('is', 'NN'),\n",
       " ('nepotizm', 'nepotizm', 'NNP'): ('nepotism', 'NNP'),\n",
       " ('etepmom', 'etepmom', 'NNP'): ('stepmom', 'NNP'),\n",
       " ('typeing', 'typeing', 'VBG'): ('typing', 'VBG'),\n",
       " ('gplc', 'gplc', 'NNP'): ('pc', 'NNP'),\n",
       " ('hemspheres', 'hemspheres', 'NNS'): ('hemispheres', 'NNS'),\n",
       " ('psu', 'psu', 'NNP'): ('psi', 'NNP'),\n",
       " ('pavlik', 'pavlik', 'NNP'): ('pali', 'NNP'),\n",
       " ('tsen', 'tsen', 'NNP'): ('then', 'NNP'),\n",
       " ('zumba', 'zumba', 'NNP'): ('rumba', 'NNP'),\n",
       " ('advisces', 'advisces', 'NNS'): ('advises', 'NNS'),\n",
       " ('ususally', 'ususally', 'RB'): ('usually', 'RB'),\n",
       " ('hourse', 'hourse', 'NN'): ('house', 'NN'),\n",
       " ('evangelista', 'evangelista', 'NNP'): ('evangelist', 'NNP'),\n",
       " ('choosen', 'choosen', 'VBN'): ('choose', 'VBN'),\n",
       " ('aquire', 'aquire', 'VB'): ('acquire', 'VB'),\n",
       " ('proplems', 'proplems', 'NNS'): ('problems', 'NNS'),\n",
       " ('bambur', 'bambur', 'NNP'): ('amber', 'NNP'),\n",
       " ('problame', 'problame', 'NN'): ('problem', 'NN'),\n",
       " ('jasim', 'jasim', 'NNP'): ('basic', 'NNP'),\n",
       " ('droped', 'droped', 'VBD'): ('dropped', 'VBD'),\n",
       " ('bankok', 'bankok', 'NNP'): ('bangkok', 'NNP'),\n",
       " ('peole', 'peole', 'NN'): ('people', 'NN'),\n",
       " ('hijri', 'hijri', 'NNP'): ('hijra', 'NNP'),\n",
       " ('skorich', 'skorich', 'NNP'): ('scorch', 'NNP'),\n",
       " ('tuk', 'tuk', 'NNP'): ('tue', 'NNP'),\n",
       " ('vaction', 'vaction', 'NN'): ('action', 'NN'),\n",
       " ('meate', 'meate', 'NN'): ('meat', 'NN'),\n",
       " ('sulki', 'sulki', 'NNP'): ('sulky', 'NNP'),\n",
       " ('competion', 'competion', 'NN'): ('completion', 'NN'),\n",
       " ('moatz', 'moatz', 'NNP'): ('moat', 'NNP'),\n",
       " ('stritof', 'stritof', 'NNP'): ('triton', 'NNP'),\n",
       " ('girlfrind', 'girlfrind', 'NN'): ('girlfriend', 'NN'),\n",
       " ('bocelli', 'bocelli', 'NNP'): ('corelli', 'NNP'),\n",
       " ('tfgp', 'tfgp', 'IN'): ('top', 'IN'),\n",
       " ('advertisments', 'advertisments', 'NNS'): ('advertisements', 'NNS'),\n",
       " ('uribe', 'uribe', 'NNP'): ('tribe', 'NNP'),\n",
       " ('tariq', 'tariq', 'NNP'): ('tarim', 'NNP'),\n",
       " ('everytime', 'everytime', 'JJ'): ('overtime', 'JJ'),\n",
       " ('gounville', 'gounville', 'NNP'): ('granville', 'NNP'),\n",
       " ('wedgewood', 'wedgewood', 'NNP'): ('wedgwood', 'NNP'),\n",
       " ('shima', 'shima', 'NNP'): ('shiva', 'NNP'),\n",
       " ('houyi', 'houyi', 'NNP'): ('house', 'NNP'),\n",
       " ('archaelogists', 'archaelogists', 'NNS'): ('archaeologists', 'NNS'),\n",
       " ('comunication', 'comunication', 'NN'): ('communication', 'NN'),\n",
       " ('chres', 'chres', 'NNP'): ('chris', 'NNP'),\n",
       " ('adviced', 'adviced', 'VBD'): ('advice', 'VBD'),\n",
       " ('seleeg', 'seleeg', 'NNP'): ('select', 'NNP'),\n",
       " ('dubie', 'dubie', 'NNP'): ('due', 'NNP'),\n",
       " ('heene', 'heene', 'NNP'): ('helene', 'NNP'),\n",
       " ('miquel', 'miquel', 'NNP'): ('miguel', 'NNP'),\n",
       " ('stors', 'stors', 'NNPS'): ('store', 'NNPS'),\n",
       " ('inconfidence', 'inconfidence', 'NN'): ('confidence', 'NN'),\n",
       " ('attendence', 'attendence', 'NN'): ('attendance', 'NN'),\n",
       " ('universites', 'universites', 'NNS'): ('universities', 'NNS'),\n",
       " ('rac', 'rac', 'VB'): ('mac', 'VB'),\n",
       " ('nass', 'nass', 'NNP'): ('ass', 'NNP'),\n",
       " ('shuld', 'shuld', 'MD'): ('should', 'MD'),\n",
       " ('universitey', 'universitey', 'NN'): ('university', 'NN'),\n",
       " ('acommodations', 'acommodations', 'NNS'): ('accommodations', 'NNS'),\n",
       " ('befor', 'befor', 'VBP'): ('before', 'VBP'),\n",
       " ('benfits', 'benfits', 'NNS'): ('benefits', 'NNS'),\n",
       " ('awaji', 'awaji', 'NNP'): ('away', 'NNP'),\n",
       " ('relized', 'relized', 'VBD'): ('realized', 'VBD'),\n",
       " ('naver', 'naver', 'NNP'): ('never', 'NNP'),\n",
       " ('nanum', 'nanum', 'NNP'): ('nahum', 'NNP'),\n",
       " ('qerqe', 'qerqe', 'NNP'): ('here', 'NNP'),\n",
       " ('befor', 'befor', 'VB'): ('before', 'VB'),\n",
       " ('indivisuals', 'indivisuals', 'NNS'): ('individuals', 'NNS'),\n",
       " ('voiling', 'voiling', 'VBG'): ('boiling', 'VBG'),\n",
       " ('plase', 'plase', 'NN'): ('please', 'NN'),\n",
       " ('comercials', 'comercials', 'NNS'): ('commercials', 'NNS'),\n",
       " ('ereli', 'ereli', 'NNP'): ('merely', 'NNP'),\n",
       " ('befor', 'befor', 'JJ'): ('before', 'JJ'),\n",
       " ('masjed', 'masjed', 'NNP'): ('masked', 'NNP'),\n",
       " ('techology', 'techology', 'NN'): ('technology', 'NN'),\n",
       " ('attintion', 'attintion', 'NN'): ('attention', 'NN'),\n",
       " ('eldery', 'eldery', 'NN'): ('elderly', 'NN'),\n",
       " ('habbits', 'habbits', 'NNS'): ('habits', 'NNS'),\n",
       " ('adivices', 'adivices', 'NNS'): ('advices', 'NNS'),\n",
       " ('teke', 'teke', 'NNP'): ('take', 'NNP'),\n",
       " ('porgrame', 'porgrame', 'NN'): ('program', 'NN'),\n",
       " ('porgram', 'porgram', 'NN'): ('program', 'NN'),\n",
       " ('adition', 'adition', 'NN'): ('edition', 'NN'),\n",
       " ('serie', 'serie', 'NNP'): ('series', 'NNP'),\n",
       " ('stil', 'stil', 'VBP'): ('still', 'VBP'),\n",
       " ('adivces', 'adivces', 'NNS'): ('advices', 'NNS'),\n",
       " ('zamzam', 'zamzam', 'NNP'): ('hamza', 'NNP'),\n",
       " ('happend', 'happend', 'VBN'): ('happen', 'VBN'),\n",
       " ('servise', 'servise', 'NN'): ('service', 'NN'),\n",
       " ('harmfull', 'harmfull', 'NN'): ('harmful', 'NN'),\n",
       " ('shoul', 'shoul', 'VBP'): ('should', 'VBP'),\n",
       " ('aviod', 'aviod', 'VB'): ('avoid', 'VB'),\n",
       " ('seperated', 'seperated', 'VBD'): ('separated', 'VBD'),\n",
       " ('hapiness', 'hapiness', 'NN'): ('happiness', 'NN'),\n",
       " ('beginging', 'beginging', 'NN'): ('beginning', 'NN'),\n",
       " ('ehrle', 'ehrle', 'NNP'): ('earle', 'NNP'),\n",
       " ('improvment', 'improvment', 'NN'): ('improvement', 'NN'),\n",
       " ('inaddition', 'inaddition', 'NNP'): ('addition', 'NNP'),\n",
       " ('ehara', 'ehara', 'NNP'): ('hard', 'NNP'),\n",
       " ('defferent', 'defferent', 'JJ'): ('different', 'JJ'),\n",
       " ('hatem', 'hatem', 'NNP'): ('hate', 'NNP'),\n",
       " ('inforamtion', 'inforamtion', 'NN'): ('information', 'NN'),\n",
       " ('pixis', 'pixis', 'NNP'): ('pixie', 'NNP'),\n",
       " ('inthe', 'inthe', 'JJ'): ('the', 'JJ'),\n",
       " ('immuno', 'immuno', 'NN'): ('immune', 'NN'),\n",
       " ('comming', 'comming', 'VBG'): ('coming', 'VBG'),\n",
       " ('becuase', 'becuase', 'VBP'): ('because', 'VBP'),\n",
       " ('precussionist', 'precussionist', 'NN'): ('percussionist', 'NN'),\n",
       " ('senteces', 'senteces', 'NNS'): ('sentences', 'NNS'),\n",
       " ('tenis', 'tenis', 'NN'): ('tennis', 'NN'),\n",
       " ('sucki', 'sucki', 'NNP'): ('suck', 'NNP'),\n",
       " ('biger', 'biger', 'JJR'): ('tiger', 'JJR'),\n",
       " ('zyou', 'zyou', 'NNP'): ('you', 'NNP'),\n",
       " ('cohee', 'cohee', 'NN'): ('cohen', 'NN'),\n",
       " ('wanying', 'wanying', 'NNP'): ('wanting', 'NNP'),\n",
       " ('shold', 'shold', 'VBP'): ('should', 'VBP'),\n",
       " ('naeil', 'naeil', 'NNP'): ('neil', 'NNP'),\n",
       " ('pufas', 'pufas', 'NNP'): ('puffs', 'NNP'),\n",
       " ('bikinies', 'bikinies', 'NNS'): ('bikinis', 'NNS'),\n",
       " ('prefering', 'prefering', 'VBG'): ('preferring', 'VBG'),\n",
       " ('minsuk', 'minsuk', 'NNP'): ('minsk', 'NNP'),\n",
       " ('espicially', 'espicially', 'RB'): ('especially', 'RB'),\n",
       " ('cp', 'cp', 'NNP'): ('up', 'NNP'),\n",
       " ('reaserch', 'reaserch', 'NN'): ('research', 'NN'),\n",
       " ('intersting', 'intersting', 'JJ'): ('interesting', 'JJ'),\n",
       " ('thesedays', 'thesedays', 'NNS'): ('tuesdays', 'NNS'),\n",
       " ('suhyang', 'suhyang', 'NNP'): ('shang', 'NNP'),\n",
       " ('insomina', 'insomina', 'NNP'): ('insomnia', 'NNP'),\n",
       " ('ch', 'ch', 'NN'): ('cd', 'NN'),\n",
       " ('edeiah', 'edeiah', 'NNP'): ('edith', 'NNP'),\n",
       " ('elementry', 'elementry', 'NN'): ('elementary', 'NN'),\n",
       " ('vrt', 'vrt', 'NNP'): ('art', 'NNP'),\n",
       " ('cioran', 'cioran', 'NNP'): ('iran', 'NNP'),\n",
       " ('sicong', 'sicong', 'NNP'): ('second', 'NNP'),\n",
       " ('ther', 'ther', 'CC'): ('the', 'CC'),\n",
       " ('shoping', 'shoping', 'VBG'): ('shopping', 'VBG'),\n",
       " ('hawai', 'hawai', 'NNP'): ('hawaii', 'NNP'),\n",
       " ('pratice', 'pratice', 'VB'): ('practice', 'VB'),\n",
       " ('pittsbugh', 'pittsbugh', 'NNP'): ('pittsburgh', 'NNP'),\n",
       " ('identfy', 'identfy', 'VB'): ('identify', 'VB'),\n",
       " ('edfu', 'edfu', 'NNP'): ('edit', 'NNP'),\n",
       " ('ther', 'ther', 'JJR'): ('the', 'JJR'),\n",
       " ('prblems', 'prblems', 'NNS'): ('problems', 'NNS'),\n",
       " ('hc', 'hc', 'NNP'): ('he', 'NNP'),\n",
       " ('elif', 'elif', 'NNP'): ('elf', 'NNP'),\n",
       " ('guseva', 'guseva', 'NNP'): ('geneva', 'NNP'),\n",
       " ('inhofe', 'inhofe', 'NNP'): ('income', 'NNP'),\n",
       " ('pitsburgh', 'pitsburgh', 'NNP'): ('pittsburgh', 'NNP'),\n",
       " ('guk', 'guk', 'NN'): ('guy', 'NN'),\n",
       " ('accross', 'accross', 'IN'): ('across', 'IN'),\n",
       " ('vegeterian', 'vegeterian', 'JJ'): ('vegetarian', 'JJ'),\n",
       " ('insadong', 'insadong', 'NNP'): ('invading', 'NNP'),\n",
       " ('wheater', 'wheater', 'NN'): ('heater', 'NN'),\n",
       " ('skrable', 'skrable', 'JJ'): ('skiable', 'JJ'),\n",
       " ('konw', 'konw', 'VB'): ('know', 'VB'),\n",
       " ('transporation', 'transporation', 'NN'): ('transportation', 'NN'),\n",
       " ('differnt', 'differnt', 'VBN'): ('different', 'VBN'),\n",
       " ('yosu', 'yosu', 'NNP'): ('you', 'NNP'),\n",
       " ('destory', 'destory', 'VB'): ('destroy', 'VB'),\n",
       " ('runyan', 'runyan', 'NNP'): ('bunyan', 'NNP'),\n",
       " ('studints', 'studints', 'NNS'): ('students', 'NNS'),\n",
       " ('aggs', 'aggs', 'NN'): ('ages', 'NN'),\n",
       " ('freetime', 'freetime', 'NN'): ('freebie', 'NN'),\n",
       " ('dittmann', 'dittmann', 'NNP'): ('pittman', 'NNP'),\n",
       " ('pai', 'pai', 'NNP'): ('pay', 'NNP'),\n",
       " ('mackin', 'mackin', 'NNP'): ('mawkin', 'NNP'),\n",
       " ('tabuk', 'tabuk', 'NNP'): ('taluk', 'NNP'),\n",
       " ('softwears', 'softwears', 'NNS'): ('software', 'NNS'),\n",
       " ('restorant', 'restorant', 'NN'): ('restaurant', 'NN'),\n",
       " ('runing', 'runing', 'VBG'): ('running', 'VBG'),\n",
       " ('swiming', 'swiming', 'VBG'): ('swimming', 'VBG'),\n",
       " ('frasan', 'frasan', 'NNP'): ('fraser', 'NNP'),\n",
       " ('restraunt', 'restraunt', 'NN'): ('restraint', 'NN'),\n",
       " ('owren', 'owren', 'NNP'): ('owen', 'NNP'),\n",
       " ('diffrences', 'diffrences', 'NNS'): ('differences', 'NNS'),\n",
       " ('foz', 'foz', 'NNP'): ('for', 'NNP'),\n",
       " ('fouts', 'fouts', 'NNP'): ('fonts', 'NNP'),\n",
       " ('restuarant', 'restuarant', 'NN'): ('restaurant', 'NN'),\n",
       " ('caffiene', 'caffiene', 'NN'): ('caffeine', 'NN'),\n",
       " ('festifal', 'festifal', 'NN'): ('festival', 'NN'),\n",
       " ('treament', 'treament', 'NN'): ('treatment', 'NN'),\n",
       " ('changi', 'changi', 'NNP'): ('change', 'NNP'),\n",
       " ('tailand', 'tailand', 'NNP'): ('thailand', 'NNP'),\n",
       " ('chidren', 'chidren', 'NN'): ('children', 'NN'),\n",
       " ('researchs', 'researchs', 'NNS'): ('research', 'NNS'),\n",
       " ('diferents', 'diferents', 'NNS'): ('different', 'NNS'),\n",
       " ('dmv', 'dmv', 'NNP'): ('div', 'NNP'),\n",
       " ('kalinka', 'kalinka', 'NNP'): ('salina', 'NNP'),\n",
       " ('fatim', 'fatim', 'NNP'): ('fatima', 'NNP'),\n",
       " ('nyu', 'nyu', 'NN'): ('nyx', 'NN'),\n",
       " ('cctv', 'cctv', 'NNP'): ('city', 'NNP'),\n",
       " ('fter', 'fter', 'NN'): ('after', 'NN'),\n",
       " ('ang', 'ang', 'NNP'): ('and', 'NNP'),\n",
       " ('favorate', 'favorate', 'JJ'): ('favourite', 'JJ'),\n",
       " ('eygpt', 'eygpt', 'NNP'): ('egypt', 'NNP'),\n",
       " ('fcc', 'fcc', 'NNP'): ('acc', 'NNP'),\n",
       " ('magreb', 'magreb', 'NNP'): ('zagreb', 'NNP'),\n",
       " ('taht', 'taht', 'NN'): ('that', 'NN'),\n",
       " ('andré', 'andré', 'NNP'): ('andre', 'NNP'),\n",
       " ('schrek', 'schrek', 'NNP'): ('shrek', 'NNP'),\n",
       " ('frienship', 'frienship', 'NN'): ('friendship', 'NN'),\n",
       " ('togther', 'togther', 'NN'): ('together', 'NN'),\n",
       " ('speacial', 'speacial', 'JJ'): ('special', 'JJ'),\n",
       " ('feerick', 'feerick', 'NNP'): ('frederick', 'NNP'),\n",
       " ('fellings', 'fellings', 'NNS'): ('feelings', 'NNS'),\n",
       " ('wimax', 'wimax', 'NNP'): ('woman', 'NNP'),\n",
       " ('schiavo', 'schiavo', 'NNP'): ('chiao', 'NNP'),\n",
       " ('ruels', 'ruels', 'NNS'): ('rules', 'NNS'),\n",
       " ('schi', 'schi', 'NNP'): ('sci', 'NNP'),\n",
       " ('npr', 'npr', 'NNP'): ('apr', 'NNP'),\n",
       " ('explaination', 'explaination', 'NN'): ('explanation', 'NN'),\n",
       " ('firecrakers', 'firecrakers', 'NNS'): ('firecrackers', 'NNS'),\n",
       " ('orser', 'orser', 'NNP'): ('order', 'NNP'),\n",
       " ('continously', 'continously', 'RB'): ('continuously', 'RB'),\n",
       " ('trademen', 'trademen', 'NNS'): ('tradesmen', 'NNS'),\n",
       " ('experenice', 'experenice', 'NN'): ('experience', 'NN'),\n",
       " ('nslp', 'nslp', 'NNP'): ('help', 'NNP'),\n",
       " ('thuruga', 'thuruga', 'NNP'): ('thurgau', 'NNP'),\n",
       " ('cheang', 'cheang', 'NNP'): ('chang', 'NNP'),\n",
       " ('krabby', 'krabby', 'NNP'): ('crabby', 'NNP'),\n",
       " ('althought', 'althought', 'NNP'): ('although', 'NNP'),\n",
       " ('rodale', 'rodale', 'NNP'): ('role', 'NNP'),\n",
       " ('firt', 'firt', 'NNP'): ('first', 'NNP'),\n",
       " ('kraybill', 'kraybill', 'NNP'): ('playbill', 'NNP'),\n",
       " ('candelaria', 'candelaria', 'NNP'): ('candelabra', 'NNP'),\n",
       " ('montecito', 'montecito', 'NNP'): ('monteith', 'NNP'),\n",
       " ('campony', 'campony', 'NN'): ('company', 'NN'),\n",
       " ('alselah', 'alselah', 'NNP'): ('allah', 'NNP'),\n",
       " ('dirsor', 'dirsor', 'NN'): ('mirror', 'NN'),\n",
       " ('canaima', 'canaima', 'NNP'): ('canada', 'NNP'),\n",
       " ('nto', 'nto', 'IN'): ('to', 'IN'),\n",
       " ('alraghe', 'alraghe', 'NNP'): ('alright', 'NNP'),\n",
       " ('capex', 'capex', 'NNP'): ('cape', 'NNP'),\n",
       " ('laih', 'laih', 'NNP'): ('laid', 'NNP'),\n",
       " ('finacial', 'finacial', 'JJ'): ('financial', 'JJ'),\n",
       " ('ahyeon', 'ahyeon', 'NNP'): ('aeon', 'NNP'),\n",
       " ('rozin', 'rozin', 'NNP'): ('robin', 'NNP'),\n",
       " ('moniter', 'moniter', 'NN'): ('monitor', 'NN'),\n",
       " ('existance', 'existance', 'NN'): ('existence', 'NN'),\n",
       " ('someome', 'someome', 'NN'): ('someone', 'NN'),\n",
       " ('souq', 'souq', 'NNP'): ('soul', 'NNP'),\n",
       " ('disipline', 'disipline', 'NN'): ('discipline', 'NN'),\n",
       " ('dishs', 'dishs', 'NN'): ('dish', 'NN'),\n",
       " ('oppotunity', 'oppotunity', 'NN'): ('opportunity', 'NN'),\n",
       " ('exmple', 'exmple', 'NN'): ('example', 'NN'),\n",
       " ('somestr', 'somestr', 'NN'): ('semester', 'NN'),\n",
       " ('schedual', 'schedual', 'JJ'): ('schedule', 'JJ'),\n",
       " ('lamen', 'lamen', 'NNS'): ('laden', 'NNS'),\n",
       " ('finaly', 'finaly', 'NN'): ('final', 'NN'),\n",
       " ('ropots', 'ropots', 'NNP'): ('roots', 'NNP'),\n",
       " ('somia', 'somia', 'NNP'): ('soma', 'NNP'),\n",
       " ('roomates', 'roomates', 'NNS'): ('roommates', 'NNS'),\n",
       " ('somthing', 'somthing', 'NN'): ('something', 'NN'),\n",
       " ('calss', 'calss', 'NN'): ('class', 'NN'),\n",
       " ('komodo', 'komodo', 'NNP'): ('kosovo', 'NNP'),\n",
       " ('expencive', 'expencive', 'JJ'): ('expensive', 'JJ'),\n",
       " ('difrent', 'difrent', 'JJ'): ('different', 'JJ'),\n",
       " ('konbu', 'konbu', 'NNP'): ('kong', 'NNP'),\n",
       " ('taht', 'taht', 'IN'): ('that', 'IN'),\n",
       " ('lv', 'lv', 'NNP'): ('la', 'NNP'),\n",
       " ('whoes', 'whoes', 'NNS'): ('shoes', 'NNS'),\n",
       " ('dolsot', 'dolsot', 'NNP'): ('dorset', 'NNP'),\n",
       " ('renée', 'renée', 'NNP'): ('renee', 'NNP'),\n",
       " ('dictioary', 'dictioary', 'NNP'): ('dictionary', 'NNP'),\n",
       " ('littel', 'littel', 'NN'): ('little', 'NN'),\n",
       " ('thoes', 'thoes', 'NNS'): ('those', 'NNS'),\n",
       " ('yearis', 'yearis', 'NNP'): ('years', 'NNP'),\n",
       " ('counrty', 'counrty', 'NN'): ('county', 'NN'),\n",
       " ('albahar', 'albahar', 'NNP'): ('alcazar', 'NNP'),\n",
       " ('spevack', 'spevack', 'NNP'): ('speak', 'NNP'),\n",
       " ('niño', 'niño', 'NNP'): ('no', 'NNP'),\n",
       " ('jonh', 'jonh', 'NNP'): ('john', 'NNP'),\n",
       " ('toelf', 'toelf', 'NNP'): ('tools', 'NNP'),\n",
       " ('anoise', 'anoise', 'JJ'): ('noise', 'JJ'),\n",
       " ('mohummed', 'mohummed', 'NNP'): ('mohammad', 'NNP'),\n",
       " ('fainally', 'fainally', 'RB'): ('finally', 'RB'),\n",
       " ('joha', 'joha', 'NNP'): ('john', 'NNP'),\n",
       " ('yahnke', 'yahnke', 'NNP'): ('yankee', 'NNP'),\n",
       " ('thop', 'thop', 'NNP'): ('top', 'NNP'),\n",
       " ('famus', 'famus', 'JJ'): ('famous', 'JJ'),\n",
       " ('tobacoo', 'tobacoo', 'NN'): ('tobacco', 'NN'),\n",
       " ('maked', 'maked', 'VBD'): ('make', 'VBD'),\n",
       " ('fanta', 'fanta', 'NNP'): ('santa', 'NNP'),\n",
       " ('remenber', 'remenber', 'VBP'): ('remember', 'VBP'),\n",
       " ('famos', 'famos', 'JJ'): ('famous', 'JJ'),\n",
       " ('judu', 'judu', 'NN'): ('judy', 'NN'),\n",
       " ('scound', 'scound', 'NNP'): ('sound', 'NNP'),\n",
       " ('salil', 'salil', 'NNP'): ('sail', 'NNP'),\n",
       " ('scound', 'scound', 'NN'): ('sound', 'NN'),\n",
       " ('corol', 'corol', 'NNP'): ('cool', 'NNP'),\n",
       " ('diciplined', 'diciplined', 'VBN'): ('disciplined', 'VBN'),\n",
       " ('whith', 'whith', 'JJ'): ('with', 'JJ'),\n",
       " ('alfath', 'alfath', 'NNP'): ('allah', 'NNP'),\n",
       " ('odaiba', 'odaiba', 'NNP'): ('copaiba', 'NNP'),\n",
       " ('sanlu', 'sanlu', 'NNP'): ('sale', 'NNP'),\n",
       " ('supose', 'supose', 'VBP'): ('suppose', 'VBP'),\n",
       " ('tofel', 'tofel', 'JJ'): ('towel', 'JJ'),\n",
       " ('liga', 'liga', 'NNP'): ('lisa', 'NNP'),\n",
       " ('mounths', 'mounths', 'NNS'): ('months', 'NNS'),\n",
       " ('facinated', 'facinated', 'VBN'): ('fascinated', 'VBN'),\n",
       " ('yeon', 'yeon', 'NNP'): ('leon', 'NNP'),\n",
       " ('dokalah', 'dokalah', 'NN'): ('douala', 'NN'),\n",
       " ('alaqad', 'alaqad', 'NNP'): ('alana', 'NNP'),\n",
       " ('treeflight', 'treeflight', 'NNP'): ('preflight', 'NNP'),\n",
       " ('donot', 'donot', 'NN'): ('donor', 'NN'),\n",
       " ('ap', 'ap', 'NNP'): ('a', 'NNP'),\n",
       " ('lke', 'lke', 'VBP'): ('like', 'VBP'),\n",
       " ('aouther', 'aouther', 'NN'): ('souther', 'NN'),\n",
       " ('naïve', 'naïve', 'JJ'): ('naive', 'JJ'),\n",
       " ('movment', 'movment', 'NN'): ('moment', 'NN'),\n",
       " ('unfortunatelly', 'unfortunatelly', 'RB'): ('unfortunately', 'RB'),\n",
       " ('proplem', 'proplem', 'NN'): ('problem', 'NN'),\n",
       " ('abdull', 'abdull', 'NNP'): ('abdul', 'NNP'),\n",
       " ('sepented', 'sepented', 'VBD'): ('repented', 'VBD'),\n",
       " ('bcak', 'bcak', 'RB'): ('back', 'RB'),\n",
       " ('sandwitch', 'sandwitch', 'NN'): ('sandwich', 'NN'),\n",
       " ('rohlman', 'rohlman', 'NNP'): ('roman', 'NNP'),\n",
       " ('merried', 'merried', 'VBN'): ('married', 'VBN'),\n",
       " ('famliy', 'famliy', 'NN'): ('family', 'NN'),\n",
       " ('servies', 'servies', 'NNS'): ('services', 'NNS'),\n",
       " ('beacause', 'beacause', 'NN'): ('because', 'NN'),\n",
       " ('useing', 'useing', 'VBG'): ('using', 'VBG'),\n",
       " ('fitst', 'fitst', 'NNP'): ('first', 'NNP'),\n",
       " ('qq', 'qq', 'NNP'): ('sqq', 'NNP'),\n",
       " ('prohibe', 'prohibe', 'VB'): ('profile', 'VB'),\n",
       " ('fimaliy', 'fimaliy', 'RB'): ('finally', 'RB'),\n",
       " ('entertaiment', 'entertaiment', 'NN'): ('entertainment', 'NN'),\n",
       " ('famouse', 'famouse', 'JJ'): ('famous', 'JJ'),\n",
       " ('acknoleged', 'acknoleged', 'VBD'): ('acknowledged', 'VBD'),\n",
       " ('belivers', 'belivers', 'NNS'): ('delivers', 'NNS'),\n",
       " ('beacause', 'beacause', 'IN'): ('because', 'IN'),\n",
       " ('behavoir', 'behavoir', 'NN'): ('behaviour', 'NN'),\n",
       " ('torro', 'torro', 'NNP'): ('torso', 'NNP'),\n",
       " ('firest', 'firest', 'JJS'): ('first', 'JJS'),\n",
       " ('quistion', 'quistion', 'NN'): ('question', 'NN'),\n",
       " ('shcool', 'shcool', 'VB'): ('school', 'VB'),\n",
       " ('seperated', 'seperated', 'VBN'): ('separated', 'VBN'),\n",
       " ('chado', 'chado', 'NNP'): ('chad', 'NNP'),\n",
       " ('totaly', 'totaly', 'NNS'): ('total', 'NNS'),\n",
       " ('mcd', 'mcd', 'NNP'): ('cd', 'NNP'),\n",
       " ('usb', 'usb', 'NN'): ('us', 'NN'),\n",
       " ('happend', 'happend', 'NN'): ('happen', 'NN'),\n",
       " ('hj', 'hj', 'NNP'): ('he', 'NNP'),\n",
       " ('famely', 'famely', 'RB'): ('family', 'RB'),\n",
       " ('hla', 'hla', 'NNP'): ('la', 'NNP'),\n",
       " ('shawer', 'shawer', 'NN'): ('shower', 'NN'),\n",
       " ('ruba', 'ruba', 'NNP'): ('cuba', 'NNP'),\n",
       " ('famers', 'famers', 'NNS'): ('farmers', 'NNS'),\n",
       " ('activties', 'activties', 'NNS'): ('activities', 'NNS'),\n",
       " ('charcteristics', 'charcteristics', 'NNS'): ('characteristics', 'NNS'),\n",
       " ('firwords', 'firwords', 'NNS'): ('fireworks', 'NNS'),\n",
       " ...}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misspell_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incorporating back into pelic_df\n",
    "\n",
    "pelic_df['tok_POS_corrected'] = pelic_df['tok_lem_POS'].apply\\\n",
    "(lambda row: [misspell_dict[(x[0].lower(),x[1],x[2])] if (x[0].lower(),x[1],x[2]) in misspell_dict else (x[0],x[2]) for x in row])\n",
    "\n",
    "# One minor issue is that this will make misspelled items lower case when originally upper case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('My', 'my', 'PRP$'), ('friend', 'friend', 'NN'), ('is', 'be', 'VBZ'), ('realy', 'realy', 'JJ'), ('nise', 'nise', 'RB'), ('guy', 'guy', 'NN'), ('.', '.', '.'), ('I', 'i', 'PRP'), ('like', 'like', 'VBP'), ('hem', 'hem', 'JJ'), ('becuase', 'becuase', 'NN'), ('he', 'he', 'PRP'), ('is', 'be', 'VBZ'), ('friendlly', 'friendlly', 'RB'), ('and', 'and', 'CC'), ('lovliy', 'lovliy', 'NN'), ('.', '.', '.'))\n",
      "[('My', 'PRP$'), ('friend', 'NN'), ('is', 'VBZ'), ('real', 'JJ'), ('nice', 'RB'), ('guy', 'NN'), ('.', '.'), ('I', 'PRP'), ('like', 'VBP'), ('hem', 'JJ'), ('because', 'NN'), ('he', 'PRP'), ('is', 'VBZ'), ('friendly', 'RB'), ('and', 'CC'), ('lovely', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Checking with 'becuase'\n",
    "\n",
    "print(pelic_df.loc[pelic_df.text.str.contains('becuase')].iloc[1,11]) #uncorrected\n",
    "print(pelic_df.loc[pelic_df.text.str.contains('becuase')].iloc[1,12]) #corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that many approrpriate corrections have been made, including _beccuase_ -> _because_ , _nise_ -> _nice_ , and _lovily_ -> _lovely_ .  \n",
    "Importantly, incorrect spellings that are actual words, e.g. _hem_ (should be _him_ in this case) are not corrected. In addition, as context is not considered, there will be some inaccuracies, e.g. _realy_ (marked as an adj) -> _real_ rather than _really_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anon_id</th>\n",
       "      <th>L1</th>\n",
       "      <th>gender</th>\n",
       "      <th>course_id</th>\n",
       "      <th>level_id</th>\n",
       "      <th>class_id</th>\n",
       "      <th>question_id</th>\n",
       "      <th>version</th>\n",
       "      <th>text_len</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tok_lem_POS</th>\n",
       "      <th>tok_POS_corrected</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eq0</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>Male</td>\n",
       "      <td>149</td>\n",
       "      <td>4</td>\n",
       "      <td>g</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>177</td>\n",
       "      <td>I met my friend Nife while I was studying in a...</td>\n",
       "      <td>[I, met, my, friend, Nife, while, I, was, stud...</td>\n",
       "      <td>((I, i, PRP), (met, meet, VBD), (my, my, PRP$)...</td>\n",
       "      <td>[(I, PRP), (met, VBD), (my, PRP$), (friend, NN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>am8</td>\n",
       "      <td>Thai</td>\n",
       "      <td>Female</td>\n",
       "      <td>149</td>\n",
       "      <td>4</td>\n",
       "      <td>g</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>137</td>\n",
       "      <td>Ten years ago, I met a women on the train betw...</td>\n",
       "      <td>[Ten, years, ago, ,, I, met, a, women, on, the...</td>\n",
       "      <td>((Ten, ten, CD), (years, year, NNS), (ago, ago...</td>\n",
       "      <td>[(Ten, CD), (years, NNS), (ago, RB), (,, ,), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dk5</td>\n",
       "      <td>Turkish</td>\n",
       "      <td>Female</td>\n",
       "      <td>115</td>\n",
       "      <td>4</td>\n",
       "      <td>w</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>In my country we usually don't use tea bags. F...</td>\n",
       "      <td>[In, my, country, we, usually, do, n't, use, t...</td>\n",
       "      <td>((In, in, IN), (my, my, PRP$), (country, count...</td>\n",
       "      <td>[(In, IN), (my, PRP$), (country, NN), (we, PRP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dk5</td>\n",
       "      <td>Turkish</td>\n",
       "      <td>Female</td>\n",
       "      <td>115</td>\n",
       "      <td>4</td>\n",
       "      <td>w</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>I organized the instructions by time.</td>\n",
       "      <td>[I, organized, the, instructions, by, time, .]</td>\n",
       "      <td>((I, i, PRP), (organized, organize, VBD), (the...</td>\n",
       "      <td>[(I, PRP), (organized, VBD), (the, DT), (instr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ad1</td>\n",
       "      <td>Korean</td>\n",
       "      <td>Female</td>\n",
       "      <td>115</td>\n",
       "      <td>4</td>\n",
       "      <td>w</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>59</td>\n",
       "      <td>First, prepare a port, loose tea, and cup.\\nSe...</td>\n",
       "      <td>[First, ,, prepare, a, port, ,, loose, tea, ,,...</td>\n",
       "      <td>((First, first, RB), (,, ,, ,), (prepare, prep...</td>\n",
       "      <td>[(First, RB), (,, ,), (prepare, VB), (a, DT), ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          anon_id       L1  gender course_id level_id class_id question_id  \\\n",
       "answer_id                                                                    \n",
       "1             eq0   Arabic    Male       149        4        g           5   \n",
       "2             am8     Thai  Female       149        4        g           5   \n",
       "3             dk5  Turkish  Female       115        4        w          12   \n",
       "4             dk5  Turkish  Female       115        4        w          13   \n",
       "5             ad1   Korean  Female       115        4        w          12   \n",
       "\n",
       "          version  text_len  \\\n",
       "answer_id                     \n",
       "1               1       177   \n",
       "2               1       137   \n",
       "3               1        64   \n",
       "4               1         6   \n",
       "5               1        59   \n",
       "\n",
       "                                                        text  \\\n",
       "answer_id                                                      \n",
       "1          I met my friend Nife while I was studying in a...   \n",
       "2          Ten years ago, I met a women on the train betw...   \n",
       "3          In my country we usually don't use tea bags. F...   \n",
       "4                      I organized the instructions by time.   \n",
       "5          First, prepare a port, loose tea, and cup.\\nSe...   \n",
       "\n",
       "                                                      tokens  \\\n",
       "answer_id                                                      \n",
       "1          [I, met, my, friend, Nife, while, I, was, stud...   \n",
       "2          [Ten, years, ago, ,, I, met, a, women, on, the...   \n",
       "3          [In, my, country, we, usually, do, n't, use, t...   \n",
       "4             [I, organized, the, instructions, by, time, .]   \n",
       "5          [First, ,, prepare, a, port, ,, loose, tea, ,,...   \n",
       "\n",
       "                                                 tok_lem_POS  \\\n",
       "answer_id                                                      \n",
       "1          ((I, i, PRP), (met, meet, VBD), (my, my, PRP$)...   \n",
       "2          ((Ten, ten, CD), (years, year, NNS), (ago, ago...   \n",
       "3          ((In, in, IN), (my, my, PRP$), (country, count...   \n",
       "4          ((I, i, PRP), (organized, organize, VBD), (the...   \n",
       "5          ((First, first, RB), (,, ,, ,), (prepare, prep...   \n",
       "\n",
       "                                           tok_POS_corrected  \n",
       "answer_id                                                     \n",
       "1          [(I, PRP), (met, VBD), (my, PRP$), (friend, NN...  \n",
       "2          [(Ten, CD), (years, NNS), (ago, RB), (,, ,), (...  \n",
       "3          [(In, IN), (my, PRP$), (country, NN), (we, PRP...  \n",
       "4          [(I, PRP), (organized, VBD), (the, DT), (instr...  \n",
       "5          [(First, RB), (,, ,), (prepare, VB), (a, DT), ...  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pelic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out new PELIC_compiled.csv\n",
    "\n",
    "pelic_df.to_csv('PELIC_compiled_spellcorrected.csv', encoding='utf-8', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle new pelic_df dataframe\n",
    "\n",
    "pelic_df.to_pickle('pelic_spellcorrected.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If preferred, this entire spelling correctin process can also be applied to [`answer.csv`]() instead of `PELIC_compiled`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to top](#Corrected-spelling)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
